{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXRPF7T0b-QZ",
        "outputId": "821fc7ca-79e6-4cf6-e81d-37fd685b71f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/home/ashutosh/Desktop/ugmqa_project/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#run\n",
        "!pip install -qU spafe pandas tensorflow seaborn opencv-python tqdm librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s2K5aSHYb-Sm"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lfcc import lfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-xNIFhPtb-U5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-08 13:46:53.051430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QgbvNKYAb-YM"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import ResNet50,ResNet101\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.model_selection import StratifiedKFold , KFold ,RepeatedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LXsQV7ivcIcc"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask zero out padding tokens.\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  return tf.matmul(attention_weights, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7WB2_09_e-qt"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# This allows to the transformer to know where there is real data and where it is padded\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PjEQIR8TcIe3"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8hF4xYdLcSQA"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "    # apply sin to even index in the array\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd index in the array\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j7ewC3iBcSSo"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "  print(padding_mask)\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_8IFloblcSWJ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder(time_steps,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            projection,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  if projection=='linear':\n",
        "    ## We implement a linear projection based on Very Deep Self-Attention Networks for End-to-End Speech Recognition. Retrieved from https://arxiv.org/abs/1904.13377\n",
        "    projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
        "    print('linear')\n",
        "\n",
        "  else:\n",
        "    projection=tf.identity(inputs)\n",
        "    print('none')\n",
        "\n",
        "  projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  projection = PositionalEncoding(time_steps, d_model)(projection)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NXgJh2PpcIiK"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def transformer(time_steps,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                output_size,\n",
        "                projection,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(tf.dtypes.cast(\n",
        "\n",
        "      #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
        "      # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked\n",
        "      tf.math.reduce_sum(\n",
        "      inputs,\n",
        "      axis=2,\n",
        "      keepdims=False,\n",
        "      name=None\n",
        "  ), tf.int32))\n",
        "\n",
        "  enc_outputs = encoder(\n",
        "      time_steps=time_steps,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      projection=projection,\n",
        "      name='encoder'\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  #We reshape for feeding our FC in the next step\n",
        "  outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
        "\n",
        "  #We predict our class\n",
        "  outputs = tf.keras.layers.Dense(units=output_size,use_bias=True, name=\"outputs\")(outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9H-kkyuLcbPZ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# num_batch_size = 32\n",
        "# num_epochs = 500\n",
        "# N_SPLIT = 10\n",
        "# num_labels=5\n",
        "# num_classes=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Iw3KzecbRt",
        "outputId": "0ffe72e2-2c24-4cbd-9441-a681fcf507e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2075, 298)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#ajit\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate5_spectral2.csv')\n",
        "# dm\n",
        "# df = pd.read_csv('./DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm.csv')\n",
        "df = pd.read_csv('./DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm_4featu.csv')\n",
        "\n",
        "\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/melspectogram_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/chroma_cqt_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/mfcc_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/spectral_centroid_meandm_2075_200.csv')\n",
        "\n",
        "df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HrbtbLXcbTx",
        "outputId": "7a1af2ba-848e-44d8-d239-a2518c22f9bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(415, 298)\n",
            "(415, 298)\n",
            "(415, 298)\n",
            "(415, 298)\n",
            "(415, 298)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "siz=415\n",
        "df_read = df.copy()\n",
        "df1 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df1.index)\n",
        "df2 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df2.index)\n",
        "df3 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df3.index)\n",
        "df4 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df4.index)\n",
        "df5 = df_read.copy()\n",
        "\n",
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)\n",
        "print(df4.shape)\n",
        "print(df5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j19bgqpcbWT",
        "outputId": "e4e9f119-b436-44fe-ab74-572cf6d7bca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index([ 881,  453, 2004, 1353,  281,  941, 1185, 1159, 1138,  599,\n",
            "       ...\n",
            "        834, 1730,  353, 1345, 1190, 1375,  185,  701, 1671, 1982],\n",
            "      dtype='int64', length=415)\n"
          ]
        }
      ],
      "source": [
        "q = list(df1.index)+list(df2.index)+list(df3.index)+list(df4.index)+list(df5.index)\n",
        "print(df1.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>881</th>\n",
              "      <td>115.100464</td>\n",
              "      <td>116.435690</td>\n",
              "      <td>-33.895737</td>\n",
              "      <td>21.440712</td>\n",
              "      <td>-8.445481</td>\n",
              "      <td>-2.221351</td>\n",
              "      <td>-5.662595</td>\n",
              "      <td>-5.307188</td>\n",
              "      <td>-3.395119</td>\n",
              "      <td>1.959972</td>\n",
              "      <td>...</td>\n",
              "      <td>1924.734414</td>\n",
              "      <td>2214.323973</td>\n",
              "      <td>2369.313522</td>\n",
              "      <td>1993.016083</td>\n",
              "      <td>1656.398893</td>\n",
              "      <td>1712.388520</td>\n",
              "      <td>1877.955538</td>\n",
              "      <td>2202.385976</td>\n",
              "      <td>2197.057367</td>\n",
              "      <td>1.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>-81.505840</td>\n",
              "      <td>93.929110</td>\n",
              "      <td>2.407967</td>\n",
              "      <td>5.197178</td>\n",
              "      <td>-4.273181</td>\n",
              "      <td>15.114481</td>\n",
              "      <td>-12.457763</td>\n",
              "      <td>-2.422184</td>\n",
              "      <td>-10.028605</td>\n",
              "      <td>11.997586</td>\n",
              "      <td>...</td>\n",
              "      <td>2165.990403</td>\n",
              "      <td>2121.735482</td>\n",
              "      <td>2169.657425</td>\n",
              "      <td>2299.472756</td>\n",
              "      <td>2309.287640</td>\n",
              "      <td>2272.999651</td>\n",
              "      <td>2427.676392</td>\n",
              "      <td>2451.911800</td>\n",
              "      <td>2408.656902</td>\n",
              "      <td>1.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>-301.374900</td>\n",
              "      <td>286.829930</td>\n",
              "      <td>-56.174410</td>\n",
              "      <td>-25.532795</td>\n",
              "      <td>53.723870</td>\n",
              "      <td>-20.187690</td>\n",
              "      <td>-24.548950</td>\n",
              "      <td>25.045732</td>\n",
              "      <td>-8.054552</td>\n",
              "      <td>-20.254957</td>\n",
              "      <td>...</td>\n",
              "      <td>902.227666</td>\n",
              "      <td>895.068748</td>\n",
              "      <td>792.717657</td>\n",
              "      <td>816.468468</td>\n",
              "      <td>821.499510</td>\n",
              "      <td>797.085182</td>\n",
              "      <td>830.891295</td>\n",
              "      <td>837.543881</td>\n",
              "      <td>856.526688</td>\n",
              "      <td>1.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1353</th>\n",
              "      <td>-354.598450</td>\n",
              "      <td>259.105130</td>\n",
              "      <td>120.287290</td>\n",
              "      <td>-6.172213</td>\n",
              "      <td>-55.095108</td>\n",
              "      <td>-30.534496</td>\n",
              "      <td>13.320685</td>\n",
              "      <td>27.653042</td>\n",
              "      <td>7.574259</td>\n",
              "      <td>-16.686695</td>\n",
              "      <td>...</td>\n",
              "      <td>541.573084</td>\n",
              "      <td>527.706392</td>\n",
              "      <td>516.779659</td>\n",
              "      <td>516.765627</td>\n",
              "      <td>550.828158</td>\n",
              "      <td>547.339139</td>\n",
              "      <td>551.372349</td>\n",
              "      <td>539.345737</td>\n",
              "      <td>516.670265</td>\n",
              "      <td>1.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>13.566546</td>\n",
              "      <td>20.589693</td>\n",
              "      <td>16.374050</td>\n",
              "      <td>13.352259</td>\n",
              "      <td>7.065914</td>\n",
              "      <td>7.064869</td>\n",
              "      <td>3.771653</td>\n",
              "      <td>3.909107</td>\n",
              "      <td>1.383886</td>\n",
              "      <td>4.150478</td>\n",
              "      <td>...</td>\n",
              "      <td>4835.892625</td>\n",
              "      <td>4971.914880</td>\n",
              "      <td>5068.061026</td>\n",
              "      <td>4916.267485</td>\n",
              "      <td>4950.245484</td>\n",
              "      <td>5003.574506</td>\n",
              "      <td>4919.739213</td>\n",
              "      <td>4890.999577</td>\n",
              "      <td>5021.892732</td>\n",
              "      <td>1.08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 298 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0           1           2          3          4          5  \\\n",
              "881   115.100464  116.435690  -33.895737  21.440712  -8.445481  -2.221351   \n",
              "453   -81.505840   93.929110    2.407967   5.197178  -4.273181  15.114481   \n",
              "2004 -301.374900  286.829930  -56.174410 -25.532795  53.723870 -20.187690   \n",
              "1353 -354.598450  259.105130  120.287290  -6.172213 -55.095108 -30.534496   \n",
              "281    13.566546   20.589693   16.374050  13.352259   7.065914   7.064869   \n",
              "\n",
              "              6          7          8          9  ...          288  \\\n",
              "881   -5.662595  -5.307188  -3.395119   1.959972  ...  1924.734414   \n",
              "453  -12.457763  -2.422184 -10.028605  11.997586  ...  2165.990403   \n",
              "2004 -24.548950  25.045732  -8.054552 -20.254957  ...   902.227666   \n",
              "1353  13.320685  27.653042   7.574259 -16.686695  ...   541.573084   \n",
              "281    3.771653   3.909107   1.383886   4.150478  ...  4835.892625   \n",
              "\n",
              "              289          290          291          292          293  \\\n",
              "881   2214.323973  2369.313522  1993.016083  1656.398893  1712.388520   \n",
              "453   2121.735482  2169.657425  2299.472756  2309.287640  2272.999651   \n",
              "2004   895.068748   792.717657   816.468468   821.499510   797.085182   \n",
              "1353   527.706392   516.779659   516.765627   550.828158   547.339139   \n",
              "281   4971.914880  5068.061026  4916.267485  4950.245484  5003.574506   \n",
              "\n",
              "              294          295          296  class  \n",
              "881   1877.955538  2202.385976  2197.057367   1.61  \n",
              "453   2427.676392  2451.911800  2408.656902   1.15  \n",
              "2004   830.891295   837.543881   856.526688   1.76  \n",
              "1353   551.372349   539.345737   516.670265   1.24  \n",
              "281   4919.739213  4890.999577  5021.892732   1.08  \n",
              "\n",
              "[5 rows x 298 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT0Jrfa5cbZv",
        "outputId": "765d52d9-b8e2-4f6d-cd94-261bb7f7c39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "(1660, 1, 296)\n",
            "d_model 296\n",
            "num_heads 4\n",
            "linear\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-08 13:46:56.160466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-10-08 13:46:56.171126: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, None), dtype=tf.float32, name='padding_mask'), name='padding_mask', description=\"created by layer 'padding_mask'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, None), dtype=tf.float32, name='padding_mask'), name='padding_mask', description=\"created by layer 'padding_mask'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, None), dtype=tf.float32, name='padding_mask'), name='padding_mask', description=\"created by layer 'padding_mask'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, None), dtype=tf.float32, name='padding_mask'), name='padding_mask', description=\"created by layer 'padding_mask'\")\n",
            "Epoch 1/1000\n",
            "52/52 [==============================] - 10s 62ms/step - loss: 2.6473 - val_loss: 2.2255\n",
            "Epoch 2/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 2.1719 - val_loss: 1.8576\n",
            "Epoch 3/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 2.0875 - val_loss: 1.7502\n",
            "Epoch 4/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 2.0063 - val_loss: 1.6344\n",
            "Epoch 5/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.9020 - val_loss: 1.6036\n",
            "Epoch 6/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.8953 - val_loss: 1.5691\n",
            "Epoch 7/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.8875 - val_loss: 1.5525\n",
            "Epoch 8/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.8180 - val_loss: 1.5388\n",
            "Epoch 9/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.7716 - val_loss: 1.5606\n",
            "Epoch 10/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.7600 - val_loss: 1.5625\n",
            "Epoch 11/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.7643 - val_loss: 1.5273\n",
            "Epoch 12/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.7139 - val_loss: 1.5076\n",
            "Epoch 13/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.6976 - val_loss: 1.5028\n",
            "Epoch 14/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.6848 - val_loss: 1.5444\n",
            "Epoch 15/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.6953 - val_loss: 1.5284\n",
            "Epoch 16/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.6503 - val_loss: 1.5534\n",
            "Epoch 17/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.6676 - val_loss: 1.5784\n",
            "Epoch 18/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.6339 - val_loss: 1.5517\n",
            "Epoch 19/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.6942 - val_loss: 1.5165\n",
            "Epoch 20/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.6633 - val_loss: 1.6101\n",
            "Epoch 21/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.6085 - val_loss: 1.5596\n",
            "Epoch 22/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.5931 - val_loss: 1.5110\n",
            "Epoch 23/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.6331 - val_loss: 1.6189\n",
            "Epoch 24/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.5557 - val_loss: 1.5922\n",
            "Epoch 25/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.5435 - val_loss: 1.5922\n",
            "Epoch 26/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.6000 - val_loss: 1.5339\n",
            "Epoch 27/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.6167 - val_loss: 1.5201\n",
            "Epoch 28/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5792 - val_loss: 1.5620\n",
            "Epoch 29/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.6174 - val_loss: 1.4648\n",
            "Epoch 30/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5843 - val_loss: 1.6015\n",
            "Epoch 31/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5483 - val_loss: 1.6030\n",
            "Epoch 32/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5481 - val_loss: 1.6243\n",
            "Epoch 33/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5658 - val_loss: 1.6632\n",
            "Epoch 34/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5534 - val_loss: 1.5409\n",
            "Epoch 35/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5644 - val_loss: 1.6642\n",
            "Epoch 36/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4996 - val_loss: 1.7283\n",
            "Epoch 37/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5239 - val_loss: 1.6503\n",
            "Epoch 38/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5631 - val_loss: 1.5856\n",
            "Epoch 39/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5683 - val_loss: 1.6341\n",
            "Epoch 40/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5138 - val_loss: 1.6121\n",
            "Epoch 41/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5461 - val_loss: 1.6221\n",
            "Epoch 42/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4837 - val_loss: 1.6089\n",
            "Epoch 43/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.5441 - val_loss: 1.4963\n",
            "Epoch 44/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.5254 - val_loss: 1.9390\n",
            "Epoch 45/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5609 - val_loss: 1.7272\n",
            "Epoch 46/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4734 - val_loss: 1.7997\n",
            "Epoch 47/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5276 - val_loss: 1.7300\n",
            "Epoch 48/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5059 - val_loss: 1.7560\n",
            "Epoch 49/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4799 - val_loss: 1.9230\n",
            "Epoch 50/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4964 - val_loss: 1.7112\n",
            "Epoch 51/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4579 - val_loss: 1.7571\n",
            "Epoch 52/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.5160 - val_loss: 1.8121\n",
            "Epoch 53/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.4781 - val_loss: 1.7255\n",
            "Epoch 54/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4979 - val_loss: 1.7990\n",
            "Epoch 55/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4158 - val_loss: 1.8289\n",
            "Epoch 56/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4992 - val_loss: 1.6524\n",
            "Epoch 57/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4867 - val_loss: 1.8032\n",
            "Epoch 58/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4220 - val_loss: 1.6771\n",
            "Epoch 59/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.5029 - val_loss: 1.6199\n",
            "Epoch 60/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4768 - val_loss: 1.6589\n",
            "Epoch 61/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4663 - val_loss: 1.6733\n",
            "Epoch 62/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4720 - val_loss: 1.6729\n",
            "Epoch 63/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4441 - val_loss: 1.9610\n",
            "Epoch 64/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4735 - val_loss: 1.8701\n",
            "Epoch 65/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4202 - val_loss: 1.8893\n",
            "Epoch 66/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4811 - val_loss: 1.7283\n",
            "Epoch 67/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.5223 - val_loss: 1.7799\n",
            "Epoch 68/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4007 - val_loss: 2.1286\n",
            "Epoch 69/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4280 - val_loss: 1.8406\n",
            "Epoch 70/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4374 - val_loss: 1.6405\n",
            "Epoch 71/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4274 - val_loss: 1.8796\n",
            "Epoch 72/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4316 - val_loss: 1.8671\n",
            "Epoch 73/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4577 - val_loss: 1.9207\n",
            "Epoch 74/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4510 - val_loss: 1.9311\n",
            "Epoch 75/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4401 - val_loss: 1.7828\n",
            "Epoch 76/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4152 - val_loss: 1.9522\n",
            "Epoch 77/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4436 - val_loss: 1.8118\n",
            "Epoch 78/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4530 - val_loss: 1.8238\n",
            "Epoch 79/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4144 - val_loss: 1.7316\n",
            "Epoch 80/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4321 - val_loss: 1.6505\n",
            "Epoch 81/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4252 - val_loss: 2.0750\n",
            "Epoch 82/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4202 - val_loss: 2.0649\n",
            "Epoch 83/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4296 - val_loss: 1.8131\n",
            "Epoch 84/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4407 - val_loss: 1.7429\n",
            "Epoch 85/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4292 - val_loss: 1.8515\n",
            "Epoch 86/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4360 - val_loss: 1.7459\n",
            "Epoch 87/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4221 - val_loss: 2.0720\n",
            "Epoch 88/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4363 - val_loss: 2.0112\n",
            "Epoch 89/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4351 - val_loss: 1.9765\n",
            "Epoch 90/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4194 - val_loss: 1.8286\n",
            "Epoch 91/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4543 - val_loss: 1.6772\n",
            "Epoch 92/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3831 - val_loss: 1.9338\n",
            "Epoch 93/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4655 - val_loss: 1.7210\n",
            "Epoch 94/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4646 - val_loss: 2.0740\n",
            "Epoch 95/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4345 - val_loss: 1.9456\n",
            "Epoch 96/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4065 - val_loss: 2.0184\n",
            "Epoch 97/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4361 - val_loss: 2.0661\n",
            "Epoch 98/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4103 - val_loss: 1.9572\n",
            "Epoch 99/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3895 - val_loss: 1.8109\n",
            "Epoch 100/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4248 - val_loss: 1.8999\n",
            "Epoch 101/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4099 - val_loss: 1.8705\n",
            "Epoch 102/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4230 - val_loss: 1.7876\n",
            "Epoch 103/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4076 - val_loss: 2.0878\n",
            "Epoch 104/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4277 - val_loss: 1.9720\n",
            "Epoch 105/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3956 - val_loss: 2.0053\n",
            "Epoch 106/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4272 - val_loss: 1.8801\n",
            "Epoch 107/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3886 - val_loss: 2.0149\n",
            "Epoch 108/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4012 - val_loss: 1.8679\n",
            "Epoch 109/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3999 - val_loss: 1.9502\n",
            "Epoch 110/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3872 - val_loss: 1.9231\n",
            "Epoch 111/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4607 - val_loss: 2.4331\n",
            "Epoch 112/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4314 - val_loss: 1.8789\n",
            "Epoch 113/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3728 - val_loss: 2.1333\n",
            "Epoch 114/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3990 - val_loss: 2.0929\n",
            "Epoch 115/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3928 - val_loss: 1.9169\n",
            "Epoch 116/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4269 - val_loss: 2.1373\n",
            "Epoch 117/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4103 - val_loss: 1.9141\n",
            "Epoch 118/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3881 - val_loss: 1.9617\n",
            "Epoch 119/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4209 - val_loss: 2.1892\n",
            "Epoch 120/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3698 - val_loss: 1.9149\n",
            "Epoch 121/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4079 - val_loss: 1.9800\n",
            "Epoch 122/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4019 - val_loss: 1.8479\n",
            "Epoch 123/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4272 - val_loss: 2.1786\n",
            "Epoch 124/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3863 - val_loss: 1.8416\n",
            "Epoch 125/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4225 - val_loss: 1.9774\n",
            "Epoch 126/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4178 - val_loss: 2.3663\n",
            "Epoch 127/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4310 - val_loss: 1.9384\n",
            "Epoch 128/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4134 - val_loss: 2.1246\n",
            "Epoch 129/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.3841 - val_loss: 1.8591\n",
            "Epoch 130/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4510 - val_loss: 2.1105\n",
            "Epoch 131/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4137 - val_loss: 2.1202\n",
            "Epoch 132/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3937 - val_loss: 2.2581\n",
            "Epoch 133/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4414 - val_loss: 1.9881\n",
            "Epoch 134/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4238 - val_loss: 2.1109\n",
            "Epoch 135/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3842 - val_loss: 1.9952\n",
            "Epoch 136/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4135 - val_loss: 1.8394\n",
            "Epoch 137/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4036 - val_loss: 2.4150\n",
            "Epoch 138/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4076 - val_loss: 2.2702\n",
            "Epoch 139/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3779 - val_loss: 2.0604\n",
            "Epoch 140/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4377 - val_loss: 2.2399\n",
            "Epoch 141/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3942 - val_loss: 2.1668\n",
            "Epoch 142/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3578 - val_loss: 1.9478\n",
            "Epoch 143/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3724 - val_loss: 2.1769\n",
            "Epoch 144/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4230 - val_loss: 2.1053\n",
            "Epoch 145/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4079 - val_loss: 2.1583\n",
            "Epoch 146/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4124 - val_loss: 2.0360\n",
            "Epoch 147/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4316 - val_loss: 2.2946\n",
            "Epoch 148/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3870 - val_loss: 2.1357\n",
            "Epoch 149/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3915 - val_loss: 2.0963\n",
            "Epoch 150/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3830 - val_loss: 1.9655\n",
            "Epoch 151/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4546 - val_loss: 2.1461\n",
            "Epoch 152/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4018 - val_loss: 2.3733\n",
            "Epoch 153/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3745 - val_loss: 2.1016\n",
            "Epoch 154/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4119 - val_loss: 2.0769\n",
            "Epoch 155/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3999 - val_loss: 1.9892\n",
            "Epoch 156/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4056 - val_loss: 1.8795\n",
            "Epoch 157/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4270 - val_loss: 2.1203\n",
            "Epoch 158/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4261 - val_loss: 2.1464\n",
            "Epoch 159/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3946 - val_loss: 2.1530\n",
            "Epoch 160/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4192 - val_loss: 2.4110\n",
            "Epoch 161/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3915 - val_loss: 2.3637\n",
            "Epoch 162/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4386 - val_loss: 2.0432\n",
            "Epoch 163/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4123 - val_loss: 2.1667\n",
            "Epoch 164/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3970 - val_loss: 2.0779\n",
            "Epoch 165/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4039 - val_loss: 2.0925\n",
            "Epoch 166/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3869 - val_loss: 1.8333\n",
            "Epoch 167/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4145 - val_loss: 2.2338\n",
            "Epoch 168/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.3922 - val_loss: 2.1749\n",
            "Epoch 169/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 1.4154 - val_loss: 2.1323\n",
            "Epoch 170/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4078 - val_loss: 2.2400\n",
            "Epoch 171/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.3899 - val_loss: 2.2485\n",
            "Epoch 172/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4092 - val_loss: 2.1298\n",
            "Epoch 173/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4232 - val_loss: 2.0811\n",
            "Epoch 174/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4092 - val_loss: 2.1639\n",
            "Epoch 175/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4453 - val_loss: 1.9080\n",
            "Epoch 176/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3824 - val_loss: 2.1123\n",
            "Epoch 177/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3725 - val_loss: 2.1815\n",
            "Epoch 178/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3620 - val_loss: 2.2666\n",
            "Epoch 179/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4149 - val_loss: 2.2731\n",
            "Epoch 180/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4168 - val_loss: 1.9252\n",
            "Epoch 181/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.3991 - val_loss: 1.9859\n",
            "Epoch 182/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4331 - val_loss: 2.1650\n",
            "Epoch 183/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4119 - val_loss: 1.8559\n",
            "Epoch 184/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4083 - val_loss: 2.0839\n",
            "Epoch 185/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.3852 - val_loss: 2.2350\n",
            "Epoch 186/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4299 - val_loss: 2.3537\n",
            "Epoch 187/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4161 - val_loss: 2.2084\n",
            "Epoch 188/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4104 - val_loss: 1.9790\n",
            "Epoch 189/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4004 - val_loss: 2.4412\n",
            "Epoch 190/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3783 - val_loss: 2.2559\n",
            "Epoch 191/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.3843 - val_loss: 2.4629\n",
            "Epoch 192/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4124 - val_loss: 2.1015\n",
            "Epoch 193/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4283 - val_loss: 2.1316\n",
            "Epoch 194/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4418 - val_loss: 2.0010\n",
            "Epoch 195/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4217 - val_loss: 2.1752\n",
            "Epoch 196/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3948 - val_loss: 2.1027\n",
            "Epoch 197/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3852 - val_loss: 2.1077\n",
            "Epoch 198/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4096 - val_loss: 2.4623\n",
            "Epoch 199/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4529 - val_loss: 2.3465\n",
            "Epoch 200/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4195 - val_loss: 2.0540\n",
            "Epoch 201/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.4063 - val_loss: 2.2877\n",
            "Epoch 202/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3733 - val_loss: 2.2966\n",
            "Epoch 203/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4148 - val_loss: 2.1902\n",
            "Epoch 204/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4277 - val_loss: 2.1295\n",
            "Epoch 205/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4451 - val_loss: 2.0818\n",
            "Epoch 206/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.3845 - val_loss: 2.2005\n",
            "Epoch 207/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.4307 - val_loss: 2.1872\n",
            "Epoch 208/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.3761 - val_loss: 1.9143\n",
            "Epoch 209/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4191 - val_loss: 2.2344\n",
            "Epoch 210/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3738 - val_loss: 2.1625\n",
            "Epoch 211/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4366 - val_loss: 2.1449\n",
            "Epoch 212/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4038 - val_loss: 2.1331\n",
            "Epoch 213/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3966 - val_loss: 2.1245\n",
            "Epoch 214/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3913 - val_loss: 2.2364\n",
            "Epoch 215/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4105 - val_loss: 2.4029\n",
            "Epoch 216/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4232 - val_loss: 2.1897\n",
            "Epoch 217/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4329 - val_loss: 2.0752\n",
            "Epoch 218/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.3945 - val_loss: 2.1377\n",
            "Epoch 219/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4213 - val_loss: 2.1208\n",
            "Epoch 220/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4239 - val_loss: 2.3609\n",
            "Epoch 221/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4357 - val_loss: 1.9487\n",
            "Epoch 222/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4182 - val_loss: 2.2826\n",
            "Epoch 223/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4151 - val_loss: 2.0602\n",
            "Epoch 224/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.4015 - val_loss: 2.2528\n",
            "Epoch 225/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4328 - val_loss: 2.2971\n",
            "Epoch 226/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3861 - val_loss: 2.3136\n",
            "Epoch 227/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4064 - val_loss: 2.2059\n",
            "Epoch 228/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4463 - val_loss: 2.0148\n",
            "Epoch 229/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4319 - val_loss: 1.9888\n",
            "Epoch 230/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4378 - val_loss: 2.3271\n",
            "Epoch 231/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3702 - val_loss: 2.2362\n",
            "Epoch 232/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4224 - val_loss: 2.3810\n",
            "Epoch 233/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4215 - val_loss: 2.1651\n",
            "Epoch 234/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.3872 - val_loss: 2.3606\n",
            "Epoch 235/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4104 - val_loss: 1.9519\n",
            "Epoch 236/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4321 - val_loss: 2.2710\n",
            "Epoch 237/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4248 - val_loss: 2.3131\n",
            "Epoch 238/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4243 - val_loss: 2.1469\n",
            "Epoch 239/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3900 - val_loss: 2.0445\n",
            "Epoch 240/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4228 - val_loss: 2.0157\n",
            "Epoch 241/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3913 - val_loss: 2.4175\n",
            "Epoch 242/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4011 - val_loss: 2.3575\n",
            "Epoch 243/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.4229 - val_loss: 2.1114\n",
            "Epoch 244/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.4378 - val_loss: 2.0293\n",
            "Epoch 245/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4145 - val_loss: 2.1367\n",
            "Epoch 246/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4564 - val_loss: 2.3440\n",
            "Epoch 247/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4272 - val_loss: 2.0849\n",
            "Epoch 248/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4345 - val_loss: 1.9026\n",
            "Epoch 249/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4251 - val_loss: 2.0223\n",
            "Epoch 250/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4112 - val_loss: 2.0687\n",
            "Epoch 251/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4285 - val_loss: 2.3670\n",
            "Epoch 252/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4293 - val_loss: 2.0931\n",
            "Epoch 253/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4206 - val_loss: 2.1671\n",
            "Epoch 254/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4419 - val_loss: 2.0765\n",
            "Epoch 255/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4099 - val_loss: 2.2421\n",
            "Epoch 256/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4167 - val_loss: 2.0766\n",
            "Epoch 257/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4241 - val_loss: 2.0289\n",
            "Epoch 258/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4508 - val_loss: 2.1717\n",
            "Epoch 259/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4272 - val_loss: 1.9639\n",
            "Epoch 260/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4142 - val_loss: 1.9706\n",
            "Epoch 261/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3981 - val_loss: 2.0838\n",
            "Epoch 262/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4256 - val_loss: 2.1676\n",
            "Epoch 263/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4437 - val_loss: 1.9278\n",
            "Epoch 264/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4308 - val_loss: 2.0254\n",
            "Epoch 265/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4135 - val_loss: 2.0982\n",
            "Epoch 266/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4281 - val_loss: 2.0195\n",
            "Epoch 267/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4022 - val_loss: 1.8300\n",
            "Epoch 268/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4265 - val_loss: 2.0616\n",
            "Epoch 269/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4496 - val_loss: 1.8365\n",
            "Epoch 270/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3816 - val_loss: 2.2929\n",
            "Epoch 271/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4209 - val_loss: 2.0100\n",
            "Epoch 272/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4343 - val_loss: 1.9589\n",
            "Epoch 273/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.4505 - val_loss: 1.9075\n",
            "Epoch 274/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4399 - val_loss: 2.0146\n",
            "Epoch 275/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4252 - val_loss: 2.0413\n",
            "Epoch 276/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4195 - val_loss: 2.0010\n",
            "Epoch 277/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.3976 - val_loss: 1.9095\n",
            "Epoch 278/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4455 - val_loss: 1.8442\n",
            "Epoch 279/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.3881 - val_loss: 1.9572\n",
            "Epoch 280/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4194 - val_loss: 2.0092\n",
            "Epoch 281/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4224 - val_loss: 2.1285\n",
            "Epoch 282/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4226 - val_loss: 2.1159\n",
            "Epoch 283/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4260 - val_loss: 2.1076\n",
            "Epoch 284/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4068 - val_loss: 2.0271\n",
            "Epoch 285/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4573 - val_loss: 2.1395\n",
            "Epoch 286/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4297 - val_loss: 1.9535\n",
            "Epoch 287/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4312 - val_loss: 2.0983\n",
            "Epoch 288/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4323 - val_loss: 2.0125\n",
            "Epoch 289/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4373 - val_loss: 2.1539\n",
            "Epoch 290/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4174 - val_loss: 2.1271\n",
            "Epoch 291/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4030 - val_loss: 2.0117\n",
            "Epoch 292/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4314 - val_loss: 2.2207\n",
            "Epoch 293/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4283 - val_loss: 2.0885\n",
            "Epoch 294/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4078 - val_loss: 1.9772\n",
            "Epoch 295/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4334 - val_loss: 2.1665\n",
            "Epoch 296/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4426 - val_loss: 2.0446\n",
            "Epoch 297/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4208 - val_loss: 1.9820\n",
            "Epoch 298/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4558 - val_loss: 1.9849\n",
            "Epoch 299/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4509 - val_loss: 2.2818\n",
            "Epoch 300/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4190 - val_loss: 1.9368\n",
            "Epoch 301/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4102 - val_loss: 1.9779\n",
            "Epoch 302/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4355 - val_loss: 2.0091\n",
            "Epoch 303/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4499 - val_loss: 2.0792\n",
            "Epoch 304/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4403 - val_loss: 2.1552\n",
            "Epoch 305/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4091 - val_loss: 2.2501\n",
            "Epoch 306/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4514 - val_loss: 2.0055\n",
            "Epoch 307/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4507 - val_loss: 2.0933\n",
            "Epoch 308/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4544 - val_loss: 2.2217\n",
            "Epoch 309/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4472 - val_loss: 2.0908\n",
            "Epoch 310/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4594 - val_loss: 1.9016\n",
            "Epoch 311/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4078 - val_loss: 1.9770\n",
            "Epoch 312/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4158 - val_loss: 1.9458\n",
            "Epoch 313/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4696 - val_loss: 1.9958\n",
            "Epoch 314/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4252 - val_loss: 1.8921\n",
            "Epoch 315/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4777 - val_loss: 2.0177\n",
            "Epoch 316/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4323 - val_loss: 1.9801\n",
            "Epoch 317/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4206 - val_loss: 1.8767\n",
            "Epoch 318/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.4336 - val_loss: 1.9128\n",
            "Epoch 319/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4759 - val_loss: 2.0683\n",
            "Epoch 320/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4394 - val_loss: 1.9952\n",
            "Epoch 321/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4630 - val_loss: 1.7962\n",
            "Epoch 322/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4466 - val_loss: 1.8661\n",
            "Epoch 323/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4170 - val_loss: 2.0224\n",
            "Epoch 324/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4333 - val_loss: 1.8727\n",
            "Epoch 325/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4634 - val_loss: 1.7999\n",
            "Epoch 326/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.4270 - val_loss: 2.1543\n",
            "Epoch 327/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4860 - val_loss: 1.9194\n",
            "Epoch 328/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.4642 - val_loss: 1.8804\n",
            "Epoch 329/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.4572 - val_loss: 1.8718\n",
            "Epoch 330/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 1.4119 - val_loss: 2.0522\n",
            "Epoch 331/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4690 - val_loss: 1.9069\n",
            "Epoch 332/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4570 - val_loss: 1.8108\n",
            "Epoch 333/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4763 - val_loss: 2.0326\n",
            "Epoch 334/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4193 - val_loss: 1.8582\n",
            "Epoch 335/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4587 - val_loss: 2.0683\n",
            "Epoch 336/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4707 - val_loss: 1.9034\n",
            "Epoch 337/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4413 - val_loss: 1.9774\n",
            "Epoch 338/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4792 - val_loss: 1.8654\n",
            "Epoch 339/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4593 - val_loss: 2.0910\n",
            "Epoch 340/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4481 - val_loss: 1.9953\n",
            "Epoch 341/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4410 - val_loss: 1.8992\n",
            "Epoch 342/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4434 - val_loss: 2.1033\n",
            "Epoch 343/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4487 - val_loss: 2.0407\n",
            "Epoch 344/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4183 - val_loss: 1.8475\n",
            "Epoch 345/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4366 - val_loss: 1.7946\n",
            "Epoch 346/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.4713 - val_loss: 1.8866\n",
            "Epoch 347/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4540 - val_loss: 2.0029\n",
            "Epoch 348/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4180 - val_loss: 2.1855\n",
            "Epoch 349/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4748 - val_loss: 1.9674\n",
            "Epoch 350/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4180 - val_loss: 2.1905\n",
            "Epoch 351/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4739 - val_loss: 1.9196\n",
            "Epoch 352/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4725 - val_loss: 2.0357\n",
            "Epoch 353/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4605 - val_loss: 2.0755\n",
            "Epoch 354/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4646 - val_loss: 1.9919\n",
            "Epoch 355/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4346 - val_loss: 2.0452\n",
            "Epoch 356/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4368 - val_loss: 2.0061\n",
            "Epoch 357/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4564 - val_loss: 1.8677\n",
            "Epoch 358/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4106 - val_loss: 2.0000\n",
            "Epoch 359/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4636 - val_loss: 1.9878\n",
            "Epoch 360/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.4359 - val_loss: 1.9413\n",
            "Epoch 361/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4249 - val_loss: 2.0437\n",
            "Epoch 362/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4395 - val_loss: 1.9151\n",
            "Epoch 363/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4439 - val_loss: 2.0238\n",
            "Epoch 364/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4696 - val_loss: 2.0451\n",
            "Epoch 365/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4404 - val_loss: 1.9800\n",
            "Epoch 366/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4271 - val_loss: 2.0379\n",
            "Epoch 367/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4638 - val_loss: 1.9413\n",
            "Epoch 368/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4734 - val_loss: 1.9693\n",
            "Epoch 369/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4626 - val_loss: 1.9408\n",
            "Epoch 370/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4266 - val_loss: 1.9814\n",
            "Epoch 371/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4532 - val_loss: 1.8416\n",
            "Epoch 372/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4273 - val_loss: 1.8612\n",
            "Epoch 373/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4417 - val_loss: 2.0572\n",
            "Epoch 374/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4533 - val_loss: 2.0283\n",
            "Epoch 375/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4811 - val_loss: 2.0874\n",
            "Epoch 376/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.4713 - val_loss: 1.8783\n",
            "Epoch 377/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4682 - val_loss: 1.9718\n",
            "Epoch 378/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4525 - val_loss: 1.9856\n",
            "Epoch 379/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4483 - val_loss: 1.9485\n",
            "Epoch 380/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4997 - val_loss: 1.8965\n",
            "Epoch 381/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4760 - val_loss: 1.9265\n",
            "Epoch 382/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4703 - val_loss: 1.8812\n",
            "Epoch 383/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4395 - val_loss: 1.8990\n",
            "Epoch 384/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4467 - val_loss: 1.9160\n",
            "Epoch 385/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4210 - val_loss: 1.9195\n",
            "Epoch 386/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4249 - val_loss: 2.1039\n",
            "Epoch 387/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5028 - val_loss: 1.8611\n",
            "Epoch 388/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4657 - val_loss: 1.8403\n",
            "Epoch 389/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4456 - val_loss: 2.0260\n",
            "Epoch 390/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4754 - val_loss: 1.9101\n",
            "Epoch 391/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4696 - val_loss: 1.9514\n",
            "Epoch 392/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4614 - val_loss: 1.8577\n",
            "Epoch 393/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4329 - val_loss: 1.9711\n",
            "Epoch 394/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4541 - val_loss: 1.8399\n",
            "Epoch 395/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4482 - val_loss: 2.0538\n",
            "Epoch 396/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4871 - val_loss: 1.9480\n",
            "Epoch 397/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4495 - val_loss: 1.8650\n",
            "Epoch 398/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4929 - val_loss: 1.7966\n",
            "Epoch 399/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4643 - val_loss: 1.8155\n",
            "Epoch 400/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4759 - val_loss: 1.7813\n",
            "Epoch 401/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4513 - val_loss: 1.9504\n",
            "Epoch 402/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4603 - val_loss: 1.9036\n",
            "Epoch 403/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4933 - val_loss: 1.9834\n",
            "Epoch 404/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4496 - val_loss: 1.7468\n",
            "Epoch 405/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4347 - val_loss: 1.7311\n",
            "Epoch 406/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4628 - val_loss: 1.9230\n",
            "Epoch 407/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4699 - val_loss: 1.8478\n",
            "Epoch 408/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4506 - val_loss: 1.9016\n",
            "Epoch 409/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4555 - val_loss: 1.8141\n",
            "Epoch 410/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4502 - val_loss: 1.9905\n",
            "Epoch 411/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4746 - val_loss: 1.8265\n",
            "Epoch 412/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4698 - val_loss: 1.9193\n",
            "Epoch 413/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4612 - val_loss: 1.7631\n",
            "Epoch 414/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4552 - val_loss: 1.7511\n",
            "Epoch 415/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4749 - val_loss: 1.9133\n",
            "Epoch 416/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4766 - val_loss: 1.7775\n",
            "Epoch 417/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4336 - val_loss: 1.8572\n",
            "Epoch 418/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4528 - val_loss: 1.8017\n",
            "Epoch 419/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4533 - val_loss: 1.7313\n",
            "Epoch 420/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4704 - val_loss: 1.8236\n",
            "Epoch 421/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4406 - val_loss: 1.9722\n",
            "Epoch 422/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4926 - val_loss: 1.9385\n",
            "Epoch 423/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4381 - val_loss: 1.9797\n",
            "Epoch 424/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4331 - val_loss: 1.9988\n",
            "Epoch 425/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4613 - val_loss: 1.9340\n",
            "Epoch 426/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4635 - val_loss: 1.9292\n",
            "Epoch 427/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4701 - val_loss: 1.7993\n",
            "Epoch 428/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4368 - val_loss: 1.9862\n",
            "Epoch 429/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4828 - val_loss: 1.9925\n",
            "Epoch 430/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4323 - val_loss: 1.9801\n",
            "Epoch 431/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4565 - val_loss: 1.8294\n",
            "Epoch 432/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4634 - val_loss: 1.9092\n",
            "Epoch 433/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4956 - val_loss: 1.9695\n",
            "Epoch 434/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4410 - val_loss: 1.8747\n",
            "Epoch 435/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4832 - val_loss: 1.7640\n",
            "Epoch 436/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4803 - val_loss: 1.8869\n",
            "Epoch 437/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4896 - val_loss: 1.9104\n",
            "Epoch 438/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4335 - val_loss: 2.0241\n",
            "Epoch 439/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4181 - val_loss: 1.9842\n",
            "Epoch 440/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4721 - val_loss: 1.8283\n",
            "Epoch 441/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4471 - val_loss: 1.8973\n",
            "Epoch 442/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5046 - val_loss: 1.9554\n",
            "Epoch 443/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4900 - val_loss: 1.9435\n",
            "Epoch 444/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4802 - val_loss: 1.8271\n",
            "Epoch 445/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4917 - val_loss: 1.9029\n",
            "Epoch 446/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4986 - val_loss: 1.7890\n",
            "Epoch 447/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4791 - val_loss: 1.9852\n",
            "Epoch 448/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4419 - val_loss: 1.9803\n",
            "Epoch 449/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4762 - val_loss: 1.8515\n",
            "Epoch 450/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4628 - val_loss: 1.8250\n",
            "Epoch 451/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4773 - val_loss: 1.8301\n",
            "Epoch 452/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4885 - val_loss: 1.8284\n",
            "Epoch 453/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5030 - val_loss: 1.9463\n",
            "Epoch 454/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5078 - val_loss: 1.8951\n",
            "Epoch 455/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4766 - val_loss: 1.8653\n",
            "Epoch 456/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4696 - val_loss: 1.9138\n",
            "Epoch 457/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4821 - val_loss: 1.8801\n",
            "Epoch 458/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4541 - val_loss: 1.7293\n",
            "Epoch 459/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4464 - val_loss: 1.8151\n",
            "Epoch 460/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4873 - val_loss: 1.8555\n",
            "Epoch 461/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4684 - val_loss: 2.0142\n",
            "Epoch 462/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4544 - val_loss: 1.7831\n",
            "Epoch 463/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4816 - val_loss: 1.8567\n",
            "Epoch 464/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4792 - val_loss: 1.8728\n",
            "Epoch 465/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4737 - val_loss: 1.7878\n",
            "Epoch 466/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4818 - val_loss: 1.8305\n",
            "Epoch 467/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4776 - val_loss: 1.9517\n",
            "Epoch 468/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4629 - val_loss: 1.7911\n",
            "Epoch 469/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4201 - val_loss: 1.9275\n",
            "Epoch 470/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4824 - val_loss: 1.8610\n",
            "Epoch 471/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4476 - val_loss: 1.9338\n",
            "Epoch 472/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4723 - val_loss: 1.7917\n",
            "Epoch 473/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4813 - val_loss: 1.8512\n",
            "Epoch 474/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4687 - val_loss: 1.8188\n",
            "Epoch 475/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4988 - val_loss: 1.7553\n",
            "Epoch 476/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4867 - val_loss: 1.8404\n",
            "Epoch 477/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4741 - val_loss: 1.8523\n",
            "Epoch 478/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4530 - val_loss: 1.8892\n",
            "Epoch 479/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4690 - val_loss: 1.9980\n",
            "Epoch 480/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4614 - val_loss: 1.7531\n",
            "Epoch 481/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4714 - val_loss: 1.8506\n",
            "Epoch 482/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4571 - val_loss: 1.8207\n",
            "Epoch 483/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4730 - val_loss: 1.9579\n",
            "Epoch 484/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4546 - val_loss: 1.7824\n",
            "Epoch 485/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4289 - val_loss: 1.9109\n",
            "Epoch 486/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4938 - val_loss: 1.8527\n",
            "Epoch 487/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5152 - val_loss: 1.8810\n",
            "Epoch 488/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5183 - val_loss: 1.8638\n",
            "Epoch 489/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4514 - val_loss: 1.9809\n",
            "Epoch 490/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4814 - val_loss: 1.8841\n",
            "Epoch 491/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4956 - val_loss: 1.7247\n",
            "Epoch 492/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5225 - val_loss: 1.7403\n",
            "Epoch 493/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4886 - val_loss: 1.9040\n",
            "Epoch 494/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5052 - val_loss: 1.8764\n",
            "Epoch 495/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4779 - val_loss: 1.7594\n",
            "Epoch 496/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4795 - val_loss: 1.7393\n",
            "Epoch 497/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4781 - val_loss: 1.8175\n",
            "Epoch 498/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4863 - val_loss: 1.9061\n",
            "Epoch 499/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4890 - val_loss: 1.8192\n",
            "Epoch 500/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4923 - val_loss: 1.9081\n",
            "Epoch 501/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4837 - val_loss: 1.8183\n",
            "Epoch 502/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4829 - val_loss: 1.8199\n",
            "Epoch 503/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5020 - val_loss: 1.8890\n",
            "Epoch 504/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4861 - val_loss: 1.7292\n",
            "Epoch 505/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4803 - val_loss: 1.8907\n",
            "Epoch 506/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4815 - val_loss: 1.7955\n",
            "Epoch 507/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4314 - val_loss: 1.7311\n",
            "Epoch 508/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4738 - val_loss: 1.7917\n",
            "Epoch 509/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.5079 - val_loss: 1.8005\n",
            "Epoch 510/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4707 - val_loss: 1.8164\n",
            "Epoch 511/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4929 - val_loss: 1.8296\n",
            "Epoch 512/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4547 - val_loss: 1.8174\n",
            "Epoch 513/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4967 - val_loss: 1.7327\n",
            "Epoch 514/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4721 - val_loss: 1.7249\n",
            "Epoch 515/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.4802 - val_loss: 1.6629\n",
            "Epoch 516/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5093 - val_loss: 1.7194\n",
            "Epoch 517/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4239 - val_loss: 1.7613\n",
            "Epoch 518/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4717 - val_loss: 1.8609\n",
            "Epoch 519/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5002 - val_loss: 1.7777\n",
            "Epoch 520/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5131 - val_loss: 1.9003\n",
            "Epoch 521/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4855 - val_loss: 1.9194\n",
            "Epoch 522/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4699 - val_loss: 1.7600\n",
            "Epoch 523/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5106 - val_loss: 1.9474\n",
            "Epoch 524/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4767 - val_loss: 1.8790\n",
            "Epoch 525/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5026 - val_loss: 1.7079\n",
            "Epoch 526/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4837 - val_loss: 1.6912\n",
            "Epoch 527/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4819 - val_loss: 1.7800\n",
            "Epoch 528/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5065 - val_loss: 1.6998\n",
            "Epoch 529/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5072 - val_loss: 1.7312\n",
            "Epoch 530/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4696 - val_loss: 1.7603\n",
            "Epoch 531/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5182 - val_loss: 1.8208\n",
            "Epoch 532/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4557 - val_loss: 1.7349\n",
            "Epoch 533/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4925 - val_loss: 1.8307\n",
            "Epoch 534/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4750 - val_loss: 1.7958\n",
            "Epoch 535/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4794 - val_loss: 1.8480\n",
            "Epoch 536/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4980 - val_loss: 1.8273\n",
            "Epoch 537/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4971 - val_loss: 1.7693\n",
            "Epoch 538/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5019 - val_loss: 1.7463\n",
            "Epoch 539/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5037 - val_loss: 1.7720\n",
            "Epoch 540/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4944 - val_loss: 1.8359\n",
            "Epoch 541/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4966 - val_loss: 1.7175\n",
            "Epoch 542/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5014 - val_loss: 1.7828\n",
            "Epoch 543/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5137 - val_loss: 1.7560\n",
            "Epoch 544/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5004 - val_loss: 1.7830\n",
            "Epoch 545/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4868 - val_loss: 1.8691\n",
            "Epoch 546/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4682 - val_loss: 1.7569\n",
            "Epoch 547/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5045 - val_loss: 1.7812\n",
            "Epoch 548/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5085 - val_loss: 1.8872\n",
            "Epoch 549/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4951 - val_loss: 1.8083\n",
            "Epoch 550/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4914 - val_loss: 1.6947\n",
            "Epoch 551/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4843 - val_loss: 1.7878\n",
            "Epoch 552/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5010 - val_loss: 1.7706\n",
            "Epoch 553/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5028 - val_loss: 1.7150\n",
            "Epoch 554/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4530 - val_loss: 1.8697\n",
            "Epoch 555/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5253 - val_loss: 1.8228\n",
            "Epoch 556/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4697 - val_loss: 1.6921\n",
            "Epoch 557/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4911 - val_loss: 1.7069\n",
            "Epoch 558/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5140 - val_loss: 1.7086\n",
            "Epoch 559/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4852 - val_loss: 1.8373\n",
            "Epoch 560/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4903 - val_loss: 1.7625\n",
            "Epoch 561/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5076 - val_loss: 1.8756\n",
            "Epoch 562/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5139 - val_loss: 1.6650\n",
            "Epoch 563/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4851 - val_loss: 1.7780\n",
            "Epoch 564/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4908 - val_loss: 1.8208\n",
            "Epoch 565/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4824 - val_loss: 1.8395\n",
            "Epoch 566/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4875 - val_loss: 1.9491\n",
            "Epoch 567/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5346 - val_loss: 1.7854\n",
            "Epoch 568/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4983 - val_loss: 1.7919\n",
            "Epoch 569/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5020 - val_loss: 1.8551\n",
            "Epoch 570/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5102 - val_loss: 1.8884\n",
            "Epoch 571/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5091 - val_loss: 1.8101\n",
            "Epoch 572/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4994 - val_loss: 1.7587\n",
            "Epoch 573/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5100 - val_loss: 1.7185\n",
            "Epoch 574/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4848 - val_loss: 1.7958\n",
            "Epoch 575/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5035 - val_loss: 1.8051\n",
            "Epoch 576/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4900 - val_loss: 1.7560\n",
            "Epoch 577/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4997 - val_loss: 1.8418\n",
            "Epoch 578/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4846 - val_loss: 1.7141\n",
            "Epoch 579/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4894 - val_loss: 1.7404\n",
            "Epoch 580/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5209 - val_loss: 1.7574\n",
            "Epoch 581/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.5016 - val_loss: 1.7594\n",
            "Epoch 582/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5099 - val_loss: 1.7194\n",
            "Epoch 583/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5280 - val_loss: 1.8256\n",
            "Epoch 584/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4850 - val_loss: 1.8425\n",
            "Epoch 585/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4790 - val_loss: 1.9401\n",
            "Epoch 586/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4946 - val_loss: 1.9412\n",
            "Epoch 587/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5540 - val_loss: 1.7975\n",
            "Epoch 588/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5052 - val_loss: 1.6866\n",
            "Epoch 589/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5263 - val_loss: 1.8091\n",
            "Epoch 590/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4946 - val_loss: 1.9236\n",
            "Epoch 591/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5102 - val_loss: 1.7991\n",
            "Epoch 592/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5233 - val_loss: 1.7363\n",
            "Epoch 593/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4801 - val_loss: 1.8551\n",
            "Epoch 594/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5038 - val_loss: 1.7713\n",
            "Epoch 595/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4969 - val_loss: 1.7619\n",
            "Epoch 596/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5356 - val_loss: 1.7317\n",
            "Epoch 597/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5102 - val_loss: 1.7979\n",
            "Epoch 598/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5051 - val_loss: 1.6825\n",
            "Epoch 599/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5322 - val_loss: 1.7512\n",
            "Epoch 600/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5184 - val_loss: 1.7277\n",
            "Epoch 601/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4881 - val_loss: 1.6844\n",
            "Epoch 602/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5102 - val_loss: 1.7681\n",
            "Epoch 603/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5173 - val_loss: 1.8364\n",
            "Epoch 604/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5331 - val_loss: 1.6879\n",
            "Epoch 605/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4954 - val_loss: 1.7539\n",
            "Epoch 606/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4911 - val_loss: 1.6698\n",
            "Epoch 607/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5001 - val_loss: 1.8610\n",
            "Epoch 608/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4966 - val_loss: 1.6969\n",
            "Epoch 609/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4902 - val_loss: 1.6717\n",
            "Epoch 610/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4906 - val_loss: 1.7883\n",
            "Epoch 611/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5195 - val_loss: 1.7456\n",
            "Epoch 612/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4904 - val_loss: 1.7346\n",
            "Epoch 613/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5194 - val_loss: 1.6403\n",
            "Epoch 614/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5015 - val_loss: 1.8184\n",
            "Epoch 615/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4860 - val_loss: 1.7983\n",
            "Epoch 616/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5065 - val_loss: 1.8096\n",
            "Epoch 617/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5187 - val_loss: 1.8159\n",
            "Epoch 618/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4979 - val_loss: 1.7604\n",
            "Epoch 619/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5053 - val_loss: 1.7982\n",
            "Epoch 620/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4930 - val_loss: 1.7751\n",
            "Epoch 621/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5215 - val_loss: 1.7032\n",
            "Epoch 622/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5207 - val_loss: 1.6742\n",
            "Epoch 623/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5250 - val_loss: 1.6235\n",
            "Epoch 624/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.4971 - val_loss: 1.7616\n",
            "Epoch 625/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5083 - val_loss: 1.8318\n",
            "Epoch 626/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5343 - val_loss: 1.7490\n",
            "Epoch 627/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4883 - val_loss: 1.9363\n",
            "Epoch 628/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5249 - val_loss: 1.7697\n",
            "Epoch 629/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4751 - val_loss: 1.8167\n",
            "Epoch 630/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5037 - val_loss: 1.7861\n",
            "Epoch 631/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5208 - val_loss: 1.7726\n",
            "Epoch 632/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5140 - val_loss: 1.7412\n",
            "Epoch 633/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4951 - val_loss: 1.7531\n",
            "Epoch 634/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5396 - val_loss: 1.7672\n",
            "Epoch 635/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5105 - val_loss: 1.7464\n",
            "Epoch 636/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4825 - val_loss: 1.7866\n",
            "Epoch 637/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4973 - val_loss: 1.8106\n",
            "Epoch 638/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5163 - val_loss: 1.8567\n",
            "Epoch 639/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5454 - val_loss: 1.7313\n",
            "Epoch 640/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4927 - val_loss: 1.8495\n",
            "Epoch 641/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5073 - val_loss: 1.7782\n",
            "Epoch 642/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4933 - val_loss: 1.7791\n",
            "Epoch 643/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5313 - val_loss: 1.7542\n",
            "Epoch 644/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4890 - val_loss: 1.7844\n",
            "Epoch 645/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5352 - val_loss: 1.7525\n",
            "Epoch 646/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5241 - val_loss: 1.8403\n",
            "Epoch 647/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4978 - val_loss: 1.7635\n",
            "Epoch 648/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5317 - val_loss: 1.7527\n",
            "Epoch 649/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5300 - val_loss: 1.6910\n",
            "Epoch 650/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5696 - val_loss: 1.6968\n",
            "Epoch 651/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5346 - val_loss: 1.6790\n",
            "Epoch 652/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5371 - val_loss: 1.8376\n",
            "Epoch 653/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5229 - val_loss: 1.6986\n",
            "Epoch 654/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5105 - val_loss: 1.6585\n",
            "Epoch 655/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5436 - val_loss: 1.7160\n",
            "Epoch 656/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5148 - val_loss: 1.6974\n",
            "Epoch 657/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5226 - val_loss: 1.6347\n",
            "Epoch 658/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5030 - val_loss: 1.7915\n",
            "Epoch 659/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5450 - val_loss: 1.6611\n",
            "Epoch 660/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5061 - val_loss: 1.6486\n",
            "Epoch 661/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4879 - val_loss: 1.6680\n",
            "Epoch 662/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5111 - val_loss: 1.6800\n",
            "Epoch 663/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5160 - val_loss: 1.7433\n",
            "Epoch 664/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.4939 - val_loss: 1.6215\n",
            "Epoch 665/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5121 - val_loss: 1.6634\n",
            "Epoch 666/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5077 - val_loss: 1.6681\n",
            "Epoch 667/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5208 - val_loss: 1.7589\n",
            "Epoch 668/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5034 - val_loss: 1.8283\n",
            "Epoch 669/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5328 - val_loss: 1.7102\n",
            "Epoch 670/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5268 - val_loss: 1.7594\n",
            "Epoch 671/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5199 - val_loss: 1.6654\n",
            "Epoch 672/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5115 - val_loss: 1.6454\n",
            "Epoch 673/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5139 - val_loss: 1.7015\n",
            "Epoch 674/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.4925 - val_loss: 1.6149\n",
            "Epoch 675/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5189 - val_loss: 1.6232\n",
            "Epoch 676/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5529 - val_loss: 1.6915\n",
            "Epoch 677/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5028 - val_loss: 1.6494\n",
            "Epoch 678/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.4935 - val_loss: 1.7771\n",
            "Epoch 679/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5209 - val_loss: 1.6424\n",
            "Epoch 680/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.4971 - val_loss: 1.6495\n",
            "Epoch 681/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5042 - val_loss: 1.7794\n",
            "Epoch 682/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5042 - val_loss: 1.8209\n",
            "Epoch 683/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5366 - val_loss: 1.7968\n",
            "Epoch 684/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5252 - val_loss: 1.6787\n",
            "Epoch 685/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5221 - val_loss: 1.7445\n",
            "Epoch 686/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5331 - val_loss: 1.7046\n",
            "Epoch 687/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5248 - val_loss: 1.6769\n",
            "Epoch 688/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5354 - val_loss: 1.6848\n",
            "Epoch 689/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5238 - val_loss: 1.6713\n",
            "Epoch 690/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5047 - val_loss: 1.8194\n",
            "Epoch 691/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5197 - val_loss: 1.6927\n",
            "Epoch 692/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5637 - val_loss: 1.6966\n",
            "Epoch 693/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5303 - val_loss: 1.7155\n",
            "Epoch 694/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5132 - val_loss: 1.6387\n",
            "Epoch 695/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5261 - val_loss: 1.6864\n",
            "Epoch 696/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.4967 - val_loss: 1.7652\n",
            "Epoch 697/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5039 - val_loss: 1.6114\n",
            "Epoch 698/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5206 - val_loss: 1.7445\n",
            "Epoch 699/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5129 - val_loss: 1.6505\n",
            "Epoch 700/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5269 - val_loss: 1.7090\n",
            "Epoch 701/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5370 - val_loss: 1.7197\n",
            "Epoch 702/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5639 - val_loss: 1.6240\n",
            "Epoch 703/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.4968 - val_loss: 1.6793\n",
            "Epoch 704/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5424 - val_loss: 1.7546\n",
            "Epoch 705/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5079 - val_loss: 1.7403\n",
            "Epoch 706/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5155 - val_loss: 1.6493\n",
            "Epoch 707/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5443 - val_loss: 1.7307\n",
            "Epoch 708/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5506 - val_loss: 1.7734\n",
            "Epoch 709/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5296 - val_loss: 1.7245\n",
            "Epoch 710/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5142 - val_loss: 1.7849\n",
            "Epoch 711/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5267 - val_loss: 1.6696\n",
            "Epoch 712/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5337 - val_loss: 1.5986\n",
            "Epoch 713/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.4973 - val_loss: 1.6255\n",
            "Epoch 714/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5155 - val_loss: 1.6787\n",
            "Epoch 715/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5042 - val_loss: 1.7239\n",
            "Epoch 716/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5213 - val_loss: 1.7455\n",
            "Epoch 717/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5375 - val_loss: 1.6797\n",
            "Epoch 718/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5454 - val_loss: 1.7278\n",
            "Epoch 719/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5476 - val_loss: 1.7141\n",
            "Epoch 720/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5401 - val_loss: 1.6058\n",
            "Epoch 721/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5247 - val_loss: 1.6267\n",
            "Epoch 722/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5425 - val_loss: 1.6584\n",
            "Epoch 723/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5352 - val_loss: 1.5994\n",
            "Epoch 724/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.4980 - val_loss: 1.6137\n",
            "Epoch 725/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5334 - val_loss: 1.6813\n",
            "Epoch 726/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5168 - val_loss: 1.6620\n",
            "Epoch 727/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5058 - val_loss: 1.6877\n",
            "Epoch 728/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5542 - val_loss: 1.6756\n",
            "Epoch 729/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5347 - val_loss: 1.6781\n",
            "Epoch 730/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5331 - val_loss: 1.6226\n",
            "Epoch 731/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5391 - val_loss: 1.7184\n",
            "Epoch 732/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5320 - val_loss: 1.7390\n",
            "Epoch 733/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5376 - val_loss: 1.6304\n",
            "Epoch 734/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5322 - val_loss: 1.6348\n",
            "Epoch 735/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5243 - val_loss: 1.6456\n",
            "Epoch 736/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5157 - val_loss: 1.5749\n",
            "Epoch 737/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5368 - val_loss: 1.6019\n",
            "Epoch 738/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5261 - val_loss: 1.5872\n",
            "Epoch 739/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5301 - val_loss: 1.6500\n",
            "Epoch 740/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5099 - val_loss: 1.7912\n",
            "Epoch 741/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5109 - val_loss: 1.7048\n",
            "Epoch 742/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5594 - val_loss: 1.6945\n",
            "Epoch 743/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5018 - val_loss: 1.6651\n",
            "Epoch 744/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5314 - val_loss: 1.6375\n",
            "Epoch 745/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5109 - val_loss: 1.7028\n",
            "Epoch 746/1000\n",
            "52/52 [==============================] - 5s 97ms/step - loss: 1.5087 - val_loss: 1.6896\n",
            "Epoch 747/1000\n",
            "52/52 [==============================] - 8s 147ms/step - loss: 1.5174 - val_loss: 1.7066\n",
            "Epoch 748/1000\n",
            "52/52 [==============================] - 8s 145ms/step - loss: 1.5085 - val_loss: 1.6506\n",
            "Epoch 749/1000\n",
            "52/52 [==============================] - 7s 142ms/step - loss: 1.5096 - val_loss: 1.6451\n",
            "Epoch 750/1000\n",
            "52/52 [==============================] - 7s 143ms/step - loss: 1.5148 - val_loss: 1.6562\n",
            "Epoch 751/1000\n",
            "52/52 [==============================] - 8s 145ms/step - loss: 1.5541 - val_loss: 1.6348\n",
            "Epoch 752/1000\n",
            "52/52 [==============================] - 7s 143ms/step - loss: 1.5195 - val_loss: 1.6695\n",
            "Epoch 753/1000\n",
            "52/52 [==============================] - 8s 145ms/step - loss: 1.5363 - val_loss: 1.6229\n",
            "Epoch 754/1000\n",
            "52/52 [==============================] - 7s 143ms/step - loss: 1.5501 - val_loss: 1.6036\n",
            "Epoch 755/1000\n",
            "52/52 [==============================] - 8s 145ms/step - loss: 1.5439 - val_loss: 1.7699\n",
            "Epoch 756/1000\n",
            "52/52 [==============================] - 3s 60ms/step - loss: 1.4969 - val_loss: 1.6854\n",
            "Epoch 757/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5461 - val_loss: 1.7381\n",
            "Epoch 758/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5383 - val_loss: 1.6739\n",
            "Epoch 759/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5435 - val_loss: 1.6401\n",
            "Epoch 760/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5458 - val_loss: 1.6362\n",
            "Epoch 761/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5409 - val_loss: 1.7434\n",
            "Epoch 762/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5398 - val_loss: 1.7138\n",
            "Epoch 763/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5620 - val_loss: 1.7352\n",
            "Epoch 764/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5149 - val_loss: 1.6496\n",
            "Epoch 765/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5561 - val_loss: 1.6668\n",
            "Epoch 766/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5190 - val_loss: 1.6809\n",
            "Epoch 767/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5310 - val_loss: 1.6659\n",
            "Epoch 768/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5472 - val_loss: 1.7518\n",
            "Epoch 769/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5287 - val_loss: 1.6849\n",
            "Epoch 770/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5199 - val_loss: 1.6175\n",
            "Epoch 771/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5457 - val_loss: 1.6762\n",
            "Epoch 772/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5445 - val_loss: 1.7439\n",
            "Epoch 773/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5579 - val_loss: 1.5960\n",
            "Epoch 774/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5347 - val_loss: 1.6815\n",
            "Epoch 775/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5349 - val_loss: 1.6058\n",
            "Epoch 776/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5370 - val_loss: 1.6858\n",
            "Epoch 777/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.4870 - val_loss: 1.6783\n",
            "Epoch 778/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5519 - val_loss: 1.7036\n",
            "Epoch 779/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5443 - val_loss: 1.6739\n",
            "Epoch 780/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5275 - val_loss: 1.7298\n",
            "Epoch 781/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5210 - val_loss: 1.6125\n",
            "Epoch 782/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5270 - val_loss: 1.6272\n",
            "Epoch 783/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.5388 - val_loss: 1.7182\n",
            "Epoch 784/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5081 - val_loss: 1.6168\n",
            "Epoch 785/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.4913 - val_loss: 1.6788\n",
            "Epoch 786/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5209 - val_loss: 1.7358\n",
            "Epoch 787/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5029 - val_loss: 1.6864\n",
            "Epoch 788/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5143 - val_loss: 1.7303\n",
            "Epoch 789/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5249 - val_loss: 1.7074\n",
            "Epoch 790/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5249 - val_loss: 1.6523\n",
            "Epoch 791/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5177 - val_loss: 1.6555\n",
            "Epoch 792/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5038 - val_loss: 1.6965\n",
            "Epoch 793/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5517 - val_loss: 1.6455\n",
            "Epoch 794/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5194 - val_loss: 1.6724\n",
            "Epoch 795/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5574 - val_loss: 1.7005\n",
            "Epoch 796/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5351 - val_loss: 1.6825\n",
            "Epoch 797/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5375 - val_loss: 1.7605\n",
            "Epoch 798/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5395 - val_loss: 1.6818\n",
            "Epoch 799/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5173 - val_loss: 1.7579\n",
            "Epoch 800/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5372 - val_loss: 1.6089\n",
            "Epoch 801/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5341 - val_loss: 1.6725\n",
            "Epoch 802/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5304 - val_loss: 1.6860\n",
            "Epoch 803/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5320 - val_loss: 1.7152\n",
            "Epoch 804/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5138 - val_loss: 1.6632\n",
            "Epoch 805/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5336 - val_loss: 1.7255\n",
            "Epoch 806/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5375 - val_loss: 1.7711\n",
            "Epoch 807/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5337 - val_loss: 1.7386\n",
            "Epoch 808/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5468 - val_loss: 1.6989\n",
            "Epoch 809/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5599 - val_loss: 1.6547\n",
            "Epoch 810/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5258 - val_loss: 1.6450\n",
            "Epoch 811/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5165 - val_loss: 1.6685\n",
            "Epoch 812/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5576 - val_loss: 1.6990\n",
            "Epoch 813/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5238 - val_loss: 1.7379\n",
            "Epoch 814/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5409 - val_loss: 1.6979\n",
            "Epoch 815/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5193 - val_loss: 1.6629\n",
            "Epoch 816/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5414 - val_loss: 1.6316\n",
            "Epoch 817/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5450 - val_loss: 1.6214\n",
            "Epoch 818/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5265 - val_loss: 1.7529\n",
            "Epoch 819/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.4917 - val_loss: 1.7487\n",
            "Epoch 820/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5279 - val_loss: 1.7623\n",
            "Epoch 821/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5438 - val_loss: 1.7229\n",
            "Epoch 822/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5372 - val_loss: 1.6427\n",
            "Epoch 823/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5326 - val_loss: 1.6385\n",
            "Epoch 824/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5430 - val_loss: 1.6817\n",
            "Epoch 825/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5241 - val_loss: 1.7312\n",
            "Epoch 826/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5244 - val_loss: 1.6255\n",
            "Epoch 827/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5067 - val_loss: 1.8149\n",
            "Epoch 828/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5620 - val_loss: 1.7602\n",
            "Epoch 829/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5418 - val_loss: 1.6392\n",
            "Epoch 830/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5622 - val_loss: 1.6665\n",
            "Epoch 831/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5129 - val_loss: 1.7042\n",
            "Epoch 832/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5570 - val_loss: 1.6252\n",
            "Epoch 833/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5322 - val_loss: 1.6936\n",
            "Epoch 834/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5434 - val_loss: 1.6922\n",
            "Epoch 835/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5427 - val_loss: 1.6383\n",
            "Epoch 836/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5164 - val_loss: 1.6627\n",
            "Epoch 837/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5366 - val_loss: 1.6527\n",
            "Epoch 838/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5690 - val_loss: 1.6526\n",
            "Epoch 839/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5464 - val_loss: 1.6212\n",
            "Epoch 840/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5309 - val_loss: 1.6776\n",
            "Epoch 841/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5465 - val_loss: 1.6220\n",
            "Epoch 842/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5709 - val_loss: 1.6436\n",
            "Epoch 843/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5111 - val_loss: 1.6595\n",
            "Epoch 844/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5634 - val_loss: 1.6829\n",
            "Epoch 845/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5409 - val_loss: 1.6758\n",
            "Epoch 846/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5511 - val_loss: 1.7032\n",
            "Epoch 847/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5412 - val_loss: 1.6605\n",
            "Epoch 848/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5534 - val_loss: 1.6048\n",
            "Epoch 849/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5684 - val_loss: 1.7003\n",
            "Epoch 850/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5466 - val_loss: 1.7026\n",
            "Epoch 851/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5741 - val_loss: 1.6527\n",
            "Epoch 852/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5550 - val_loss: 1.6909\n",
            "Epoch 853/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5608 - val_loss: 1.7082\n",
            "Epoch 854/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5408 - val_loss: 1.7083\n",
            "Epoch 855/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5470 - val_loss: 1.6781\n",
            "Epoch 856/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5549 - val_loss: 1.6976\n",
            "Epoch 857/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5322 - val_loss: 1.7620\n",
            "Epoch 858/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5549 - val_loss: 1.7121\n",
            "Epoch 859/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5284 - val_loss: 1.7127\n",
            "Epoch 860/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5516 - val_loss: 1.6250\n",
            "Epoch 861/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5445 - val_loss: 1.6333\n",
            "Epoch 862/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5458 - val_loss: 1.6715\n",
            "Epoch 863/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5739 - val_loss: 1.6452\n",
            "Epoch 864/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5278 - val_loss: 1.7077\n",
            "Epoch 865/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5661 - val_loss: 1.7275\n",
            "Epoch 866/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5701 - val_loss: 1.6748\n",
            "Epoch 867/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5417 - val_loss: 1.6335\n",
            "Epoch 868/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5351 - val_loss: 1.7245\n",
            "Epoch 869/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5397 - val_loss: 1.6563\n",
            "Epoch 870/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5591 - val_loss: 1.7181\n",
            "Epoch 871/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5634 - val_loss: 1.6418\n",
            "Epoch 872/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5490 - val_loss: 1.7679\n",
            "Epoch 873/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5541 - val_loss: 1.6643\n",
            "Epoch 874/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5486 - val_loss: 1.6565\n",
            "Epoch 875/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5463 - val_loss: 1.6566\n",
            "Epoch 876/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5354 - val_loss: 1.7645\n",
            "Epoch 877/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5376 - val_loss: 1.6344\n",
            "Epoch 878/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5496 - val_loss: 1.6223\n",
            "Epoch 879/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5156 - val_loss: 1.6387\n",
            "Epoch 880/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5290 - val_loss: 1.7108\n",
            "Epoch 881/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5724 - val_loss: 1.6230\n",
            "Epoch 882/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5449 - val_loss: 1.6708\n",
            "Epoch 883/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5356 - val_loss: 1.6277\n",
            "Epoch 884/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5558 - val_loss: 1.6619\n",
            "Epoch 885/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5491 - val_loss: 1.5979\n",
            "Epoch 886/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5629 - val_loss: 1.6191\n",
            "Epoch 887/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5473 - val_loss: 1.6523\n",
            "Epoch 888/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5334 - val_loss: 1.6480\n",
            "Epoch 889/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5618 - val_loss: 1.6365\n",
            "Epoch 890/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5274 - val_loss: 1.6900\n",
            "Epoch 891/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5390 - val_loss: 1.7570\n",
            "Epoch 892/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5397 - val_loss: 1.6171\n",
            "Epoch 893/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5518 - val_loss: 1.6798\n",
            "Epoch 894/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5577 - val_loss: 1.6969\n",
            "Epoch 895/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5539 - val_loss: 1.6495\n",
            "Epoch 896/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5526 - val_loss: 1.6400\n",
            "Epoch 897/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5753 - val_loss: 1.6129\n",
            "Epoch 898/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5372 - val_loss: 1.6670\n",
            "Epoch 899/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5338 - val_loss: 1.7038\n",
            "Epoch 900/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5561 - val_loss: 1.6651\n",
            "Epoch 901/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5676 - val_loss: 1.7272\n",
            "Epoch 902/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5652 - val_loss: 1.5608\n",
            "Epoch 903/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5726 - val_loss: 1.6315\n",
            "Epoch 904/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5405 - val_loss: 1.6982\n",
            "Epoch 905/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5386 - val_loss: 1.6349\n",
            "Epoch 906/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5630 - val_loss: 1.6232\n",
            "Epoch 907/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5315 - val_loss: 1.6124\n",
            "Epoch 908/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5417 - val_loss: 1.6319\n",
            "Epoch 909/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5575 - val_loss: 1.6690\n",
            "Epoch 910/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5539 - val_loss: 1.6544\n",
            "Epoch 911/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5437 - val_loss: 1.6958\n",
            "Epoch 912/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5343 - val_loss: 1.6496\n",
            "Epoch 913/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5648 - val_loss: 1.6531\n",
            "Epoch 914/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5615 - val_loss: 1.6094\n",
            "Epoch 915/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5601 - val_loss: 1.6387\n",
            "Epoch 916/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5285 - val_loss: 1.6188\n",
            "Epoch 917/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5627 - val_loss: 1.6084\n",
            "Epoch 918/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5414 - val_loss: 1.6113\n",
            "Epoch 919/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5353 - val_loss: 1.6211\n",
            "Epoch 920/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5241 - val_loss: 1.6525\n",
            "Epoch 921/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5646 - val_loss: 1.6942\n",
            "Epoch 922/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5246 - val_loss: 1.6768\n",
            "Epoch 923/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5697 - val_loss: 1.6669\n",
            "Epoch 924/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5479 - val_loss: 1.6345\n",
            "Epoch 925/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5609 - val_loss: 1.6123\n",
            "Epoch 926/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5517 - val_loss: 1.7495\n",
            "Epoch 927/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5512 - val_loss: 1.5935\n",
            "Epoch 928/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5612 - val_loss: 1.6478\n",
            "Epoch 929/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5687 - val_loss: 1.6355\n",
            "Epoch 930/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5441 - val_loss: 1.6446\n",
            "Epoch 931/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5507 - val_loss: 1.6887\n",
            "Epoch 932/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5584 - val_loss: 1.6515\n",
            "Epoch 933/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5372 - val_loss: 1.6730\n",
            "Epoch 934/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5596 - val_loss: 1.6316\n",
            "Epoch 935/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5279 - val_loss: 1.6310\n",
            "Epoch 936/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5519 - val_loss: 1.6934\n",
            "Epoch 937/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5691 - val_loss: 1.6836\n",
            "Epoch 938/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5563 - val_loss: 1.6777\n",
            "Epoch 939/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5398 - val_loss: 1.7378\n",
            "Epoch 940/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5393 - val_loss: 1.6767\n",
            "Epoch 941/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5458 - val_loss: 1.7330\n",
            "Epoch 942/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5459 - val_loss: 1.6885\n",
            "Epoch 943/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5631 - val_loss: 1.6590\n",
            "Epoch 944/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5518 - val_loss: 1.6174\n",
            "Epoch 945/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5700 - val_loss: 1.6188\n",
            "Epoch 946/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5246 - val_loss: 1.6706\n",
            "Epoch 947/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5845 - val_loss: 1.7096\n",
            "Epoch 948/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5429 - val_loss: 1.6735\n",
            "Epoch 949/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5491 - val_loss: 1.6587\n",
            "Epoch 950/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5421 - val_loss: 1.6180\n",
            "Epoch 951/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5618 - val_loss: 1.6878\n",
            "Epoch 952/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5665 - val_loss: 1.7070\n",
            "Epoch 953/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5372 - val_loss: 1.6583\n",
            "Epoch 954/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5305 - val_loss: 1.6876\n",
            "Epoch 955/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5831 - val_loss: 1.6597\n",
            "Epoch 956/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5616 - val_loss: 1.7425\n",
            "Epoch 957/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5343 - val_loss: 1.7951\n",
            "Epoch 958/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5681 - val_loss: 1.7034\n",
            "Epoch 959/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5494 - val_loss: 1.7205\n",
            "Epoch 960/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5532 - val_loss: 1.6725\n",
            "Epoch 961/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5385 - val_loss: 1.6428\n",
            "Epoch 962/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5540 - val_loss: 1.7507\n",
            "Epoch 963/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5458 - val_loss: 1.6706\n",
            "Epoch 964/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5425 - val_loss: 1.7096\n",
            "Epoch 965/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5564 - val_loss: 1.6256\n",
            "Epoch 966/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5593 - val_loss: 1.6427\n",
            "Epoch 967/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5340 - val_loss: 1.6896\n",
            "Epoch 968/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5291 - val_loss: 1.6754\n",
            "Epoch 969/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5801 - val_loss: 1.7365\n",
            "Epoch 970/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5434 - val_loss: 1.7357\n",
            "Epoch 971/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5551 - val_loss: 1.7326\n",
            "Epoch 972/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5326 - val_loss: 1.6312\n",
            "Epoch 973/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5888 - val_loss: 1.6984\n",
            "Epoch 974/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5449 - val_loss: 1.8030\n",
            "Epoch 975/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5572 - val_loss: 1.6670\n",
            "Epoch 976/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5486 - val_loss: 1.7386\n",
            "Epoch 977/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5713 - val_loss: 1.6329\n",
            "Epoch 978/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5434 - val_loss: 1.6334\n",
            "Epoch 979/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5556 - val_loss: 1.6755\n",
            "Epoch 980/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5439 - val_loss: 1.6310\n",
            "Epoch 981/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5581 - val_loss: 1.6749\n",
            "Epoch 982/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5769 - val_loss: 1.6670\n",
            "Epoch 983/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5524 - val_loss: 1.6917\n",
            "Epoch 984/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5729 - val_loss: 1.6895\n",
            "Epoch 985/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5496 - val_loss: 1.6725\n",
            "Epoch 986/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5490 - val_loss: 1.6907\n",
            "Epoch 987/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5949 - val_loss: 1.6609\n",
            "Epoch 988/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5483 - val_loss: 1.7207\n",
            "Epoch 989/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5733 - val_loss: 1.6733\n",
            "Epoch 990/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5324 - val_loss: 1.6906\n",
            "Epoch 991/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.5636 - val_loss: 1.6495\n",
            "Epoch 992/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5430 - val_loss: 1.6672\n",
            "Epoch 993/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5493 - val_loss: 1.6657\n",
            "Epoch 994/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5622 - val_loss: 1.6794\n",
            "Epoch 995/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5455 - val_loss: 1.6575\n",
            "Epoch 996/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5595 - val_loss: 1.7308\n",
            "Epoch 997/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5618 - val_loss: 1.6358\n",
            "Epoch 998/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5561 - val_loss: 1.6957\n",
            "Epoch 999/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5671 - val_loss: 1.6585\n",
            "Epoch 1000/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.5616 - val_loss: 1.6781\n"
          ]
        }
      ],
      "source": [
        "#run call the transformer model\n",
        "#df_read = df.copy()\n",
        "#df_read=(df_read-df_read.mean())/df_read.std()\n",
        "from sklearn.model_pselection import train_test_split\n",
        "\n",
        "#train, test = train_test_split(df_read, test_size=0.2)\n",
        "\n",
        "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
        "test = df5.copy()\n",
        "\n",
        "Y_train = np.array(train['class'])\n",
        "X_train= np.array(train.drop(['0', 'class'],axis=1))\n",
        "X_train=X_train.reshape(X_train.shape[0], 1 , X_train.shape[1])\n",
        "print(X_train.shape[1])\n",
        "print(X_train.shape)\n",
        "\n",
        "Y_val=np.array(test['class'])\n",
        "X_val = np.array(test.drop(['0', 'class'],axis=1))\n",
        "X_val=X_val.reshape(X_val.shape[0], 1 , X_val.shape[1])\n",
        "NUM_LAYERS =  4\n",
        "\n",
        "D_MODEL = X_train.shape[2]\n",
        "print('d_model', D_MODEL)\n",
        "NUM_HEADS =  4\n",
        "print('num_heads', NUM_HEADS)\n",
        "UNITS =  2048\n",
        "DROPOUT = 0.1 #0.1\n",
        "TIME_STEPS= X_train.shape[1]\n",
        "OUTPUT_SIZE=1\n",
        "batch_size=64\n",
        "\n",
        "model = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=NUM_LAYERS,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "\n",
        "#run\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss='mae') #org\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.000003), loss='mse')\n",
        "# model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mae')\n",
        "#\n",
        "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=80, restore_best_weights=True)\n",
        "# history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UxMyaAwpcqdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00027257792920951383\n",
            "(1660, 1, 296)\n",
            "(415, 1, 296)\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "import time\n",
        "st = time.time()\n",
        "p1 = np.array(model(X_val)).flatten()\n",
        "end = time.time()\n",
        "# print(end, st, len(p1))\n",
        "print((end-st)/len(p1))\n",
        "p2 = np.array(model(X_train)).flatten()\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Say3gnZgi331",
        "outputId": "8cb662ec-46e3-428c-a3dc-5f3adeb2ff3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.39747312]\n",
            " [0.39747312 1.        ]] SignificanceResult(statistic=0.47680083910046683, pvalue=5.697917043142921e-95) 0.3306479389605111\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.2576862680778203, 0.3827231211554616, 0.25683960585142535)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import kendalltau\n",
        "print(np.corrcoef(p2, Y_train), stats.spearmanr(p2, Y_train), kendalltau(p2,Y_train).correlation)\n",
        "np.corrcoef(p1, Y_val)[1][0], stats.spearmanr(p1, Y_val).correlation, kendalltau(p1,Y_val).correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST5WiH4KoeOh",
        "outputId": "b995e4a2-4e2a-412c-9c59-0ee632503d82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1660, 1, 296)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ksXNLzMScqg6",
        "outputId": "218ba87c-4fe6-43c8-bf76-19bc5d9ce9b8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAACfaElEQVR4nO2dd3wT5R/HP0lHuhfQUqBQEARk770EGSqKggNRcIsWBLeIW7G4tyiKIgqioIwfInvvvfcolFFW6d7J/f64Jnnucne5JJfZ7/v1giZ3z909uYz73HfqOI7jQBAEQRAEESDovT0BgiAIgiAILSFxQxAEQRBEQEHihiAIgiCIgILEDUEQBEEQAQWJG4IgCIIgAgoSNwRBEARBBBQkbgiCIAiCCCiCvT0BT2MymXDhwgVER0dDp9N5ezoEQRAEQaiA4zjk5+ejVq1a0OuVbTNVTtxcuHABKSkp3p4GQRAEQRBOkJmZiTp16iiOqXLiJjo6GgB/cmJiYrw8G4IgCIIg1JCXl4eUlBTLdVyJKiduzK6omJgYEjcEQRAE4WeoCSmhgGKCIAiCIAIKEjcEQRAEQQQUJG4IgiAIgggoSNwQBEEQBBFQkLghCIIgCCKgIHFDEARBEERAQeKGIAiCIIiAgsQNQRAEQRABBYkbgiAIgiACChI3BEEQBEEEFCRuCIIgCIIIKEjcEARBEAQRUFS5xpnuwmjicCGnGACQkhDh5dkQBEEQRNWFxI1GXCsoRY+PVkOvA06l3+bt6RAEQRBElYXcUlphvwM7QRAEQRAegMSNxnDengBBEARBVHFI3GiErtJ0w5G6IQiCIAivQuJGI3TkliIIgiAIn4DEjUaw2oYj8w1BEARBeA0SNxqhY0w3pG0IgiAIwnuQuNEIgeXGa7MgCIIgCILEjUawMTfkliIIgiAI70HiRiN0jO2GpA1BEARBeA+vipv09HR06NAB0dHRSExMxJAhQ3D06FG72+Xk5CAtLQ3JyckwGAy48cYbsXjxYg/MWAGB5cZ70yAIgiCIqo5X2y+sXbsWaWlp6NChAyoqKvDaa6+hf//+OHToECIjIyW3KSsrwy233ILExETMnTsXtWvXxpkzZxAXF+fZyYsQuKXIdkMQBEEQXsOr4mbJkiWC59OnT0diYiJ27tyJnj17Sm7z888/Izs7G5s2bUJISAgAIDU1VfYYpaWlKC0ttTzPy8tzfeISCFPB3XIIgiAIgiBU4FMxN7m5uQCAhIQE2TELFy5Ely5dkJaWhqSkJDRv3hwffPABjEaj5Pj09HTExsZa/qWkpLhl7jqq4kcQBEEQPoHPiBuTyYTx48ejW7duaN68uey4U6dOYe7cuTAajVi8eDHeeOMNfPrpp3j//fclx0+YMAG5ubmWf5mZmW6ZP1luCIIgCMI38KpbiiUtLQ0HDhzAhg0bFMeZTCYkJiZi6tSpCAoKQrt27XD+/Hl8/PHHeOutt2zGGwwGGAwGd03bAsXcEARBEIRv4BPiZsyYMVi0aBHWrVuHOnXqKI5NTk5GSEgIgoKCLMuaNm2KrKwslJWVITQ01N3TlUSQCk7ahiAIgiC8hlfdUhzHYcyYMZg3bx5WrVqF+vXr292mW7duOHHiBEwmk2XZsWPHkJyc7DVhA1DjTIIgCILwFbwqbtLS0vD7779j1qxZiI6ORlZWFrKyslBcXGwZM3LkSEyYMMHy/Omnn0Z2djbGjRuHY8eO4d9//8UHH3yAtLQ0b7wESchwQxAEQRDew6tuqSlTpgAAevfuLVj+yy+/4OGHHwYAnD17Fnq9VYOlpKRg6dKleO6559CyZUvUrl0b48aNwyuvvOKpaduF2i8QBEEQhPfwqrhRIwLWrFljs6xLly7YsmWLG2bkPMKAYoIgCIIgvIXPpIL7OxRQTBAEQRC+AYkbjRAEFJO4IQiCIAivQeJGI4TahtQNQRAEQXgLEjcawbZfILcUQRAEQXgPEjcaQV4pgiAIgvANSNxohCBbikw3BEEQBOE1SNxohMAt5cV5EARBEERVh8SNGyDDDUEQBEF4DxI3GmI23lC2FEEQBEF4DxI3GmJxTJG2IQiCIAivQeJGQ8xxN6RtCIIgCMJ7kLjRELPlhmJuCIIgCMJ7kLjREEELBoIgCIIgvAKJGzdAAcUEQRAE4T1I3GiIuTM4uaUIgiAIwnuQuNESSyo4QRAEQRDegsSNhlgDikneEARBEIS3IHGjIZYifqRtCIIgCMJrkLjREB0oXYogCIIgvA2JGw0hyw1BEARBeB8SNxpiibmhkGKCIAiC8BokbjTE0n6BtA1BEARBeA0SNxpitdwQBEEQBOEtSNxoiSXmhuQNQRAEQXgLEjcaQpYbgiAIgvA+JG40hGJuCIIgCML7kLjREGtXcFI3mmKs8PYMCIIgCD+CxI2GWNsveHUagcWVY0B6bWDlu4CxHPh1MP+YIAiCIGQgcUP4NivfASpKgPWfAkcXA6fX8Y8JgiAIQgYSNxpiibnx8jwCFmO5t2dAEARB+AEkbjSE3FJugD2Z+iDvzYMgCILwG0jcaIiltxTZbjSEOZc6+rgSBEEQ9qGrhaZQKrjmnFxlfawjyw1BEARhHxI3GkJdwTXm9Ho+mNgMWW4IgiAIFdDVQkMCois4xwEX9gAVZd6eCZC5VficjbkhBUkQBEHIQOJGQwLCcrP5W2BqL2DOKG/PhK2KWPmcETcmo2fnQhAEQfgNJG40RAed/UG+zuZv+L9HF3t3HoCtG0rPPDdR1WKCIAhCGhI3GhIQlhufQmy5IXFDEARB2IfEjYYERMyNL2HjltJY3BjLgdxzru+HIAiC8ClI3GhIlewKfvkwsO4ToKxI+32L3VJszA1ncn3/M4YAnzcDzmxyfV8EQRCEz0Dixg1UJW2D7zoDq94D1qQrjzOWA9t/4hthqkZsuWGea2G5ObOB/7t9muv7IgiCIHwGr4qb9PR0dOjQAdHR0UhMTMSQIUNw9OhR1dvPnj0bOp0OQ4YMcd8kHcAac1Ol5A3P+Z3K63f8Avz7AvBtB/X7FFtuWGuNljE3Rh9IeycIgiA0w6viZu3atUhLS8OWLVuwfPlylJeXo3///igsLLS7bUZGBl588UX06NHDAzNVh7X9gh/jrDCzl5ptT/xIYSNumLlJiZuyQuDA30BJrmPHoYacBEEQAUWwNw++ZMkSwfPp06cjMTERO3fuRM+ePWW3MxqNGDFiBN555x2sX78eOTk5smNLS0tRWlpqeZ6Xl+fyvOUIiFRwZ7EXAxNscHyf4oBi2BE3/74A7P0DqN8LGLVQ/XGMpfbHEARBEH6DT8Xc5Obyd9wJCQmK4959910kJibiscces7vP9PR0xMbGWv6lpKRoMlclqqJXCpwdy01wmOP7VLTcSBxv7x/839NrHTsOWW4IgiACCp8RNyaTCePHj0e3bt3QvHlz2XEbNmzAtGnT8OOPP6ra74QJE5Cbm2v5l5mZqdWUbbAaGqqgurFruQnV9hhaViiuIMsNQRBEIOFVtxRLWloaDhw4gA0bNsiOyc/Px0MPPYQff/wR1atXV7Vfg8EAg8EJl4gTWOrc+LO2sXEFqcSuuNHAcmPPLeUsFFBMEAQRUPiEuBkzZgwWLVqEdevWoU6dOrLjTp48iYyMDAwePNiyzGTiL6rBwcE4evQobrjhBrfPVw5LnRuvzUADnA4otiNugjSIubEXUOws5JYiCIIIKLwqbjiOw9ixYzFv3jysWbMG9evXVxzfpEkT7N+/X7Ds9ddfR35+Pr788kuPxNMoERCWG2dxJKDYZBR2+JZFKaDYCbfUoYXAqdXAoI+AoBDrcrLcEARBBBReFTdpaWmYNWsWFixYgOjoaGRlZQEAYmNjER4eDgAYOXIkateujfT0dISFhdnE48TFxQGAYpyOx6jKdW4ccUuVFwOGKPv7dDQV3B5/PcT/TWoOdGCC0UncEARBBBReDSieMmUKcnNz0bt3byQnJ1v+/fnnn5YxZ8+excWLF704S/VYe0tVQaTEze7fgZXv8qKEtZSUF6vbp7vcUgWXhc9J3BAEQQQUXndL2WPNmjWK66dPn67NZDSgSvaWMiOVCr4gjf974yDh8gq14kYhoNhe6rnifkWiiRU3xgogyCdC0QiCIAgn8ZlU8EAg4LuCH1oATO0DZJ+yXafklirJgUCYlJeoPKC7AopF+62oFDf5WcCH9YAFY1zYN0EQBOFtSNxoiC7Q/VJ/jQQu7JK++CvG3OiE68tVdhC3cUs5UOfmwD/AFZk+ZZlbhc/NlpvtPwFlBcDu39TNjyAIgvBJyP6uIeb2C4GqbSxI9W6yF1Bsr7qwFK7UuZn7CP/3bYm5nlwJXDtpfU4xNwRBEAEFWW40xNoV3LvzcD8Shf7svWiB+FFxgq5nAKsnyR/D1To3AqtOwL9hBEEQVQqy3LiBgI25UULJGqODUJjYs/IAwC+3AnnnRQs1FDfOVmImCIIgfB6y3GhIYGRLqZm8xBi7bilmvZoTZCNs4FjMjV2kgopJ8BAEQQQCZLnRkECPJ1bEXmq2o24pyX240XLzfg0gOtm1fRIEQRA+AVluNKRKezrElhsb64yDbinpgyjs30FsgpUB5PtHsUiCIAhCGRI3bsC/2y84qdDEbiIbseOgW0oKwXaunuOqrEQJgiACGxI3GmLJlvLuNLyDWLAInuu0cUtpYv2phLQNQRBEwELiRkN0CAR1o2byUqngCpYa8XNnhQnnglvq7Fb7YwiCIIiAgMSNhlgtN36tbpzDrrjRIF7G0XRylp/7C59LxdwQBEEQAQH9wmuIJVuqCmobm2wpVnzodK4JE+uGMo+dgfxSBEEQgQqJGy0JiDo3TuKIW0qLVHBHBFLhNdtlVTq1jSAIIrAhcaMhVbvOjR1xo0UatzMZV1umAB83sF0+407n5kAQBEH4PCRuNMTaW6oKyhtFcaNzTpjYHkT+eHIsedXJYxEEQRD+CokbDSHLjdrnWtS5IQiCIAhpSNxoSGD0llKBmnAV9iToxJYbLSoUu1jnhiAIgghYSNxoiPWaH+jqxg6F14CKYutzjtMoFZwRNP+9DJxa49x+CIIgiICGxI2GWGNuvDsPr5J7jg/g/aqtdRlnco9byhtBwafXA3+NBPKzPH9sgiAIQhXUFVxDzBWK/VrbuKrMTqzg/xpL2Z1q75byFr/ezv81VgDDZ3l3LgRBEIQkZLnRErLcSFf+5ThokwruQyc256y3Z0AQBEHIQOJGQ6zZUj50EfY4Mn2ntKhQ7FNBxA68xxkbgGn9gaz97psOQRAEYYHEjYYERNFbVS/CwRfKcdrE3PiraJx+G5C5Ffh9mLdnQhAEUSUgceMGfMl74nEk3VKigGK5E3RuJ7BqElBeLL3el06sM3MpvKz9PAiCIAgbKKBYQyigGNKWH7VuqZ9u5v/Gpbg2B4IgCKJKQ5YbDanS7RcsSLmsVLilCq9aH5eXSO/aX2NuLASC35IgCML3IXGjIQERc+MqspYbO26p6xnWxxEJ0vv2JdHoS3MhCIIgBJBbSkMsbqmqcN3b+atMITsZcWMvFdxYpuKgHjyxHEdqlSAIwk8hcaMhFreUf0fdqON/z0ovl7TcqHBLGcuF46XwpFvKrripAu8xQRCEn0JuKTcQ8JYbxYu+GrcU8zjvAmAyCsWNnHAwGR2ZpWu4S0idXA1s+roKfEgIgiC8B1luNKTKdAVXQk74SMXcHF8BzBwKdHwSaNBHeqzcPtyOnTfR2Tf5tyH838SmQMN+zu2DIAiCUIQsNxpirVAc4CiJDNmAYsEC/s+i8fzfbVOFMTe+4Jba+j2Qe17bfbLnRut9EwRBEBZI3GhIlUkFN8mIjB0/Sy+Xc0vlZjL7rGA3kNmPB91Sy17nWybI4uJ7rA9ybXuCIAhCFhI3GlJ1LDcyImPRc/KNM8VuqXM7hWPUWG7kRNWeWcD5XfLzdZa8c9rv04zUeSIIgiA0gWJuNERnTZfyY1RMXmBlEaMiFfzaCWDhGOEYgbhxIObm9Dpg/tMK83ETLldyJssNQRCEu6DbR62oKEP90iNoqTsZ+KngTsXcMNtcOmA7RpAK7oC4uXpMfi5uxcUKxeSWIgiCcBtkudGK4my8cXEMjKE6zOGGeHs27kUxJVtF+wV9iO0QVtyYym3XA9LuMH+1gBRcokKBBEEQboIsN5qhs/zv33YbFRdbRXEj8erFlpsgCU3NuqWMMm4vKcuNt8SBPbfUth+BbzoKs6LY+S99DVj7kXvmRhAEUcUhcaMVlRdZvc6/pY0qlGJupC764q7gdi03MvuXFFU+avlY/CJw9Siw4i3rMrHlac0Hnp0TQRBEFYHEjVYw2S+cyZ8Fjoq5K6VkS1lXxNlSQaG2Y1jLjZy48ceu4OXF7p0GQRAEYYNXxU16ejo6dOiA6OhoJCYmYsiQITh69KjiNj/++CN69OiB+Ph4xMfHo1+/fti2bZuHZqyE1YLAwZcuwm5AyS0lKW5Ebim9hFvKpCbmJsDPK0EQBKEJXhU3a9euRVpaGrZs2YLly5ejvLwc/fv3R2Fhoew2a9aswfDhw7F69Wps3rwZKSkp6N+/P86f93LFVzb2I9CL+DlTTI89J1IihXVLBULMjaPjCIIgCM3warbUkiVLBM+nT5+OxMRE7Ny5Ez179pTcZubMmYLnP/30E/7++2+sXLkSI0eOtBlfWlqK0tJSy/O8vDwNZq5Mla1QDMhbblg3jpQ4UuOWkrIYLXpefi5uRa24IWsTQRCEp/GpmJvc3FwAQEJCguptioqKUF5eLrtNeno6YmNjLf9SUlI0masNAstNgF/QHI65UegKbkYgbhxwS8mN9RkCXOgSBEH4ID4jbkwmE8aPH49u3bqhefPmqrd75ZVXUKtWLfTrJ91hecKECcjNzbX8y8zMlBznMlWpnL7DMTeigGKp7VlXlKxbyoO9peyh2i3lAaFbUWp/DEEQRBXCZ67IaWlpOHDgAGbPnq16m8mTJ2P27NmYN28ewsLCJMcYDAbExMQI/rkHJqBYyW3jb+yfC8x/Bqhg2yMoiAypC604Fdyu5cYfsqVU4u4575oBvJ8IHPjHvcchCILwI3yiQvGYMWOwaNEirFu3DnXq1FG1zSeffILJkydjxYoVaNmypZtnqIJAdUv9/Rj/t1Yb6zIly42suGHdUk7G3PjUefWBgOLs08DCsfzjuY8Aze9237EIgiD8CK+KG47jMHbsWMybNw9r1qxB/fr1VW330UcfYdKkSVi6dCnat2/v5lmqhU0FD8A4i6JrzBOF11dRIrFQoiu4GDXtF/zRIuZOQfaXbQA9QRAE4WVxk5aWhlmzZmHBggWIjo5GVlYWACA2Nhbh4eEAgJEjR6J27dpIT08HAHz44Yd48803MWvWLKSmplq2iYqKQlRUlHdeCCCy3HhvGm5jTbq6cfvn2C5TE1BsUpEKXi5fIsDjqH6P3fhhuHbSffsmCILwY7waczNlyhTk5uaid+/eSE5Otvz7888/LWPOnj2LixcvCrYpKyvDsGHDBNt88skn3ngJVtgKxYGeCq6EVMdvcSq4ZECxCrfUoQUuTc0pyqUsUYBPuKUIgiAISbzulrLHmjVrBM8zMjLcMxmXYS03PpTV4yjuuBhznCigWOL8sLE6vpTe/XFD4KUTQLDBuUKNbo0TIuFEEAQhhc9kS/k9zIXP5FOBrz6AOBX8xArbMcXXrY8PznP/nNRSlg9MSgIWjBGtUCksMtZrPiWCIAhCGRI3mmEVNxVGP76jdks7A86+pePKETccV0P2/O7tGRAEQRAqIXGjFYwoMBrJciNAHFAcCPhCLI0vzIEgCMIHIXGjFUxAsdHoxzE37iAQxY0S++e6tn3xdWDJa8DFvdrMhyAIoopB4kYzWLeUH1/I3RVQbAy0FgEy5ynvgrXwobMsnQhs+Rb4Qbp5rN05EARBVHFI3GiFwC1FlhsBZQXApUPenoVnKC1wfR9Z+13fB0EQRBWGxI1WsOLGHyvpupOL+3wrvVsL5CxcwaGenQdBEARhg0/0lgoUOOigA4dyf3ZLseRdtD9GDUVXtdmPTyEhbrZ8Dxz0YANLCigmCIKQhMSNhpjFjcmf3VJsKvhnTbTZZ1G2NvvxdZa8os1+3JKOTxAEUXUgt5SWVF6UKvzZLeUOawBboM/b6DT6yLvVakLihiAIwhVI3GgKf1HyeJ2b1R8A3/fQJpjVHQRavI2YXTOAbzp6exYEQRBEJSRuNMR8L+/xgOK1HwJZ+/iLrD+RcIMXDqqVVYSx3CwcC1w9qtF+4YBbimJuCIIgpCBxoyWVLg+vuaX8zUISFuv5Y/pFPIs/zJEgCMJ3IXGjIVzlRclrAcX+lj2jpdCo1RZ4/rCag2pzPHeea78QYARBEL4LiRstqbwoea/OjRYXXE8KJI0v4jG1VBzSH4SDyjn6m5glCILwECRuNMUcUEwXHVV4RWi4Ieam+o0a7ZMgCILQAhI3mmK23PhxnRuPxnt4QdxoJahYq0lopDb7NGNvjud2AjmZ0MTKZqwAtv8EXNEwIJogCMLLUBE/LbG4pchyowpNLTdqz7kLx5RzA5kqnN+no1w5Bvx0M/9Yr8HXd8c04L+X+cdv57q+P4IgCB+ALDdaUnmxNnnyYufX+Jnl5psOzBNG6GgeY6Uwx6x92h4qc5u2+yMIgvABSNxoSiDE3Hhw7jo9oAtybJuYOq4e1PlNrx2XXu5JMVuab31MAcUEQRCSkLjREkv7BT9LBb92Esg+re1cVMEBQSH2hwUZrI/1Mh9Zta/dHTE3rogbqXnLzXHrVGDReJkdqXxdueeAijJ2Auq2IwiC8CNI3GiK2S3lRxeMsiLg67bAV60Bo4eLAHKcOssN2w/K5d5QbnCFcS6IWU7KpSUzx/9ecu2453cCnzcDfh7g2HYEQRB+BokbLam88Hq8t5QrlORYH5cVevjgnLqgWC3FjWZBzKzlRmNx464U+d2/838v7GKO70dCnCAIQiUkbrTEElDsR6ngrFjgTPBokC/HAc2G2B+nZ6w7suLGw24pFlfcUpp9Vjj7gc3GMomFJG4Iggg8SNxoirdTwUXH/WsU8PNA3t1UcJmvaQIAq94Htk/jHwvEDWe7D3czMN3+GFaQsPN97pATB9Qq5oZ57FLMjQNuKXv8cZ/yeiNl8REEUTUgcaMl3m6/IHYxHJoPnN0MbPkO+KQRH2tx6SCw7mPg3+f5MTaWG0/CqSuAp5Ox3MTWth07dBoQnuDatOr3UjFIY7cUx/GB3RwnbV1SE/B9fJnyeinLDbmlCIIIQEjcaIjOLG58LeZm+Vv83/M7hKnESyYA5UXW564ExjqDMxlO9mJuWgwDXj6lbl9ytLxX3bzMuCRuKrdd8TYf2L32I9sxJ1bwAd+uQvWXCIKoIlCFYi2pvPCavNY4Uw5GRLDiYMt3wguer178HA0odjWuJjjM/hjNUsErPysbv+D/rvkAqNdNOGbHL87vn0VynmS5IQgi8CDLjZaYA4o97t6RQM4qIk69Zt0dHhc3ai03rFtKRriodq+oED5qxA2L1nVu3BXUfXaze/ZLEAThY5C40RCd+aLEcV4KKmaOKStuRBdONhXbZPTNGAxXU8Hj6zNPVLy+kHAVO2XPtQtuKSmXFvseaWUFvHIMKL5uu1z8fnMccGgBH/9DEAThp5C40ZLKi5IOHMq9HncjJ25EbzmbZu3pFHa1QoqdY82Wjh2j7SjHLTEhEerHcpwbsqUYJtcFTq93fv9mLu2Xm4Dw6fFlwF8j+fgfgiAIP4VibjREZxE3QIW3qxTLWm6UxE2F+wrIuQI7p9YPAIk3AXU7iwYpWKocfU0hDsTcuOqCtGf1KctXXi+mogwIDuUfF14DwmKBoGAgNFrd9ue2O3Y8giAIH4QsN1pSKRz0MKHCXyw3OpG48ShOxNzoQ4AuzwC1VVoWOA6CGBY11iJVlhsOyLvgessKrSsU/3o7kLEB+KQx8HED4Lch/HJDlMzxRefD0UamBEEQPohT4iYzMxPnzp2zPN+2bRvGjx+PqVOnajYxv4Sx3JR7ozM4e0i5i7jYUiCIuXGzuGnQWzQXJ9xSco0zlXBULKhxY5UXAZ81tdYLchYti/gBQOZWYPptQEEW/zxjPVBaoK7NBaBB7y6CIAjv49Qv2QMPPIDVq1cDALKysnDLLbdg27ZtmDhxIt59911NJ+hPmAOKdeBQ4fV0cBnhUCEq5KZZQLGKC3L354AH/3Fi1y4EFOt0cFgsOBJzs2emY/sWYy+gWAt+Hqg+norEDUEQAYBTv2QHDhxAx44dAQB//fUXmjdvjk2bNmHmzJmYPn26lvPzL5iA4gpvWG5YBCKFuVhWlAjHiWNunEXNBTkoFGjYl1ngTCq4E24TR7WCqmwpjfBE2YBL+9UfxxnLGEEQhI/h1C9ZeXk5DAYDAGDFihW44447AABNmjTBxYsXtZudv8Hc9XonW4qz/1gsbgSba5Qt9fgqmRUilaG6QrEKy43irhy13HhS3HhIBMuJG/b4hVcp5oYgiIDAKXHTrFkzfP/991i/fj2WL1+OgQMHAgAuXLiAatWqaTpB/4K/iOph8t1sKbG40axCMSMgqjeUGSL+uDkhbhxGnC0lcczYFOFzfRDfo8oTWAQlM8crR91wHDmxzZyPj28Aygq0Pe6Bv4FFz1HTToIgPIpTV40PP/wQP/zwA3r37o3hw4ejVatWAICFCxda3FVVEkFAsY/G3JSLxA3bTNGVVHB2O7ngVWdFisBV4oxotPOanjtgu6zFMCC5tRPHchDOxBfqY92D+W6wfrJWubBY+XFXjmh73LmPAjt+BvbO0na/BEEQCjhV56Z37964evUq8vLyEB8fb1n+5JNPIiLCgWDMgMNXY24YKoqFz9lU5vO7gaJsJw/INreUcW04K24EncvlzqvC+WaFlyNvS7BB/diQCCCyOpBz1oEDgO/SPm2A+zPVWMuNkivMXQHFuefsjynNBwwq6/EQBEEo4NQvWXFxMUpLSy3C5syZM/jiiy9w9OhRJCYmajpBv0JQxM9Dlhv2QsXJxdwwVJQKn7PiZvX78tvZQ5XlRhxzo3bfLsSB6HSeyQDSBzt36pa9DpTmaj4dGwSfEyWh44bWD4Dt507MwXlAeh1g7cfaHZMgiCqLU7/6d955J2bMmAEAyMnJQadOnfDpp59iyJAhmDJliur9pKeno0OHDoiOjkZiYiKGDBmCo0ftxxvMmTMHTZo0QVhYGFq0aIHFixc78zK0p/IiqtN50HIjW89Gzi0lstyYXCxCJ4XejZYbZxSEXGr34C+Bx1fKb+dIsK/ca7aHpwJ4WUEjSAtXeI1aWpOMZcrrF4zl/65+X7tjEgRRZXHqarNr1y706NEDADB37lwkJSXhzJkzmDFjBr766ivV+1m7di3S0tKwZcsWLF++HOXl5ejfvz8KCwtlt9m0aROGDx+Oxx57DLt378aQIUMwZMgQHDggETfhcdg6N55ySznophEHjNq76KiGdUvJxLjYLHeiiJ8z2UWCGBNm+3YPA3Xay2/nSJq2Pli471veU7ddUIj6Y7gCK2iULDeskNRS+Nqz3BAEQWiIU+KmqKgI0dG8b3zZsmW4++67odfr0blzZ5w5c0b1fpYsWYKHH34YzZo1Q6tWrTB9+nScPXsWO3fulN3myy+/xMCBA/HSSy+hadOmeO+999C2bVt88803zrwUbfFG40xHLTelol5FrrYPMKMmEFlsufFIGrROOYBWCUfETXC48PV0e1bddp4SNwJBo/C62PfIkc9GWZFyoUCjPXHjg93oCYLwW5wSNw0bNsT8+fORmZmJpUuXon///gCAy5cvIyYmxunJ5ObysQcJCQmyYzZv3ox+/foJlg0YMACbN2+WHF9aWoq8vDzBP/fhhYBiwYVKRcxNqej1u8NyIzvEyVRwVjSI07alxoiPwYobRwSVI+ImJAyqXk/9nsALx4D4VP751WPqj+EKasUNi9qqxkXZfPfyGXfKjxFXxhZjb06554HvugA7flE3J4IgqjROiZs333wTL774IlJTU9GxY0d06dIFAG/FadOmjVMTMZlMGD9+PLp164bmzZvLjsvKykJSUpJgWVJSErKysiTHp6enIzY21vIvJUXm4qgF3ggolrugetpyowanA3s54NndwOiNQKQTdZQ8YrkxqBNOoVFAdJLni+XJihvxnJnnat1SR/7lx2asB7L2A8eX246xZ7mxd+6WvQ5cPgQsGq9uTgRBVGmcutoMGzYMZ8+exY4dO7B06VLL8r59++Lzzz93aiJpaWk4cOAAZs+e7dT2ckyYMAG5ubmWf5mZmZruX4DALeXlgGI5/NEtxXFAQgOgprzoVTioC+LGgXMbHA5VlpvQSP6vp3s4caIgYrnXxgYRiz8bmduAL1oCR0QB/CVMttf33YGZw2wLEdqz3Ng7d2XycXgEQRBinP6FrVmzJtq0aYMLFy5YOoR37NgRTZo0cXhfY8aMwaJFi7B69WrUqVPH7nEvXbokWHbp0iXUrFlTcrzBYEBMTIzgn/swVyj2ZONMmVRwOauDWNxoFjTqRreUqnEKY2q2VHkcZ45bidqaOKFR/F9ns6ucRSxmVr7LLxMvZ11RZqFTWhmE/vswIOcMMHu4cJuSHNvjXT4sfG6ujH1+F7DpG/UuLzOe6MFFEETA4JS4MZlMePfddxEbG4t69eqhXr16iIuLw3vvvQeTAxd1juMwZswYzJs3D6tWrUL9+vXtbtOlSxesXClM312+fLnFNeZVvGK5UdEziMWXLDdqcTXwOLUbcMfXwCP/ue+4IeEq3VJestysfFf4fMNnwIXdsBFw4nYcqyYB6bWBY8uAchnrSXGO7bKcs8I6OebYrhlDgGUTgZXvCMfbO3dSn/Mrx4D9cz3Xn4sgCL/BqQrFEydOxLRp0zB58mR069YNALBhwwa8/fbbKCkpwaRJk1TtJy0tDbNmzcKCBQsQHR1tiZuJjY1FeDjfvHDkyJGoXbs20tPTAQDjxo1Dr1698Omnn+K2227D7NmzsWPHDkydOtWZl6Ix5gu8D9S5kQ0o1lDctHqAKavvZNsGVbhwLs2iq+1Ix/flaMyNGsyWG0/H3ORIZDFKpWeL3VLrPuIfL35Rft8lEkUIr5+WbsRqLli4+TvgFlZw2RM3Evv6tgP/NzQSaDxIeXuCIKoUTt0+/vrrr/jpp5/w9NNPo2XLlmjZsiWeeeYZ/Pjjj5g+fbrq/UyZMgW5ubno3bs3kpOTLf/+/PNPy5izZ88KOo137doVs2bNwtSpU9GqVSvMnTsX8+fPVwxC9hj+GFDsiltK7+DHx9GYG3Nm1IB0x46jFY6mgsu9F40GWB+bLTdXDkuPdQkHBaapXMItJdNIVa7qNABcz7BdVlYkKgIompv4c+eM5cbM+V3K2xIEUeVwynKTnZ0tGVvTpEkTZGer703EqTAnr1mzxmbZPffcg3vuuUf1cTyGuUKx1wKKVaSCi1O/XUkFZ8WKrJtFZ52LozE3fSYCze92rMeTlriaLfX4St71k9IJOF4ZeG+otNy4o5eUPsix/UoF6Z5aY33MCpCgEEiKpy1TgHPbbJcf/h/Q7C7rc2cbslrmovRekFuKIAghTlluWrVqJVk075tvvkHLls4GbwYCTECxx4r4ORhzI774GV24yArEjcwYNnDW0TgTnV69sJEVyuJ+Vg5cCB3pzh0SbrusTnug4xPCC7vZLeUOHHV12ctAYj8bcvte8qr08vJC4I/7HJiMC5YbgiAIEU5Zbj766CPcdtttWLFihSWQd/PmzcjMzPSdPk/egAko9kj7hezTCjEzasWNRpYbOXWTcANw9ajEeNgXGp4OuhUjblWhRLAB8hdoVtxEujIjZfTBKioBM5QVQrm3FPPZku0XFiQdD2Mzzo7lxhW3FAUUEwQhwqmrR69evXDs2DHcddddyMnJQU5ODu6++24cPHgQv/32m9Zz9CM82H6hKBv4qrU1qFKM2h98V2JulMTHyIW8W6nlPQrj7YkbdwQpO3AhHPwVEBqtbmxwmPw5Z1+3u8WNI5QVKn9OBDE3MuJGrimpzb7sfR8cCCjeM0sUxEzihiAIIU5ZbgCgVq1aNllRe/fuxbRp03wkc8kLMAHFRndbbrJP2y6Tjb9xEwK3lEiINOjF/9v4pfR4QIXlxhFxIycsXBBI7UbxWVbvxNkfq+Q+E7ilVIolZ3C0do49y41RFFAsdS5DwoGyfNvlYs5sABa/LL/eEcvN/KeBhreo35YgiCqHl+3+AYbl4u2BgGK5TKXiHODr9rZ1TdyBGrcUa01wVGh42y0FqJ9zUChQ/Ua5nVgf+pTlxo7brYAplim3b6lYIzm2/aBu3N7ZwPTbgcJr1mVit9QJiRYP3qQkj6ooE4QP4QNXj0DCgwHFchf+HdOAa8eBfX9Kr3fXHOREgE4poNgDMTfiQNjBX/F/+0x0fd8s+mBg6I9Ai3uBJ1aL5uApt5Sjlhs74mbBM8y+5cSNSreUFBkbgfM7K58wn4V5T/F9qtYwJQAUA4qduJHIPg3MeQS4sMfxbcWUlwCTU4APapEViSB8BBI3WuLJgGKpiw1ncrysvSuwF+2b3+D/WorlVeJKtpQrhQF7vwbE1QV6PC9c3vIe4NWzQC8FF4kz6IOB2Dq8wKndVriOjV1xp7hxNFuqvET9xVg25sYBy42Y6bcCP94sH4/DtnXQOqD4r5HAwX+Aqb0c31ZMzlnrY6XvX0UZf84JgnA7Dtmx7777bsX1OTk5rswlALDG3LgcUHztJHBkEdDhcekLotSFjDPBJUHgKKxYafcwcMPN1sJ7ljE66ceq9u+KuHmF/yeFs400lVCympQXWR+7MxXcUctNRTFUWz10QZD8bDnqCpNCLsOL/XwpChgnxM21k45vo+b4ciLswm7gt7uAiOpA2jbHC2ASBOEQDv0yxcYqXxRiY2MxcuRIxTEBDWu5cTXm5ut2ADgg7yIwaLLteskLGedRbWMjXOLrSYxRKPSnpQnf2+4ApYs8G4sRHOrZOXR+BtjynfT4ilIHLDcyr08LS1R5sfTyihKg+DoQHq9sEXHmvddClFmOzwgaubT4A3/zr6X4Oi8qXTlv53by/bn6TwLqtHN+PwQRwDj0Df/ll1/cNY/AQGeNuSl3uf1C5Q/22U0ObGICDi9y8bgOoMbNpFjF2N5FyZNKzcVjKl0sUzoCUUlAvP3GsC4hNYf+7/OBzovG266TExWS+5axCkXWUL8POaR6XAHAoQX8v1fOaF/ET8uu7Ky4khNh7O+Bq67jnwfwJRx+HgC8edW1fRFEgKLh7QvhlsaZcnelUj/2F/YAF/doc1w1OCpuXKkW7CmcDWJWEjch4cD4A5UtDNyI1Bx0evm5ndlkGx8ku28ZMWC2VASH8ZYWZ6iwI7Ky9msfUKyl5UbglpITLqwAcrH1hrk2lSs1qggiwCHHr5YIAoo1utM0VQDHV9h2XpYSBqfXanNMtagJYFXVf0puWw3q3DiKs3E+9i6WwaFuKkrIILV/nU5emFQUA2c2qtt3xgbp2BizFaJBH3X7kcJekK0+WPuAYre5pVS0Q9n5izZZWgRByELiRlPYgGKNLraXDwEzhwIzhgiX+0KvHVWWGxdSwb2ChEB4djcfu6KElm4OrXE0i0qK4uvC52ZBYrZUhIQ5v297lht9kLafd2M5kH9Bu/0J3FIy82StNSvf1SZLiyAIWUjcaAnTFVzzOjcXdokW+IAwcDXmRuolvMxWXvZCzI2U9SOhAdD4VuXtNHVzOICa47pDeC2pzEQzX8yDXOjcLhdzY8Ze/ypHLTcbPndsvD3UBBSTC4kgPArF3GgJE1Ds9jo3fmO5cdAtxdZNccSNE+yC5UCAXKVlOwLBW+Km+o1A56eByERg1XvSY9xR6XnndP7Y5ou5K/FEvwxSXq/TaZsKvvcPx8bbgw0QlvteuhpnQxCEQ5DlRlMq3VI6DzTO9EQwbvUblV0aasSHIF1cjVvKQWvN0GlAtUbA3Rr1M5OttGznq+I1y00QXzix8UDh8s7PALd/UTnGTXNb+pr1wh7kxhR3nU5ZzLPCIesAn3atBNvWQQtYq4xcJpSRxA1BeBKy3GiJlnVu7OEJy81d3wO12so3jnTYciMjHKo3Bq4eBWq3U9eviqXFMP6fVsi9JrvixksxN6xwYQXvQKZ1gTvnZsmWcsEtZQ+Tncrbmdv4f2c3A8vf5JdF1gDq95QeX5orvdzp+THCRdYtReKGIDwJiRtNsV6M3e6W8kjMjU7ZOuOquDFfjB/6B9j5K9DhMfdnFNlDL+NeEVuwWj0AXD/NX1AB34650SKgWA6TBm4pe3BGZTF/cQ8w7RbhsksH5cWN1rDCRbbODcXcEIQnIbeUljABxeeuF4Fzp+vIE5Ybe0JDjbhRYzWIrQPcPBGIrgmBtcaTQmfoNP5u/wGZhqPiudzxNXAj4wpyVNy0vM+x8XIIstGcjBdyBU6DgGJ7mOyIG28jsNzIxdx4sOcbQRAkbjSl8uISqgeuFpTh1NVCOxu4gEeSpTQQN4pjJF6EO4Jf1dBiGPDicaBeF+n14nnpdEJXjKPWkTu/A57e7Ng2Usi5pVjceU4t4saNMTecUTlbytuoCSg2kuWGIDwJiRtN4cVASgKf8bP9dLb7DuUvlhulMVIXY8ExPeyiUnq9NtYPndAV46jlJigYSLrJsW3MdH1WYV4SuNNy4wm3lDOWmyWvApNq8X2YtJqD7DrGcpOxHsi/JDGGxA1BeBISN1pSeXGsXy0CALA947rSaBdx0XSj6m7e3ZYbqfFeckvZQ8pyw7pinBUQ9/zq+DaskFJV50ZiTIPejh9XCk8EFHNG+eJ4SpQXArPudf34JXnA582BeaOl17PiZtFzwFetlccQBOF2SNy4gWqR/F3s1QI7xclcwVXLjSph4gW3lK8idjuJ3VLOBhQ3GwLEpzo4F+Z9ERxXzi0lIby0CjL2lOXGXhVjOcpUuIbtxcMcmMtXNJarjyMWLuVFtmMoFZwgPAqJGy2pvJAH6/mLj1tr3bgsbtRc3OyJGzV1bhx0S/kqUq+DjTNxJVvKXoVeJdSICimrUnic88dksRTxc3NAsbOWDzXfE2OZ8nqlz7C9NHXLOBI3BOFJKBVcSyov9sE6/qLtXnHjAbdUtRuU16txxaT2AGo0BWo0lljp5+JGC8sNACTcAORfdG5bNe+BlJDt+BSQ2h2ITgb+uN+5YwNM+wU3BxQ7va0acVMurIxtg4yIzz4F/HgzEBZr/xhSMTcmE6Cn+0uCcAckbjSF/xEMqvwtLKvwZcuN6Ee1dnvg/A7+cZsHgYGT7fzgS+xDiuBQ4JnNTsbP+FDMjdRFSGC5ccHNc9f3wMp3gP1zVG7AuqXUWG6YuXcZAzQfCtRuC9Tt5NA0JbHE3LhR3LiSRq3me6I0pqIMOLfN+rwkj6+A3HQwsPwtvqGouKmoFFKWG84IMp4ThHugb5aWmC03lW6pMrdWKdbYcsPGJoREAoZoNTtReSwfEinO4k63VFwKMPQn57ZV5ZZi5hZZnRc2WuGJ9gtq4mZkUfE9URI3i18Adv9uff7vC8Ci8cDvQ9Xt24xUzA25qgjCbZC40RSh5canY27ElghTBZ9iHJ4AdB+vbh+u1k+x51rzJU3kTreUHPf9Lr2cFZ7scW/9mP/b6xXheEGhP42/8mZ3izvFTWme89uKvydSViAly9CuGcLnh+bzfy/uccw1LCVkpI7Lcby7y5/i0QjCByFxoyWWgGL+qXvdUq7++ImUg6kC6P8e8NIJIKaWyl24++PjQ+pGKm5FK8uN7DHF6ed6oM9EoFpDZg6M5Sa1OzAxC+jzmnA71mWmdSuG0oLKebhR3JS4IG7EzH7AdplDNwrMZ9KR7SRjbiQEz/I3gK/aAOs/dWBOlZRJZGkRRBWFxI2W6PzIciO+cJpjJ6RiR6RETExtDYrD+dHdqdQ5ENSbcUehPJG4u/UToNfLwrmIY26k4qRYQaMm+NURCi/zf50VN7Ep9seU5Di3bymOLbFd5kjAMnvuHRI3EsdYNtH2JmXT1/zfVe+p3zcAbPsR+CAZOPCPY9sRRIBC4kZLKi92wTr+h8yt4sblmBux5UbhB158YX9sBfD0Jve5pSKq8X9rt3Nt/1oiKW4Y0eAWy43oPWo7ynYuqor4MeMja7g+LymcDShWI4o2f+PcvtXiiEjRqbTciL9PUu0Xdv8uLbacYfGL/N+5j2izP4LwcyhbSksqL3ZB4H/0Sn3aLSVCKbhRfGFP6SC93GFkXsNzh4CKEu1qsWiBPYuWW7qCMxfSpnfwLRvExw1ScVzWchPlJnHj7Ot3Z2VjtbBCZOFYPqB+0GSZwSrFjbFc+JmR+34VSLRqcJV9c4CW92i/X4LwI8hyoyVmyw08YLlxVdyIt1cSNze/Ib3cXVlQIWG+JWwASMb/sKLBHW4pOReIwGLkYBG/yETb9VoU4HM2lseVysa9Jzi/LYvZLZV3kQ8g3joFKM2XHqvaclMOFF6zxiTJ9ZZyR4+4DZ9pv0+C8DNI3GhJpbgxW258us6N2GqiJG66jrU+DmUyddydLeVLSAm52DrWx2pEhivHZN/v4DDrYzXigK3AK+WWikhwfG5inBV3rgQia9Xywfw5ZL8DRddkBjPvidJ3pvAq8HEDYHJlTFGFTBVkd4gbzgSs/gCY9zT/2soK/eu7RhAaQG4pLbGIG/5O0MQBRhOHIL07LBxau6WUYm50wMiFfADk7V9al1eXqjrsCH7+gxsSBrySwVst3FJplr2QMu8PK27UuIPi6wONb+Nr3ISE2a7v9zYw7ym+eOP5XcDlQ05M1Vlx44LVSKsMLfO5Zdtg7JsjHcjMCs7T6+T3eXEv/5cz8VYgud5YZtGRf4l3iWmByQis/ZB/fGN/YM7DwI0DgQf+1Gb/BOEHkLjREpG4AXjXVJA7XBau3vE54pYCgAa9gNEbhMuSbgKG/wnEJLs2F38mPN6NO2feI/b9DnHQcqPTAcNnya9vdT+Q0gmIq8vf5R/4my9U5wjOfsZdqWzsiLhRanVgPrfZJ63LVr/v/LwAobUsP8v++CWvAMeX2h9XlG1/DJv9tXUq/1erwGWC8BPILaUlooBiQOOg4gP/AL/cxscGuGxmdlDcyNF4IJDcyrFt7vgaCA4H7v3NuWN6A9YF9+xuzxyTfU84Fyw3akioz39+w2J4IesozrooXWmtoNYtNfcx4MuWQEmu9HrzuZ11r/19qY0zY61Aeeflx5mFVd4Fdfv99wX7Y6Qys+TY/B2wyc3ZaAThBchyoyWVpnk9Z70oaRpUbE7zXPku0HiQa/sSayNPloJvOxJoPcJNtWHcRHgc0OJePjA0vr5njslepGTdUm6I9XHG3ePse1n9RuD0Wue2VevSOjCX/3t4kfR6hwSWWnFTYn2ce05+nFncqJ3DmY0qjq2yy3xJHrC0Mii7zYM+GMRPEM5DlhstqbyL1pmMCKms5OeWjKnyIvcEInoSfxI2Zob+CNwz3XO9stgMG7mAYnfE+jgTB8PG3PR4Ebj5devzx5YDT0nEpyS1ALqOcfxYZhwVYXIZUJwJ2KXSiqj2vWfdUkqNNc0WWLlCguZsK/F4xWOrFDfsHMU3N8eWArNH8IHRAHBhN3D1uLr9EoQPQOJGS8wuAlMFQoP4U+uWjClDFFwPxvXzYN6qANtsUSBuGPFhcsPny5ksJHYbnU4okOLr867LbuOty8Yf4AVPsESAs1ocjdfJzZRezhmBhSpFltrzzVpu2Me2B6/8I7HfM5uA9NrA1N7AKRnrVsFliWPLZGbZHFrhtcy6FziyCFj2On+Mqb2Bb9qr228gkX/JMTcf4TOQuNESszXCVAFDCP/YLYX8QqM0CCj2c8tPVcCkwi3lSOsAtThTWI+N/eFMQguH+XvBLotL4a1OrvS6ctRyI1cwzxGBKFevRgwrMJTEhsUtJTGHZZX1pS7sBmbcUXmRFd2ULHxW4tiMmFL6fLDWGjm32LWTQM5Z+X0EMpcOAp/eCEy7xdszIZzAq+Jm3bp1GDx4MGrVqgWdTof58+fb3WbmzJlo1aoVIiIikJycjEcffRTXrsnVpPAwFsuNEWGV3TNLyt1w8QmN0r6IH+F7sHeMoZHWx2xVYnfESjkVc8OKGw6C2BS9RGVlyzpXxI2EhalmCz6DTwq5O3BHhL5RpVWEdQ0pWW7Mx5YSIeL6Q+XFtt/b8zuEFj5+Z8w8FObLrpMTQWIXV1X63dg/h/97wUMJBISmeFXcFBYWolWrVvj2229Vjd+4cSNGjhyJxx57DAcPHsScOXOwbds2PPHEE26eqUrMP+KcEWGh/I92cZkbxA1n1OZHRq7yMOEbmCqAu3/iL9i3fSI/RmvkBEf354ERf0uvEwgNTsZyI/Fz40ohSCkRpg+Rn7/cuXLE+qX2fG/+zvpYSWBYCghKiZtqwucVpbCx3BjLgVKFruk2wkdmndzrqiiFbL2lQMcdwfr+ismkPlDdR/BqttSgQYMwaJD6rJ/NmzcjNTUVzz7Lm2Lr16+Pp556Ch9++KG7pugYTMxNWDD/A1viDrfUhs+BXq+4uBMO6Pmi492HCc9hquB7BCn1CfJklps+SD4eh70QeMxyI+E+CwqVD/qVExnuuGBrYbkJF1luKkqk61Mp1b5RcqMpBRSzx2ThjKgySbZaFYkMBKbfyluwXjzOl4vwA/wq5qZLly7IzMzE4sWLwXEcLl26hLlz5+LWW2+V3aa0tBR5eXmCf26DibkJd6flBlCf3SFHVTIv+ysGFT8i7ggolkMXJC9G2OXimB1LXI2d/lyOIiW0gkLk9yknMtx9R6q0/5XvAGs/lrHciApELn8DkpYbpWwspWBYuZgulooy4dvmSTHtbbRq7+EvKFn5zm7mvz+nVntuPi7iV+KmW7dumDlzJu677z6EhoaiZs2aiI2NVXRrpaenIzY21vIvJSXFfRNkYm7CLQHFbhI3WveWInyLlvcDLVUUlXNHQLEcOoUAYJ0OuOU9ILk10PkZ+wHF4nXOIOmWCpbfZ7lMC4TyIufnoAbFbCnw1ZCl3kfxuT44T8Jy46C4KSuSXicnbsTWLndYuXz1RqsqiZs9s4D0OsCJlXYGeqgMhgb4lbg5dOgQxo0bhzfffBM7d+7EkiVLkJGRgdGjR8tuM2HCBOTm5lr+ZWbKpINqAeuWCuFPrdssN5TtFHg8vpIvbvjCMeDuH5R/XBv15/+2fsA9c+kv0X5Ap1cWI92eBZ5aW1kMjvkRNIsayZgbjcVNUKj8PstlRIac6NEKNanZUhaWYgl3k3iunEl6nBnWOrNzOvBBMrD798pjyril2PkGhUJ1s1BnOPofMLkecGSxtvvVAvbz5asCTCvmP833P/vjfuVxnqrxpQF+5TxNT09Ht27d8NJLLwEAWrZsicjISPTo0QPvv/8+kpNtexwZDAYYDC4053MEgbipdEu5I1sK0K63lE5PQslXqNOe/6eG4X8C5YWAIdr+WGdodjdf44RFp3NNjFRraLtMSSw16KNsBpd1S8ncs8lZaLxtuQGkXVebvrZdJlWgT63l5n/j+L8L0viKxALLDSNaygutj8XnWOm34sJuvkxBYlP5MWLMF9PZw4G3ZdpjeAtW3BjLXeuD5i/YbedC4sYtFBUVIThYOOWgIP7HkfMFZc2KG0NlQHG5m4SDy+6IyvM1ahHfEfpWmWwcwjfR690nbIDKQpHiYwa5dufW7C6+FQEr4KT2FxwOPLmav0ju+QOYL2OZlarH44xbSsnyoQVq0sfVpphLIdczC1COuZFzS7HnSXzzI2e5Kb7OF/oDgLdy/OoOXxZW2FWUVF1xw15b/eh99aq4KSgowIkTJyzPT58+jT179iAhIQF169bFhAkTcP78ecyYMQMAMHjwYDzxxBOYMmUKBgwYgIsXL2L8+PHo2LEjatWq5a2XYcUSUGyNufF5y01qN+C5A67PhwgsQiXEjT23lGCsVPCwjnddKaEPAV49a72QKLnmpH6IlQKK5Sw0K99VnpMaarYEmg8Fcs4AO34WrjNbbvQh8tlLrgQ1K3UdV8qWMklYbspLgEMLmeVGofCRi7nJuygcE+RX983SsJ8vP0uDdhopq6fgWuM/4sarMTc7duxAmzZt0KZNGwDA888/jzZt2uDNN98EAFy8eBFnz1qrYz788MP47LPP8M0336B58+a455570LhxY/zzzz9emb8NjOXGnC1V6jZxI2Opuv0LPnbD/g60nA0RaOiD+D5aLZg0dKWAYhuc+BFsdhcwIdP+HXLbkcC4vdI/xEGh8v221LiHnKVaQ6D7eCAyUeK4lRdGpVYTaisfS3HliPw6R4v4LX4RWPKKcDlrJZaz3GTtsz/G32B/YyvcHJflK0jdvLAWPlfqUnkYr8rr3r17K7qTpk+fbrNs7NixGDt2rBtn5QKCOjeVAcWettzog4G4eiq2J3FD2KHZXUBEdWulVqVUcDHOmK9DIoCQcNF+JH5MazQB4lOl67vogyUEmA4A515xY7YwhUgIGIu4CQVc8D7JcvmQ/DoloSEVc7NbVGLCVCG01si5w+c9pe6YjnDtJJ+C3Gq49o12Dy8C9v4B3PkNEB4vPYZ9rVXFciNlDWXfT3JLVVEYcRMbwd99Xsl305dCzjys06tMYSRxQ6iAFRcOWW6cOZaUK0uq8F/l90zqgicVUBwUKgzEDYsDSnKcnaU05jmFRNiuM4sqV5qEKqEUc6OEXEAxi6lCZLkxAleOAgkNrL8z4rgercTN120r918GtH9Um32a+XME/zemNnDrR9Jj2BtIrYTxP08B104AD//Lx4z5mliwJ278CP+xMfkDTBG/BtX5XkD/HcjCphNXtT+WnOVGp1MnbshyQ6hBLG60voOWO5aaZVJCKyjUdo7i74PdjBAnMB9TbHkCrO4fZxqSuhM1RfzElps9M4FvOwKzR1iXlRUKt9E6+/LMZm32I1XwMu+88Hl5MbD9JyD3vEjcOHCTmncROLRA+pzum833A5uUBPw2RP0+PYXU91vwOnxMjClA4kZLmCJ+DWpYGx1+u+aEzAYuICtu9NQThdAOVlzo9eqtDzcN4f+m9nDuWGqWSf0QS7mlxOLGHcXZzN/9YAlxk18ZbCvVLsIVGvR2bXs25kYq7RzgL2zsb83GL/m/x5dal4mz0LS+09dCLC2dCLwbDxxbKlwu3veyN4B/XwBmDhNe1B2x3HzfDfhrpG1guZhTa6SXlxaoP5bWSN0wsCLYj8qGkLjREsYtVSfeap6umxAps4ELKIkbcksRWiG23ETb1pKSJKoG8NpFYORC+2Mt+5f4YVWqaiznsrKx3IgClIPDgFiNK5XrFCw3luM6KW5iaksvD3Hxd4Utt396rfQYcbaU+HfnylFg6/eibbQWNxrELW7+hv87614+lseyb9Hr2TOT/3v5kPOWm6Jr/N9jSxyfZ8ZGIL02sGSC49tqgaTlhnk/PVkR3UVI3GgJI26C9DqM6cMXLdO7w5KnJG58zY9L+C82bikHfjJCIxwbLyVWpArCmS0gUmKovMh2P2JLZkgEMHYXULeL+rnZQ8ktZcZZcdP6AaDD47bLQyXiexxBTW0dccyNmG87Ahu/sN1GS8TunW0/AvOfcb4VxMU90vvOOSssF8D+xjpjsRBvo6YP3Iq3+L9bvlMeJ3tMDii47Ny2ZnbNEApA9v30o/gbEjdawogbAIgK45+7p5CfjOWFhA2hJezHySwaIqq551hSd40JDYBHlwJjdgJdxwK12vJZXHLji3MkAopFMTahEXzmUlisunm9lQN0GaM8xnxMd4gbXZC0O1AqeBkAEm5Qt1816efimBtV+60ASvK0i+sTi4TFL/IWFmcsI4BQFLPCTdwCQk3xQiXE81Zj9XBVPKz+APikER835AzXTgALx1qDuQGhhc8dvcXcBIkbLWGK+AGAoTIdvMRdzTOl8KM6BIQfwAbfmi8KozcAQ76XruniCnKf3bqdgeoN+X5XT662pltLCfmSHNvlYreUWYCozfxSc8OgStw4mS0l5WoOiwPiZFxraduAV84o73PL98D5XcJlkpYFDtj3p/Q+MjbIL5+cYtu+QxIV55bjgL1/AgdE9cz+e4V/HY7CimJWgEQkWB9HJYnEjRO/4TaWGxXCxVXxsK4y8+vfF9RvY++YArcUxdxUTUSWG3N/KbcV8pOCxA2hJVFJ1sfmH7aYWkDr4dqnNmvx2ZVKixYH8ppjVeREy8gFtsvU/qjLWVMA5y03er2tQHtqrbARaJ2OzPgg+1apJa8ARxYJl8lZcsTjzEy/TWbflfEi5jgXMSdWAln7+cdq3vOrR4F5TwJzHxEGL+dmCgsOqsUkYYkovi68yAcbRJWZnbHciDu4S+zDZowbi76W5kuvsxdPxM5bqWCkj0FXQi2xETf86S2t8KTalfjB7v488PJpD86BCBgia1gfF4lKGmjtAdXCpVqng+0FQ+yWMltX5NLaG/TmG4c6gvkirST4nM1ilLLc6IOB9o/wBeg6pwF932DG6ypLQjjYC0mpD5UjlClk+1w5Cvx+N/B9d/65GnFzjck2PbPRdr34/T63A1jzofzrYbuecxyfAfZhqjWYGODT27V2S0kJF/EyzQJ2Rd+lxS8B6XWAzG22Q+Vir4wVwD9PAovGW5et+9i72VwOQEX8tEQsboLNzTO9aLkxxAD93vLc8YnAghUA4h5GWlsJXdlf/V5Ao1uAdo9Ys1XMiEWFORBXyS0lvsjYix+xuKUULDfOXrh0Qba1eXRBQEwy8NJJ/j06vc52uyCDYw05XWkBoRaxK0wf5Nhxfx9qu8zcy4rjeFH3U9/K5eXAzRKuMbago7EUWM63+0HGeuvysiJhBpkmAcVS4qZCKL61styIbxS2/8j/nXYL8GomEBZjXScnAq8ckXZJFl2TbqwLAHkXeEtQQn3H56wxZLnRElHMjdktpSqguKIMWP6WvB9bLeILhB9FtxM+TmR14fPa7bTdvyvVj+t15QOODVG2P+xJNwnTps0CRPxdCY8HnljNP7a5yNgTN5XHVIq5Yfc57BcguZXyPi371tu61sy/Nea/UkHejrrB3OESybsgfF58XTRAA2sdZ+TF3Yf1gP1zrcvXfQzkZNqOZ90wclaIimLgxArrc1csN6fXARf3Sotb8TKp4+SeB85udfz4cmyZIjqmnIVLpraP0k3IZ02Br1rzgf1ehsSNlogsN5aAYjWWm20/8CmVcn5stZC4IbTm4cVAx6eATk8Ll9/6CZ9F9PQm1/ZvLvTX+gHn98HGBkXVtD5u/yjQ7x2gNpP9YRY3YrfU0J+s42zcBWotNyrFTb1uwIPzlPdpRh9k65YSC8GkZnzA9TCmcJyj4kYrtxTLL7cKn4vbXmhh/TMZgd/u5uOt/n5MuO7QAtv3jhU34urKssdwUtzknAV+HQz80BNY8qrEflW4pT6/Cfi5P3Bhj+NzkKI0T/hczronK25kBCn7WnLPOT4vjSG3lJaIxY05oFhNzM3VY8LnEdV481/v14A1H8hv12gAUL8nsGwi/5zEDaE1qd34f2IiEoABk1zf/8gF/A+uXANDNdTvaX0cHApMOMd/F0IrLTY1W1jdDnKWGwMThOuoG0KparIZ9g5ZHwxEqkyp10kEFEsdp6uoobCjMTfucEtdF8X6sXf0HKeNuOGMCtaHYlvRxtayUYoPYlFr1WKFFGcCLjMBuAf+ltiv6PdZ6TjndwC1WqubhxJisWeUuUbIBRqzc758BNg2FejxgtB66AMlSchyoyWsuOE4S0CxUzE35i9909uty6o3th0Xkww0Yaw94h8LuR9p8V04QXgLfZDzwua5g8BT64BqotouhmirsAGEsTByqeBsHILNHbRKy40SYXHWx4706NIFqRM3Yhx2S3ngRoh1S3Em7Sw3cpSX2Fomcpg0edXiRuW5YY/FmYAymQwldozgOAqvxaGeaAriwuaYMsJQzqLDzvGnvsCOacDfj4vGk7gJLET1E6wxNyrEjVhNW54zHxIpv7pOL6oiq/JD1f89deMIwpeJraMudoW90FvEjWiMIdr6uMMT/F9z/ya1bimAD2qu2VK4vs1DQIth1ucOiRuJZrhq4pMczpZyk7hhLSesq8NUoc0dvpKVrbzY9uK9+3dmbioDrtVa8tj9XdhtP/ZE3JdLKehcq4av4tci65ZSYbkxi8PzO4X7IctNgMF++EwViK6sUFxQWoFyo6PR9pU/puyHRLLcuk74Q6n4oWLWuaN5IEH4KuyF3ix0xHrFwFhuGg8Ent0NjKgMULV3cWPFzeAvgNHr+VgfAHhoPnDnN8LfB0cuVHpnLTcO1iFyV7aUXH0VU4WEpdmJqsZK1g4pt5QzqLHclBUCO6cLl7FByVJ80RxY8ba64zhSSkDpOsAZ+QKI6z+rrH8jY706tVp6uWS9HpNQ3PhAJWMSN1oiEjfVIw0IDdLDxAFZufa6yipYbrqOBZoPtb0bBCQsNwpvqQ+oaYLwCqzlxnzRFwsW1o0F8K0fLDcBzPezsVTQv1R9qfHAxCzghj6Vu2D24Yi4kYy5UbG9LwQUA/IXOlOFUKRdPgJ82gTYOtWx/StZO0oLHEuHl+P4cnkRYLa+bPratirz9Qz7+97wufWxuEo0KwwdsfYpcekg3+x05Tt89/Lpt0qPEws1M6rEjQfKCtiBxI2WiMSNXq9DrTj+h/RCTrHMRpXY3LAwlhtzFoSUOHFE3PiAH5QgvIJA3FQ+ZmNsAPXiXxz8r7StXPaUI2nvOqlsKRU/3Y66pZZOdGy8WuQudCaj0BrxXSegIAv47yXH9n/4f/Lryov4VGpXObUamPuo7fJV7wOTagLfdgbWpNuul6qYrYRYOEztbX3skLVd4bPMirTDCx3YZyWy4oZ5n8lyE2Cwyrryza0Vx/+4nc0uktpCHqmYGymTrU5HlhuCsEeQhOUmorr0WCnY717+Rdv1jgbGOtotnbUq6fTqvsuOWm7OuFhjSw653kSmCluB6QxrP5Jfd34Xn0atBceX2i5b9zH/98ph6W0crebLWqFOrRFWZ/5rJB/bAgCF19S78GziOV0UHpKxWZzQcqM2UNuNkLjREvYHrvILXbtS3Lw0dx/eXHAA7y06BE7yQyleJhFzI3dMwV2gwnjqO0VUVaQsN2xRwoGT7eyA+X5K/XC787sl7hWl1uqTdcA983EUVtwI7u6d6DguhZLbKf+C/DpnuHrC/hgWRy/y7PmYcaft+p/6ARkbgY8bAPNGW5fv+wuYJ5MBu+8v4XNHrUk2c+SzgW2Codn3Ycad2nWFdxK62mmJTmdT68ZsuQGAGZvPYNqG0zh4Ic92W/aDYDIyv6WsWJH6sOgcsMiQ5YaooggCiistN6y46WynNEL7SpfEDTdb9xXOdJFWJW6c/LHX6YXBzmqFVO5Z546nNeYLtrFC1LSyQpteSp6s5fWNg1W5HX199l4LZwLWf8I/3jfbuvyfJ4C9s4Rjz+3gKzTPe1K4vFDUI85RTBV8scRJNYXLr4s60WsR6+QCJG60RiRu4iNs/aS5xVI+aOaHz1gG9ZYbHVlkCMIeUpabBn34ysbmCslKJLcCXjrFZ0+NXADU6w6MYmI93Ony1elFvYDsdHE2M0Ch+KcnMVUA53YCk1OAkyuFy7Ww3HjaBeJWi4SKfas5vqmcr0HzXRfbdWo/P7L7rpAuSLjsDeFztdWf3QRVKNYasbiJtA3qK5OqWMyJxI3aL5CNuFHYjmJuiKqKlOXGEAWMP6A+C8VcUbheV+CRf4Xr1NxgqO0lJUanU27IKUeXNGDpa84dU0uM5cBPN0ssr5BOsY+t63WXhiLGMl4gZ0h0KPcIDpwbe0UEnUHOulRwSXTsQr6KuZegW36tETXPHNQ82WaI3XYMxnI4FnPDvI1KPwpk4SGqKmzNF4HQCdUmxVbNdyu6JjBuL/Ay05KAbRvRe4L0duZu1+7ClYalapDLyJGz3BhLnevC7SlK84F9c+RTqN2Nt4WfXN0isUXIy8KarnZaI7LchAbrMbxjimCIZMViNl1SYLmxky0lLuKnRI0m6sYRRKARLGG50RK1Nw7xqcK72fv/AB6aB7xxFej4pPQ2rrhu1LjcnLEKOcLJVdLL5WJuKkp8W9ys/gD453EvTkDcG8rDNWXEzUnlcCbNXENI3GiNSNwAsLRhMJNfKmHWY0tdaxFzI1Xk695fgRb3AE+uUd4nQQQabD0Vd4gbZ4P1DVGVQcoh8jcp5gu9M5bXB/+237dLrhZPqweAuxwsqGemZgvevQTYFkc0I2e5qSgFLsukVrtCs7u12c+Bucrrb/8CuEki00kJRzLbCi4Ln3s5tsVXIXGjNRLiJlwkbgpKJMQNG1leIWO5kfK1NrtbvbiJqwsM/Qmo1UZ67gQRsDDfHUfrv6hBC7dRkMy8zOLG0aJ8AP9aazRVHiMnqqJqAK3us/bZcqT8f0iEVTSFRkmPMRnlLTc/qLA4OYpW1iC5nktm2j8izG5Tw/fdbKsTy3HliPB5uYoaatUaAS8ed2xOgHQ/Qz+BxI3WWGJurB9UG8tNiciMeGEPcHyZ9blay018KpDcUjqg2JEfIoKoSrjDcqPFPkPCgMFfAoM+4jOzzLgibgD7butImWKG5licgZOBp9bz7STUEhxmvcGSszhplS0FqOsqr1XKeIW9VjpwrndfqRP1Z4wVQJkKcRNscFxwAUB8fce3MePuWC47kLjRGgnLTXCQUKAUit1SU3sJn6uNuYmtjOWRtNx494NFED5FFFOTI0jDJNFerwB1u/DuXi1o9zDQ6SlrZhZgtW442+xW6bcgqQUQmSizXeV5Cgrmb6IcaR4aEm497pFF0uO1FDdqxKVY3ATLuOO0wJmby/+Nd3ybimJ1lpugUF48OyqQ5VyWAHDrJ8rbyrkjPQSJG62REDdiMVNmr0O42mypmNryYxxpzEcQgU50Et+d+/GVdoc6RJ/XgEeX8BcOd2EWFc5aY2u3l18XHgcYoqXXiUWRPcuHON3e3m+QVkX8ANtzIyV22PmP+Jt/79yFM0L00HzHtylXKW7Mrlgl602rB2yXKYmb+j2B+r3k198gkf7vQUjcaI2EuMkXxdiUlNsTNypjbqT6spi3I3FDEEJu6APUUbjQ+yoWt5ST4qbni0CLe6XXBYXK93cSuxXsWVnYmKGQcNvfIHEvL5NRO8uNuFeXPXHTqJ+wpYWc9crp+Xjo93f7T+oCis3CM1QmM+6Or/l/YpQsYiERwKiFvBtVits/l17uIUjcaI0l5sb6RYqPEJoCSyvsfKGNpZC03LQeIRwnmcJZuV3NFvbnShCE72O+YXFW3ISEA33fkF6nFIshFgz2hAhrvZKy3IgtROWFzllupAKvxUJMHwQkNmOeh9jOv9oN1sdxdYVjnZ1Ln8rO6vbeq2qN1B9DibUfqrTcVL43IRKuoq5jgbYjpd21SmUCzOvkPhdeLOAHkLjRHovlxvqGP95DGJRVatdyUy5tuUlsCjzO1IyQy0IAgDu/BdqOorRvgvB3XA0oBuQzsYJCgbA46XWxdYXPldxS1RoCrYZbn7MxN2bEFqK/RjqewZTYDLj7B9vlYiGlCwIeY5I0wuNs5594k/VxZA3rY0eCwxMaWB93Gwf0erlyPnbETVSS+mMokdpDvqgei7nOk5SbSSnwV8ndat5X6wf4jLyeL9mfhwchcaM1Em6p6LAQ1Ii2/rjYr1CskC0Vw1Q8ljMxAnyMwR1fUdo3Qfg7rsbcAMIiho8tZ5YbgA4yRdnqdRU+lxM3CTcAY3bwAsKyXynLjYpsHdZVJKbJ7cAzm4DqN9quEwspfRBfQ2joND6Y/L6ZtvOPSADqdODdZfW6yO9LiQTmxpWdu72gdbFVzFHi6vF/s/YBxdftjzeLW6kgXyXrmZLQM68LjwPStgA3v25/Hh6ExI3WSIgbAAgNsp5qu26pskKZmBsIP2xS9Trs1bQgCMK/MIsMV7K8WMsNW7skKJS/yPd7x7ospg7Q/XkgtrZwH3LWBn0QfxPGWpakYm7UiBu2HYWYrP2Vc5b43RMLErM1osUw4MWjQN1O0uLskSXA+P1ANHPTWK+b/XmaiWHOEZstxgrRRv2tj6s15AsjuhqTY7Y0leSqa3NgnpuU5YZ1K42YCzQfZn0uZ9UDXBdobsa3Z+ePWMSNsJZNpMH65duecR3frj4hv4/5T0PWcsMKGvYL8uJxYOwu3mJDEIT/8+Jx4InV1oabrrilBL8bQbbL2RiR2z8H+r1lu4+uY4Gmd9guNwsJe9lScoHL0PEp9aHRQJ/Xbd0kZnHQujKbJ1jiPNjE3Ehc2qTETVAwbwFn44GaDQE6jZaZqwhW8LEigT2fbA2esTv5woiu1oBh3WhqML92qRgatn1Do1uEgcWOFvEzt/twRCC6CRI3WmNWxuXFgsWf3dta8PzjpUf5B0Z7haVE4iZIRtxEJQoD5AiC8G+iEoHaba3PXRE3+iBeQHR8ii/+adln5e8Ja2mQC4Y1RAH3/Cqxb3M9HJHlRtxIUa7uSbCBT8t+JQNIbAI8vkK4/v5ZvFupS5rtcSxzkLHcsCgFRLMuJX0wMOhD6/OUztbHaduE27FBs2z8ECsYOj/D/23Yj9lORjSwMTxKSBVeFLv0uj/PrKsUllLvgVj0se8/W28pQcX15Z7pwIB04N4Z9se6GcoX1hpzkK8oyKt57VjMHd0Fw77fLBxfbieNT2y5YU3TXq4ASRCEB3HVlSFV18VsBWF/V5TaU0jW1Kq8RxaLG9ENnmzNFLPAMs+hdlu+QGluZuXyEKDp7cx4KXEjOjdScTNKAdGsqBCPY/dlY41iBAW7Hdsbq1Zr4LWLwpACudpCjfoDW7+Xn6cZc8wNS3wqcHEvs69bgOqNgGNLrQJL0i0lqpjPvsaI6sB9vwMnVvCVqq8eA9Z9LBRqLJHVgS7P2J+/ByDLjdaYP7SlBTarzG0YwlECPSpVvt1Id4UiflTLhiCqDlKBtK5iFgqsYFCyEEmJG7Nlg73jDwkHSvKE49g0ZPbiKOVmUpqDKnEj8duoJG5Y91J2ZeuLRgP4v93GW9dFM5WuAaFIYS1DTW7l/5pdiqERQlcZG3zNopQByxIvIW7Egic4jHfl3fsrUKMxv0zSLSU6LzqdtaJ3rTZA08F8W5CQcP713Pc7X0nbx6Gro9ZYxE2e7apgPaJRhJ2Gp3CUS8HEeam4fnIHvlPan2KF4mT5dQRBBBY3v87XNNGi1UO1hsC1E8BNQ/jnrJVFbT+hAelAWYHVKsBafMLigJIc4fj4erxlQh8idI9IZeTIxudA2rIkbkEj6ZZSEDc6HS8erx4DGt7CLxv+B5B/EYitAzx/hLdwhEby+zZnGKV2l95/k9v5rLQaTaSP1/FJYNdvQEEWL/rMFnyDSnEjZbkRhyVInVc1lhsAeHYX71pTeh98HBI3WmMWN2W2lhtDcBA66Q8jVGdEC10GBm89ix76LMBRV/o9v/JfQh8I2iIIwkOExQB3fqPNvkZvAAqvAnGV/ekKLlnXRaoMIq3RGGjY1/qctaiExwHFOcLxQaHAiDn84wVp0tuZUQpklbLKiN1QZssJi70ihE+sBvLOW60c+iBe2ADCG8kXjgA5mUDN5kKhxaZU63RASkf5Y0Ul8vvR6YCzW4GfK4Om1TQABazvGwsbGwRIi0CzcK1+Iy+8Di8EOqfZjvNyXygt8Kpbat26dRg8eDBq1aoFnU6H+fPn292mtLQUEydORL169WAwGJCamoqff/7Z/ZNVi8VyY+tuiszej59CP7U8D0U5EmBr4REgZblpNoQvFqVk1SEIgpAjJFx4gWw7ko/Z6Pe2+n2IRQnrlgqLs40nZF0ubGKE1EVYSdzodMALx+Tn0v996YJy9npjGaKswkaJqESgTjvbeZscLEho/v1m3XIR1fms18eWKxf6k0rRFospKctNy3uBR5cCT67lb5JfPs2/lgDEq5abwsJCtGrVCo8++ijuvvtuVdvce++9uHTpEqZNm4aGDRvi4sWLMDn6oXInloBiW8tN/D/DBc9jUIRqOhdibgiCILQgri4wbq/9cQDQ9Vng6nFbyzHrGpK6+JqtIID9GB97KcjikhdBoXz9GIBPtZbihpuBvX8Ia9NoiT3xJAcr9CKr8+6lajfwlqSTK/mAYHNn9X7v8OvEwiXxJj5zq8PjfL8pQFrc6IOAuoyFx8stEtyJV8XNoEGDMGjQINXjlyxZgrVr1+LUqVNISODflNTUVDfNzkkULDf64muC5zG6QrTRH1fen9iXTBAE4U36vye9vIJJ/Q6LAdo9Auz8xbqMtRQF2Uk9d1SANB8KNLdzgzzoIyCpOW/5dgdqrD5SsBYgtrlobG3eonacqSjdfbz18eiNwIVdfGaWOe271QOMuFHIeqsC+FXMzcKFC9G+fXt89NFH+O233xAZGYk77rgD7733HsLDpdMMS0tLUVpq/dLl5dlxA7mKJebGfr+POBTgFv1O5UHkeiIIwh9gu1MHhVR2i+aAndP5ZYL2BIy15tx22321fwQ4tEAY06NEs7vsjwmLAbqOUbc/R3h8FXB6Ld/Lz1Wk4p3kYoVqNuf/CcYywcGO9MgKQPxK3Jw6dQobNmxAWFgY5s2bh6tXr+KZZ57BtWvX8Msvv0huk56ejnfeeUdynVtQsNwIouwB1NDlIEwnEaku3Ei7uREEQbgLc+ZQTKX7KTgUuOU94PQ63iXEYq8gYWgk8Phy5TFmEpt59yawTjvX4lbYIGIpd54jndONZdbHrrTrUAHHcXj+r70I1uvw8T2t3HosZ/ArcWMymaDT6TBz5kzExvJ3AZ999hmGDRuG7777TtJ6M2HCBDz/vLVSY15eHlJSJCLNtUJJ3OiDAaP1g/p+iIpAaLLcEAThD0QlAi+dFAYOh8XwAbLi3zFHu4FLcf8fwJp04O4fXd+XN4lIAB5bwdfCkfq9dySWJ1r78iAcx+HghTzcUCMK4aHWrLQr+aWYt/s8AOCNwTchJsyFxq5uwK/ETXJyMmrXrm0RNgDQtGlTcByHc+fOoVGjRjbbGAwGGAwe9D0qBBTz4sbqIitXdfpJ3BAE4SdItQWQumDbq8yuhia3Sqd8+yMpHeTXOSJuqjfixZ5SphWAvJJyhAbpLYVllVi49wLGzd6DtnXj8M8z1iDyChNneVxe4UNJPZX4VbRqt27dcOHCBRQUWIXDsWPHoNfrUadOHYUtPYjZclNRbFv5UWQmDEcZ7EKWG4IgAg22aGCPF7w3D3/AXn0eMS3vBRr0kl1dWFqBlm8vQ8dJfA+vY5fy8dC0rdh5Jtv20CYOs7fxbTB2nc0RrCtlBE0JiRshBQUF2LNnD/bs2QMAOH36NPbs2YOzZ88C4F1KI0eOtIx/4IEHUK1aNTzyyCM4dOgQ1q1bh5deegmPPvqobECxx2HLcbNBxafW8u3pGSIgaiwH8J13zRUyAZDlhiCIgKO8xPr45je8Nw9/wNkUcxmOX+aNA3klFTCZOKTN3IX1x69i6BRh38Oisgr0+mQ1Np+6ZrOPy/klKCy1zqu4zEEB5gG8Km527NiBNm3aoE2bNgCA559/Hm3atMGbb74JALh48aJF6ABAVFQUli9fjpycHLRv3x4jRozA4MGD8dVXX3ll/pIEhVij1Nm4mxl32Aw1SAUT930LqNnC+pwsNwRBBBqsWyoAfuNKyo34a3smLueXCJbvP5eLXWevK27LcZzk8o0nruKp33agrNxe0oljBDHnu7jciKzcEslxyw9dQmZ2sc3y/edy0XHSStz13UbLsst50vvwJl6Nuendu7fsGwsA06dPt1nWpEkTLF+uMoreW4RGARUl0nE39tAFiUqJ+/8XnyAI73Dicj5iwkOQGO1jacE3DgQOzgMiE709E4cpKK1AQUkFflp/Cg92rofU6pH4fMUx/LD2FBomRmHF87xLqKzChMHfbAAAfHpPKwxpUxtBeuHv+Qt/7cW2jGv4b1xPRBmEl+MRP20FAFyILUBq5bLSCiMMwdJxMpfySpBfUo6GidEoN5oQrNdBp9OB4zhcLypHQiSfoXY4y1oOpbjciAhDEPJLba1DegnRmTZzFzKvFwEAyo3Wa/cDP21FxuTb5E6ZV/CrmBu/Qdw8s0JFbE0lr847gL93X7QuCIC7GoIgPM+560Xo99k6dJy00ttTsaXFvcCIucDTm7w9E4c4cbkAzd9ais7pK/HThtN46OetyMwuwg9rT1nWm8kvsVpcXpizF4v3X7TZ39+7ziEzu1hynZmCYqtV5My1Istjk0loGOj0wUr0+2wd9p3LQcdJK/DinH0AgB/WnULb95ZjyYGLqDCa8PLcfZZt9p/PxaU8a3jEe4sOWR6LhRgA/Lv/Ivady7VZznLqSgE+XHIEeSXaWpwchcSNO4iswf89/D++KZq4O64Cs7dn4kw2a+IjcUMQhOPst3MRcgSjicOrf+/DX9szLctWHr6Eh6ZtxcVcW9eFXfR6oNEtQFQN1ZscycrDJQ+7P64XlmHx/ovYm5kDAPhp/SnB+szsYjw7e7fktvklQmvIfwcu4u2FB3HgvO37cupKIaauO4kD53Nt4leCYH3+6bKjyCkqw84z2Wj+9lKk/3cYv27KwBvzD1jGpM3ahetF5fh71zkAwOT/jgAAnvtzr2WZmUd+ERZQnLbhtMWbImW5UeLb1Sew9dQ13PzpWkxZcxLPzd7j0PZa41ep4H5DbG3gHIDN3wA7fgYeXeLQ5kZGc76z6BDeurOlxhMkCCLQ0TEXpwqjCcFBzt/LLj2YhdnbMzF7eybu7cDXCXvs1x0AgPcXHca3I9o6ve8Tl/MxZ8c5PNXrBovrRExmdhEGfrEeACzujwqjCRnXinBDjUjBa5WiuMwoqNEC8HVa3lxwAHe3rYOuN1RDpMH2cjjgi3W4nM9bNta91AchEudwtyiLaN2xK+h5Yw0by8Xi/VmW9ateFIZkfL/2pOXxTckxmP2Utf9TEKyZSEsPXgKwD9FhISgqM1osRixsnMw4RniVVBjxyt/7bcaLyS0uR2mFCR8uOWJ3LMvHS48Knq88chkcx9l9b9wFWW7cAdsXpbwIuJ7h0OYm5m2ZvvmMRpMiCMKbmEwcTl8tVIwz1BL2mlJU7lo2S06RvIvhepE6t/uV/FJkXLWtb3PrVxvww7pTmDiPv/CaTByOX8oXnKedZ6xBuXN38taH1+cfQL/P1mLBnguKx/146RE0fXMJ9lRaX8y8Nm8//juQhSdm7EDrd5ehwmibzmwWNgDQ8+PVkq4aMSN/3ob0/w7Lum9OXS2EycShpFw6ffrQxTwM+Hyd5fkr5U+ihAvBe+UPAuAFjvkc2IM9N2o/djsyrqNz+kqclnivHKFRYhSKvJhFReLGHcSKKiDnqvsgmjEyriiO3FIEERC8u+gQ+nyyBtM2nPbI8SqYgM9CiYBRMUey8tD749VYsOe8zTrWYCEWZ+EhQcjMLsKJy8r99DpMWoHen6yxySgqq6yRYraAfLD4MG75fB3eXXTIMu9iRpy9OIfvXj670kX27qJDmDhvv2SdllNXCvDtat4qMunfQ4J1u5kspnIjh2wVIu1CjjoX3A9rT+F1xlUkZsrak4KYHDEXmQym3VwjNCv9GdOMnilY+PiMHaqFkBzVIkOxZHxPSWuYpyBx4w5iRR1tczKlx8lgFLwtJG4IIhCYvikDgDUGQmvEoqOozCpo1Iib5/7ci4xrRRgnESvBxl+I78b1eh16fLQa/T5bJ3DFLN5/EYv22VpV5GKBzIf4qVL8/bIxA8N/3ILconIby9GSA9YA3OzCMszcetamTgsADJ1iDVguFRWayy4UiplSGUsKiziOxlk+XnoUHT9QH+hthP1Kwr7Ef+N6qLJyuROKuXEHMSJxs3WKQ5ubSHMSRMBicoNbqqzChLu+24j61SPxzQN8/Atr7Vh37Cr2ZuZi2aEs1Ig24P0hLWz2kVesLruloLQCEUz8ypqjly2PL+QUI6ZmCDYcv4pnZu4CAFSLNKBdvXjB9lJIBbDuO5eLVu8us1k++vddkvsoqzAhNNj6+3mdEUVlInEjSjbCN6tO4MSVAjzSLRVFZUYkxdimz0sVtHMXtePCcV6lpcgbNKkZjSNZ0ta6RIlz52lI3LiDWNdaQRhJ3BBEwKKltCmrMGF+pRvp4IU8HLyQh28e4NexFpZ3FwldMu/e0Rx60Z11hUneclHCCKWC0grEhlubJLL1TszWkMWMZWX4j1vw/pDmludLDmRhcMtaNsfXgg0nrqB1SrxkYPKRrHxcyClGrbhwybinP3fwFnY2vscR2tSNswkuFpOSEI6+TZIsVjwlujesbpmTI+x7uz/u/m6TIC3dHfw3rgcu5pag6+RVguXse+1N6CrqDiIkmsepII+LAECWG4LwNziOs4klkR/L/y0orcCCPecVYy+kMJo4ixXix/Wn8PLcfYLaJRVGE7adzsbVfIn2LpUUSwQYszE6ZRUmvL3wIFYevoTDF/Ow9bQ1nqWwtAK5MlYes7gRX+TZ+JP/DmShwWuLkb74MF6fb83e0Wvws/fo9B3o9fFq2fVdJ6/C2mNXNHcN3nJTEmY93hlxEcqdsaMMIWhTN07VPlvUsTaIToxW3/w5JiwEvz3WUXJdtAsxMGl9bsDku1vg7ra1sWR8D+h0OtSKE7Y9OvLeQDzYuZ7Tx9ASuoq6A70e6Pe2Q5t82WIeOpd+A4AsNwThb3y45Cg6TlqJ+bttg3HleO2f/Rg3ew9avL0Mm05exc8bTuOVuftsirOxcByH275aj94fr0ZZhQnrjl2xGfPS3H2494fNltgVKcxWnaNZ+Xhxzl6cu14k6PK8YM95TN+Ugcd+3YFBX67Hon1WS0xBaYVsyf4xs3bj7YUHcfKKfavBD+tO4fct1vY6mdnFsplkd7epLblcivzKnklyjPp5G35YZ5tC7SwfDW2JKSPaIjw0CMue66k4tkNqPBomRtndZ0RoEHo0st4kP9mzAerEq++fmBwbjimi9PwHO9fFN6Jl3zzQxmbbdvXisWhsd5vlYcFBuL9jXXx2b2s0qRljs751SpyqLuOegq6i7qL7c0DzYaqHl0TWQhF4P6VY3IgD3wiC8C3MdUre/t9Bm3VlFSbM222bMblwrzXY9oEft+LdRYfw545MbM+wzfoxU1Ju4t0ruSU4m10oiC8xM0+FwDIHG784Zy/m7jyHB3/aCiMjCPIUAmcLSipw7JJ8ZtT0TRk28S1qYc8Jy4Rbmzq0n8IybZtNKtEoKcpSQygxOkzyPQF4gfbigMa4KTkGT/e+QbDu3Tub4baWyZbnA5vXRL1qkbivfQoMwXrcclOSJZYKAIZ3FGXkSjCoRbLg+ftDWqDXjcKiiR3rJwiezx3dBX8/3RU3JkVDDBs3xZIcy1+3BjSraXdOnoTEjTsx2FfoZkKZXEuxW2rAF+vEwwmCqMRTdWPMfLXyOL5aeVxynVHCYvDt6hN47s+9gmVsto8YOaNDYWkFXvnb6n6at/s8gp2MWzFbbswiJeNakSDmJjJU/g78yd92Oh2XAvDuDTnk6reEhUhfql6/TVr0nLteLHB5SVE7Tr0lRAlxr6flEtablnVi8dl9rRETFgKdTodXBjaxrLsxKQoju6Tiq/utVpSbknnLyOShLbD/7QGoVy0SrerEYnSvG/DpPa2QfndLbH2tr925ta10gTWoHim5PjE6DPe0q4N29eJxYtIgtE/lxU5osB43N+H7fr1+W1PMeLQjujaUDreYn9YNX97fGo/3qG93Pp6EAordSXiC/TGVsGrfyAm/yFcUfOeX80tw3w9bcF+HFIzuJf+jQRCByJZT1zD69514545muLO1eteFs+SVlOOz5ccAAKO6pCJWFGMhdofszczBlxJCSC7bB+AFUkm5EaevFiIhMhSJ0QaUlJvQ7K2lgnHm+i3OUFRmxLWCUkF6NFtUbta2s1KbWZi93X6ga3RYsE3qdO24cLw0oIns3OUsPnLuDrmsnEFfrrc7P4OMhQXgXSzion8s89O6Yci3GyvnJtxPvWqRWPxsD+w/n4OO9avh100ZeLJnA9l9xYXzwc9Beh2+uK811h27goe68HErOp0OocE6y+NXB1lFUVJMGH59tCP+2HoWSw7y1Y/FMT/fjmiLnzecxsguqZZlnRskYMupbNxyUxIA4ON7WknO64eH2uHA+Vy0qhOnGPydFBPmke+eo5C4cSdRTMfbVg8Ae2fJDmW/aFIxNycu5yNYr0eqSIF/vvw4Tl8txOT/jpC4IaoM53OKkTZzl+UCNG72Hpd+YL9ccRwbTlzBb491UowbEATdSlS0NYqsSHdWXgAdoaisAiN+2mqxjsRFhChWCLZHTFiwjZtpR0Y2Jv93WHYbe80R1dCuXjzuaZeCtFlWIWdON5dLc2YDl1mk2h4AQJIDgbZilFLy7Ymbakw2lkHi83JTrRjcVIu3vrx9RzPFebCZXUPa1MYQB+KLet1YA91uqIZPlx/DlfxSPHfLjYL1ybHhmHjbTYJlU0a0w+IDF3F7y1qK+w4J0qNNXWlXlD9Abil3Esn4N8PjhOtChCIlRMEtBQD9PluH3p+ssbkzlKsZQRCBzMM/b1O8+CixIyMbP647Jfgufb7iGLZnXBdU572QU4xn/9gtOA7runlo2lZ8vPQIxjP9exTiWFVTVGYUuH1cETYAJNOi0/87gu0Z9l1LwzumYNJdzqX21owJw60thHEY+ZW/Vx1SXbtojunTEOP7NULdahGWZbe3TJYdP6ydbXkOEwd0TLVa11kXX5QhWBDQK4a9GVWyAKmhfg1pl5FagoP0eGVgE3xyTytVrrb4yFCM6FRPkM4fiJDlxp2w4iYkArjvd+BPvj8Igg2AsRQw8V/2cuYu0KRQlbi43CgoaV1W4b3eHQThChzHIbuwDAmRoQ431zuusoZHWYUJVwtKUSsuHBlXC/G/vRfwaaVbadLiw/jlkQ7o09hqYS1jLDOv/L0P649fxX8HLuLvp7uiepRBcLd/JCvfpohZWYUJzd9aittaJCP9bttCeWrQOhg2PjIUGdeKnNo2OTYcNaKcs44kxYTJvq9v3H4TsovKJbO9AN6l1fPGGviXydJ68/ab8N2aE5j1RGdLwOt1JtmiTd14XM4vxTYJ689LAxpjTJ+GuJRXghmbz+Df/RfxZM8GGNi8Jn7ZeBr3d6iL0goj+n3GxzdGGIIQI7r4pySEY1jbFLRkUrQBOB339NHQlli0/yKe6U0Wd3dA4sadsG6pkDCg6WDgzu+A/14G7p0B/HaXZXVhqVWkKKWCF5WJxY1zWQkE4UkOX8yDTgdBCukny47i29UnMf2RDujNCAxnyS0qt4mBufeHzdiTmYNFY7vjvh82o1DUOuCRX7ajdUqc5bn5OlXMWE/KjRzu+GYjmtWKEWSsyFFQWoE/d2Q65F5gKda42eDEW5ti2Pe2rQnUEGUIRrUo6U7dYva+2V9QTVguawgAqkUZMOPRjvhh7UmkS9Sc6dc0ycZt9Gj3+nikW6pAMLEuRI7jZOu4hAUHISkmDKnVI9EqJQ5P9myAFrVjodfr8NIAPo7lYq7VTRYZGiywbEy8tSlua5lsqevCcRw6piZAr4fTFpB7O6RYOqwT2kNuKXcSz0SPm/tLtRkBvHoWqN9D0La3qNx6t6ZUxK+orAIfLD6M+37YjLIKk6TfnyDsUWE0Wbogf7PqOO77YbOgCq2WlFYYMejL9Rj4xXrLhbvCaLIElc7YfEaT47y+gC8Ud+56EV6fvx+nrhRYXEp/7zpnI2zMsG4nvU6HdceuoO17y216KB28kIexf8gHAosZ/uMWx15AJWpqxDhC+9QE7H7jFsUxzWrZ1i0BgKiwYMRFWMXN492tv2l3MeLtk3ta2QjLmDBbodFJlHr8aPf6grTm2nHheKRbKt65UzpORWwJYl1CJo6TFRoGJug3LCQIrVJsg2TZm8bgIB0e7ZYKgC/Q90TPBoKCdTqdDn8+1Rl/PNHZYasj4RnIcuNOQsKATqOBrd8DrUdYl+vNdxvWLwUbqKhkuen18RrL42WHslQ1eyMIFo7jLRElFUYsG98Tnyzj3TRzd57TrLqoOT1bp9OhiLFKDvl2I0Z1TUWXG6pZltVj4ibEGE0cRv++E42TovHigMYAICvCFu+/iK+Ht8Gzf+zGrrM5WH3E6vJQ280542ohJvwjn0J84Hyeqv24wh/bHC+5b4/4yFB8NKyloJKxmfb14hEdFgzbCj1An8aJqBFtwAu33Ijq0QbsYSoPpyRY3zepmJZh7YRWiYjQIHz/YDvBspAgPR7sXM/ymje+erMDrwoCgWLieHeSmbvb1EZBaQXCQ4NUFZeLEI1pmBiNPW/egugwacFEosa3IcuNuxmQDrySAdTtZLtOZz39j/eojxrRBozp0xDFUOfjziuuIMsN4TAFpRU4dDEPp64UCjJW2OD0y/klOJKVh5yiMiw5cFHR/Xm1oBQ5RdbYh3KjCYO+XI/Hft0BQJhVdPRSPl6btx+Z2dYYECU3zO6z17H80CV8s/qExdJ0vUi6qGV4SBDKKkzYVXkBZl/b0oOXZI/BomXlWrXcIyEMtIINir23fQrG9W0kWP/szQ0xZ3QXSxE6lo6pCahRmY00tm8jDO9YVyAeasfJN0dM63MDwitr5ZibbL4/pDniJYKbm9WKxS8Pd7Cp7lsvQV70SmHiOESGWu/XP7mnFaaObI8v77etwisFew7MHrG4iFCvd7cmnIMsN+5GrwfCZTIDGOWfHBuOba/1hU6nQ/3VTbHE2AEnOfnof4B3UVHMDeEobG0T1vVSzizvOGklAGutkmdvbojn+ze22VdxmRHt318BADj1wa3Q63XYdy7HEmy7cO8FQdqsmczrVnGTr5Dxx2YfXcwtQUpCBK4XSmcPGYL1eHDaVtl9eYKl43vioWlbcVmhNpWYBJUxLQBQI9qgWPeKpWlyDL4eLrywi4Nk29SLh06nk+xd9M0IW1HwdK8bcCmvBA90rIdODRKw88x1dGOKuy1+tgeWHcoSlKVY+UIv7M3MQf+b5CvY9mliG3M1uvcNuJxfalNpVw6Og0VQAXCpMScZZfwfEjdeRfgNMps5Oegxuvw5u1sXlRlJ3AQoW05dw9WCUru1KJyBFTdsA0SzhYVtFWAuwjZ/zwVJccMGYZYZTQjTBwn2/+wfu222AfieRmYKSirwxYpjOHghD1NGtBXcQRcywifjWiFSEiJQKpMheK2wDNdk6qQ4ysfDWqJuQgR+3ZyBxfuzVG9Xr1qEYjq4XgcsHtcDA7/gi8y9NKCx4DXWig3DBYm+TeP7NcKZa0V44/ab0Pa95YJ1UYZgi9UtLERvKcb3VM8GgngZ83ozX9zXGr0ry/E/d8uNOJqVj5ubJqK4zIjhHesiMdrWMpMYE4bvRlhdSx8NExaAY+u7mEmODUdyrOPVgCNCgzF5aEvV400mDkPb1sEXK46jK+P2dIS72tTGxhNX3fK9IzwLiRtv4uLtQVGZUXAhIQKH+6fywahNakajYaJtnxdnOXe9CHnF1osp604yixtxqwBAXbprabkJyw9dUmzYaIYN4l177ArWVqYEv/2/g+jbNAk1ogxYe+wKzjIpzBnXitCjkWcyBJvXjkXT5BisOKzOnWXGEKxHUowBVwus1pXqUdbnkYZgQcaYIViPe9vXx8K9F3BXm9p4vHsDXCkowbbT12HiOEs37Tta1UKDGnw7l98e64iHpm0DwIuVkCDre3PkvUGYve0stp3Olqz7cnvLWvh65Ql0bpAgyOaqHmXA3Ke7OvRafYXqUaG4WlCGPk0SkZIQgT1v3iIIDnaEz+9rDaOJI1dUAEDixpvopEOeJgxqIpkeKaaojGJu/B2TicOWU9fQrFasTbYJAJy4XGgRNz+tP4XswjK8zPSlcYSjWfkY8MU6SwwEICwQV14hb3KQMvFXGE2CXkqlFUaMlbHUiJGrgPv7lrOCTtEsZ64WArCtDKzXaVM8j8Uca8JaL25ukohVRy4Lxr1+W1O8/y9f6bdf0yTodDp8fl9rvPr3PjRNjsHeczn4/N7WuOVzvn6KuB2BISQINaIN2PCKNZA2NiLE8p6nJETgSn6pRdgAQI9G1vpZIUF63Nm6NqZvyrA0Nry/Y13c37Gu5OuKDQ/BxldvDqiL9+oXe+NSXonlnImtVY4SSOemKkPixqtIf4me6nUDhneqi5ZvL5NcbyanqFy1/53wTebszMQrf/PZOc1qxeC9Ic3Rqk6cZf2FnGLc/Oka9L+ppqXz9F1taqNRZRGz/+29gH3ncjCiUz2b1hwAn7V08koBUqtFYtE+vuMyG2dzjSmC9vPG07LdhoMqrYyrj14Gx3H4a/s5bDx5FR8Ps7oN3G1FNBeiyy4UBhTXqxaJ05XCp0a0Ab8+0hHp/x3G+uNXnT5WfOUFsnq09UI56a7m6JK+SjDu8R4N8ECnuvjf3gsWV8aNSdH455lugnEPd03F9E0ZllTqW1vUxJZT2bjDjvtD3MVZjCFYj5cHNkbz2rGWRof2CLSLd3RYiGxGE1F1IXHjTRR+Y2JUfFkX7r2g4WQIb/DN6hOWxwcv5OHBn7Zi5+vWmiRfrjyO3OJyi7ABgKsFZUiMKccH/x7Gnzv4FNof15/Gv892R7Na1uqpF3KKseH4Vbz89z480aO+IHXXzI4MYYyK2cIgJqe4DDsysvHIL9sFy39ab3VBnc1WroJ7Y1IUqkcZsOnkNcVxcmw6eRWpr/5rs7xB9Ujc3jIZf2zLxMfDWuKmWjH49J5W6PjBSrv7jDYEo3uj6vjvgDWupnfjGhYBYG5qCPAxIFJEhAbjvg7SlhIzb9x+E3o3roGOlXVevn2gLcqNnGKhOyW6NKiGzaeu4YFO9RARGiyZik0QVRkSN95Exi1l5uNhLbFw7wWX7kDFcByHi7klSI6VL41OeIb953KRmS2sv8LHUVktK2zAr5nC0gp8tfK4RdiYWXHoskXcHL+ULxAqP64/jQmDbN1Za2XK34u5lFcqWeV2B9MDacRPyplKxy4VoF415/voiIvqmTGE6PFC/8Z4gQl4lmpmKMWMxzpiwR7hTQJbi4XNvokMFe6zrgOpykF6naAKM9vt2RmmjmyHHWeuo3tD+f5HBFGVoTo3XkX5x+2e9in47TGJ+jgu8NuWM+g6eRWemLFT0/0SjvO/fdKWN3sWueyiMmRUumFYPl9xDPvP5WLivP34YuVxm/VSQknrWBV7iJsMilOVAd5y4tg+bYUMmxUkRe/GNfBw11S0TonD+H7C2i9swbfmtXmxWC0y1KYWzM8Pd3BonloSHRaCPo0TZbtlE0RVh74Z3mToT/zfgR9qsjuO45BXotxB2NwJeMXhS5YqsiyHL+ah32dr8d/+izbrCPuIu7bLceZaIabKFIx7c4FUrVgrVwtKZbNBBn+zATO3nhU0HDRjz23kaaLDgjG4lW3MSeMkYXbY67c1VdyPVFfmUDsX/Sd6NMDbdzSDTqdDXEQomibLtB8wBGPvm/2x7uU+NusaJkZJbEEQhC9A4sabNOwLvH4F6DxacZjYHC7HLxsz0PLtZfifzJ0/x3E4xfStkeq188Jfe3HicgGenqm+h44UJhOH7RnZgqq3vsrxS/l4dPp27DuX49J+xs/ejV6frBbULQGAv3eew1+MC4njOEEbDUe5nFeKSIO6zwTLeZUtCNwJq6fnp3WTHMP28AH42ipKSIkbey5XsWWnQiHrMDYixOnUYoIgvAOJG28TbD9tcdYTndG2bpzdce8uOgQAGPvHbgz4fB1+2yJsSDhtw2kcvGDtjZMn4aZQ0zzx7LUibLATBzR7eybu+X4zRjjZPFCOebvPYci3G5ElUejMWZ6YsQOrjlzG3d9tEiw/mpWPM9eE7h+TiQPHcTialW8jYubvuYDM7GIsYYJTi8oq8MKcvXh57j4cu5SPqwWlloq+znLqaqFTGS+7mb5A3oKDVd3cUEPa8lFDVC1X3PNHjJq+QWLErqyBzfnqufUlMs4IgvA/SNz4Aa1S4mxSS6VgMy+OXsrHG5UFwMx8vPSo4Lm45gagLhCz58er8eC0rdh19rrsGLOlYq9MPRNnee7PvdiTmYNJiw+r3objOEFRNTHmFOMKxqWUU1SGAV+sQ6+P11jcd6UVRvT7fC1avrMMA75Yh8d+3S65PzYlmj3H/T9fh/bvrxCkXzvDumNXZGvBeAp3pRM/0i3VJlA3OEiHIa1599X/xnS32UZcEVdMtchQ3Nu+Dp7q2cCyLFxkDR1zc0N8eX9r/PVUF8V9fXoPX5H3FSdrDREE4RlI3AQQNe2Y78XFz/Il4nPsBWKy7MyQFzeOXPs2nbyKAZ+vs0lLFsPGCGUXluLthQfxikSXYzFfrjyO9u+vsNR5UTNX1oWz88x1/L3zHDaduIZTVwotgmXLqWw8+NNWPDljh2BuZUy2k5Ko8hZtVFgBAWBEp7q4TaKvz59PdsazNzeS2EKZ6lEGSIR5oVNlejQAvDW4mY3wqBMfgc/ubY29b/ZH89pWIVM9KhRv3H4T7pCI22GpGRuGj4a1ErxusbXHEByEO1vXtrEaiRnarg72vtkfT/e+QXEcQRDehcRNAFFUZmuJ2XcuB8//tQdZuSU2FxYpy02YROaJHJMWH8amk9LuKbk7++zCMptA5gd+3Iqjl/IVmx4+OWMHhny70fL8emE5pm/KwJ87Mu0WMvxiBZ859LKMEBJnwbw0Zy9u+2qD5fmw7zfjhTl7JbOYNpy4imWHLiGPOZesiGT34yvMfLwTfn20o+KYaaPaY9JdLfDpvdbeQY91r4+MybehU4NqKFbhvhTzxxOdJD8Xn93XGq1T4vDl/a0BQFBBuWNqAhomRkGv1yE2IkQQS3NjUjQe617fbnyN+Zh6ZlyYk/VlAEhWkiYIwrcgcePHiKuXXi2wdXcMnbIJ/+w6j87ptgXNvl19wiaQ0hHLDcALk3PXi9D/87WYtdXqKpG64Kw8fAlt31tuKVcvxtzwT0xhaQWWHbokcHFdyrPG3JSUG/Hx0iMCy8+560VYdjBLIKSKyow2MTRHsvJsehXN2XkOUqw+ellyOSCMX/pg8RF8vfI4LufLxwV5s0hsRGiwzWcntVoEfnjIWt+lTV2+lD9r4ShgBJw43kgNjZKi8WL/xkiIDMULt9xoWV47Lhzz07rhztZ8r6Nw5phD29W22Y8ZtQLLLGpMzGdBbB0iCCKwIHHjx/S0U5odAMqN8qnJO85cxx/bhLEb7MWsXCKD5LpEvMjk/47g2KUCvDaPbyNw4Hwurkm4Y8z9sqYpNFa8WlCKcbN3Y8spaxVbtvO0GTZu5Yd1J/Ht6pMY9v1mHLuUj9fm7Uf3D1fjyd92YswsYa+jv3eew9ZT1ywWp5fmCK05KxUaJbJ9mMSIa8h8uvwYFuyWr1cT70T/m471E3DkvYHo1tC5jsdi/nyys+VxhSiFPS7c1joRzDRoZMVN9Sjp17JorG18TGr1SOx8vR/G9pV3a7GfQaXWacUyRf3EmC037HfBEQslQRD+B4kbP0aLm/+tp7NRUm7EjoxsXMgpFlxYLueXWuq25JeU4/DFPLR5b7nNPtgMq2OX8nH71xtw8orVQsJxHHKKyuzWHgnS6/DB4sNYsOcC7p+6BYO+XI9953Jw7rpyCjMbXHv7VxsEFqR/RfV64iJCcd/ULXjgx624kl8qsAABwGO/7lA8lhxS9YWUgp7jnHBt1IkLR1hIEDrVd17csG6hjkysS1GZEU1qWuvLsI0y37uzGRonRSOtT0PLsge71AMA9GlcAzMe5QtN3pgUhdqVady148ItBfDE2HMjsandRpO8ulFruakTH165L6u4kWoEShBE4EDFG/wYLbonLNp3EScuF+BIVj4AvmuwmW6TV6Fbw2qY8WgntFBo4sleMJcfsrV8fLrsmKCHkhzBeh3OXrMWmjt8MQ8jftqKCYOUi7ix2OuSfr3IavH5bXOG6rt/e3y27JhD43nLjW2VYSXM51muKu3al3rbrZ/TNNkqYFiRUVhagXrVIvHPM11RPVIYVPtQl1Q81CVVsKxt3Xhsfa2vpXLviud7IiYsBOGhQfhzeyZulQhEVgs7L6NEUcSWdWKx71wuBttpOjn9kQ74Y9tZvHH7TbL7IggiMCFx48dIaZsejao73IvKLGwAW/fKxhPX8MUK9Rducbo5AMzbfV52/ApGDIUE6W0aCeaXVOCChsXnvl5lFVlfrbIvuNTC9lhqlBiF45cLFEbzFiRHMYsbuVpE4gygW1vUxPnrxagVF25pDPnl/bbtDgDrhb9tZayNGpKY7LyGiVbR9HgPa8p1ZGiQZLFItUh5VWc82hGbTl5D36bKXbB7N04U9HOKlXC1EQQRmJC48VP6NklEzVhh6nfPG2ugdUqcjbipHmVwKSX5azsiQByQK0YusLawtALP/bXH8lyng2THaC3FjSdIjDHYFTdSAa11EyIsLRL6NK6B1UeFTS2jKqvkqhE3w9rVwTt3NLNU1uU4TtId1ColDnszczDm5oY267QgISoUhdnOv38JkbaCJC4i1CnL0M1NEjGqSz20rBPn9HwIgvAPKObGD+mQGo8fHmqH/jfVFBQmizIESWY7/fN0V2yb2FdQI0RLxBdhMVJBzT0/Wo1mby0VpKNLpaYDQOZ13+qJBCgXsaseJXTrhEsURgxhtp94a1OM6dMQM5j0bHGw+E3JMXimMuZlaLs6Nvt7tm8jwXHuaVdH0DJALs7ly/ta48v7W2OcQoCvK7w0gC92N7St7ZyV+Gp4G4zoVNeu68kR9Hod3rmzueT5IwgisPCquFm3bh0GDx6MWrVqQafTYf78+aq33bhxI4KDg9G6dWu3zc/XmDKiLbo1rIZvR7RFcJAeer0OE261xqNEhAbjgY51kZJg7c2z8dWbUbdaBBKjw/D+kBbemLYkjjRx3K5QLNBbBCuIG3EmlLjrNCDMPGqVEocXBzRGavVIrHi+F967sxke6lzPsj4lIRyLx/VAQiS/3xuTorHttb449v4g/Ptsdywa2x3j+zZCCLNPtdElqdUjcWfr2naDfJ1lcMtkrH6xNz4c6thn745WtTDprhY2NYgIgiDU4NVfjsLCQrRq1QrffvutQ9vl5ORg5MiR6Nu3r5tm5psMapGMmY93RmK0dCXiyNAgxEWEYt1LfXDkvYHYPrGfJXsFgODiRwjZ+1Z/vDywMaIMwbivfQoAvvaLmVoiF6BcUC8AxIQJvb1hIUE26ds1Y63vSwsmq6hhYhQe6pKK4CA9/niiM9rXi8ePI9vbHCMxJgyhwXo0qxWL5rVjodfroNPpEF1prZHrcu1pdDod6lePJJFCEIRH8WrMzaBBgzBo0CCHtxs9ejQeeOABBAUFOWTtCXTMbgidToewkCCbAFN7qdhVldAgPWLDQ/BM74Z4qucNMHEc+jdLQvt6CWj1Lp8l1qJOLC4wzTrl3FJ3t62N6DBrnEirlDgMbVcHw9rVwbFL+SguM+JIVj5GdK6LTSeuokWdWNmCcl1uqIa5T3d16LVsm9gPJeVGCp4lCKJK43dXu19++QWnTp3CW2+9pWp8aWkp8vLyBP8ClWTGSiMFa214b0hzy+N+drJO/BW1r4u1aAXpdQgJ0qNv0yRBmX2xmJGygm17rS8+GdYKt7fig13b1YvHgrRuiDIEI9IQjDZ149G1YXU82r0+DMFBmPt0V7w1uJkzL02W8NAgxEc6nolFEAQRSPiVuDl+/DheffVV/P777wgOVmd0Sk9PR2xsrOVfSkqKm2fpeT4c2gKDW9WyuFPkYNOsO6ZaC7gFavmPT+9trWqcUrNEc5+jno1q4L9xPSzLg/V6wTkEeFeRXq9Dcmw4dr1xC2Y90cnxSRMEQRAu4zfixmg04oEHHsA777yDG2+80f4GlUyYMAG5ubmWf5mZmW6cpXe4r0NdfD28jU2NGDGs5YZtTtg+VX1tE2eRK9EvxYRBTWyWRTjRCyg2PERVD6fa8fIWr2XP9cQX97XGsHZ1BHEswUE6fDuireW52OWXEBkKA5X4JwiC8Ap+U+cmPz8fO3bswO7duzFmzBgAgMlkAsdxCA4OxrJly3DzzTfbbGcwGGAwyN+ZVyXYCzBbfr5eQiRev62pbENLLfhvXE90mLRC1dhmtWzL9kcaglGkohhcfEQIPru3NVIS+GBgNVapsTfLp0HXiY9AnfgIm+UhQXqBxceesCQIgiA8h9+Im5iYGOzfv1+w7LvvvsOqVaswd+5c1K9f30sz8x+C2FRhjsP3D7bDjoxsDGxeEwWlFfhn13kcuuh6TFJSjAGX8oRFA2tEG3Bi0iAUlhnx7B+7sfbYFYzudQMMwXo0TY7G6N93WcZWj7a18qgpnX9ri5r44K4Wguq/XRpUw+ZTtoUB/3iiM3ZkZOOWZkloUtPxzCJzKvhHQ1vi7f8dxNSR7exsQRAEQXgKr4qbgoICnDhhrX57+vRp7NmzBwkJCahbty4mTJiA8+fPY8aMGdDr9WjevLlg+8TERISFhdksJ6QJY6wL8RGhGNi8JgY2rwmAd+EsHtcD1wpK0e59q4Wlc4MEbDmVDQB48/ab0LJOLIZ9vxkPd03FTckxeHPhAZSUCysUfz28Le79YbPN8YOD9IgN1+ObB9pgR8Z1dGtYHaHBelwWNa+sHReOh7umQqcDftmYAQCIDgvGN8PbYMrakzh4IQ/ZEt3Jw0OCbdoafDm8Nf7anomTVwotbSBWvdALDWpEocsNzjegbFAjEgBwb4cUDG1XR7GoH0EQBOFZvCpuduzYgT59+lieP//88wCAUaNGYfr06bh48SLOnj0rtznhIHyDw14wmjhB9VqWaqLqujqmg9Wj3Xnr2JH3BlrSzO9uWxsNJ/4n2CYx2oD1L/dBj49WSx4jOiwEfZpYM5nEFX3DQoLw9h18FpFZ3AxtWwddG1ZH14bVcc/3myTFjYmzte4kRodhzM2N8P6iQ5ZlDWpESc5LDbOf7IyZW8/ijdutxRNJ2BAEQfgWXhU3vXv3BidxQTIzffp0xe3ffvttvP3229pOKsBpmOjYhd0o8f6w9XOCg/SY+lA7PPnbTsH6mrFhlv5IbDE8KfR6HWrFhlnqyLDVfxekdcPW09fwWHdrmwm2AvCITnUxcysvgJWynrTKCOvcoBo6N3De4kMQBEG4H7+JuSE8x9SH2uH5v/bi03tbYeGeC3bH929WE7e1TMa/+y4C4F1IAPDJPa3w6+YzuEdFL5+61SIs4oZtBdAqJQ6tUuIEY9+4/SZkXi/GY93rY2jb2miSHINNJ64irY9888easRRUThAEUVUgcUPY0L9ZTex7Kwl6vQ6t6sQht7gcI7vUU9wmp8jqJjK7vKpFGfD8LerS9uslRFpie+yRkhAhqDnzUOd6gl5MUozskooTlwvQt2mSqmMQBEEQ/guJG0ISc6p4zdgw/P64/WJ01wvLXTpe0+Rol7a3R1hIED4a1sqtxyAIgiB8AxI3hCYEu9iU8/6OdbHm2BV0rJ9gfzBBEARBKEDihtCESUNaIG3WLrw0oLFT24eFBGH6Ix01nhVBEARRFSFxQ2hCizqxWPdyH/sDCYIgCMLNUM14giAIgiACChI3BEEQBEEEFCRuCIIgCIIIKEjcEARBEAQRUJC4IQiCIAgioCBxQxAEQRBEQEHihiAIgiCIgILEDUEQBEEQAQWJG4IgCIIgAgoSNwRBEARBBBQkbgiCIAiCCChI3BAEQRAEEVCQuCEIgiAIIqAgcUMQBEEQREAR7O0JeBqO4wAAeXl5Xp4JQRAEQRBqMV+3zddxJaqcuMnPzwcApKSkeHkmBEEQBEE4Sn5+PmJjYxXH6Dg1EiiAMJlMuHDhAqKjo6HT6TTdd15eHlJSUpCZmYmYmBhN901YofPsGeg8ew46156BzrNncNd55jgO+fn5qFWrFvR65aiaKme50ev1qFOnjluPERMTQ18cD0Dn2TPQefYcdK49A51nz+CO82zPYmOGAooJgiAIgggoSNwQBEEQBBFQkLjREIPBgLfeegsGg8HbUwlo6Dx7BjrPnoPOtWeg8+wZfOE8V7mAYoIgCIIgAhuy3BAEQRAEEVCQuCEIgiAIIqAgcUMQBEEQREBB4oYgCIIgiICCxI1GfPvtt0hNTUVYWBg6deqEbdu2eXtKfkV6ejo6dOiA6OhoJCYmYsiQITh69KhgTElJCdLS0lCtWjVERUVh6NChuHTpkmDM2bNncdtttyEiIgKJiYl46aWXUFFR4cmX4ldMnjwZOp0O48ePtyyj86wd58+fx4MPPohq1aohPDwcLVq0wI4dOyzrOY7Dm2++ieTkZISHh6Nfv344fvy4YB/Z2dkYMWIEYmJiEBcXh8ceewwFBQWefik+i9FoxBtvvIH69esjPDwcN9xwA9577z1B/yE6z46zbt06DB48GLVq1YJOp8P8+fMF67U6p/v27UOPHj0QFhaGlJQUfPTRR9q8AI5wmdmzZ3OhoaHczz//zB08eJB74oknuLi4OO7SpUvenprfMGDAAO6XX37hDhw4wO3Zs4e79dZbubp163IFBQWWMaNHj+ZSUlK4lStXcjt27OA6d+7Mde3a1bK+oqKCa968OdevXz9u9+7d3OLFi7nq1atzEyZM8MZL8nm2bdvGpaamci1btuTGjRtnWU7nWRuys7O5evXqcQ8//DC3detW7tSpU9zSpUu5EydOWMZMnjyZi42N5ebPn8/t3buXu+OOO7j69etzxcXFljEDBw7kWrVqxW3ZsoVbv34917BhQ2748OHeeEk+yaRJk7hq1apxixYt4k6fPs3NmTOHi4qK4r788kvLGDrPjrN48WJu4sSJ3D///MMB4ObNmydYr8U5zc3N5ZKSkrgRI0ZwBw4c4P744w8uPDyc++GHH1yeP4kbDejYsSOXlpZmeW40GrlatWpx6enpXpyVf3P58mUOALd27VqO4zguJyeHCwkJ4ebMmWMZc/jwYQ4At3nzZo7j+C+jXq/nsrKyLGOmTJnCxcTEcKWlpZ59AT5Ofn4+16hRI2758uVcr169LOKGzrN2vPLKK1z37t1l15tMJq5mzZrcxx9/bFmWk5PDGQwG7o8//uA4juMOHTrEAeC2b99uGfPff/9xOp2OO3/+vPsm70fcdttt3KOPPipYdvfdd3MjRozgOI7OsxaIxY1W5/S7777j4uPjBb8br7zyCte4cWOX50xuKRcpKyvDzp070a9fP8syvV6Pfv36YfPmzV6cmX+Tm5sLAEhISAAA7Ny5E+Xl5YLz3KRJE9StW9dynjdv3owWLVogKSnJMmbAgAHIy8vDwYMHPTh73yctLQ233Xab4HwCdJ61ZOHChWjfvj3uueceJCYmok2bNvjxxx8t60+fPo2srCzBuY6NjUWnTp0E5zouLg7t27e3jOnXrx/0ej22bt3quRfjw3Tt2hUrV67EsWPHAAB79+7Fhg0bMGjQIAB0nt2BVud08+bN6NmzJ0JDQy1jBgwYgKNHj+L69esuzbHKNc7UmqtXr8JoNAp+6AEgKSkJR44c8dKs/BuTyYTx48ejW7duaN68OQAgKysLoaGhiIuLE4xNSkpCVlaWZYzU+2BeR/DMnj0bu3btwvbt223W0XnWjlOnTmHKlCl4/vnn8dprr2H79u149tlnERoailGjRlnOldS5ZM91YmKiYH1wcDASEhLoXFfy6quvIi8vD02aNEFQUBCMRiMmTZqEESNGAACdZzeg1TnNyspC/fr1bfZhXhcfH+/0HEncED5HWloaDhw4gA0bNnh7KgFHZmYmxo0bh+XLlyMsLMzb0wloTCYT2rdvjw8++AAA0KZNGxw4cADff/89Ro0a5eXZBQ5//fUXZs6ciVmzZqFZs2bYs2cPxo8fj1q1atF5rsKQW8pFqlevjqCgIJtskkuXLqFmzZpempX/MmbMGCxatAirV69GnTp1LMtr1qyJsrIy5OTkCMaz57lmzZqS74N5HcG7nS5fvoy2bdsiODgYwcHBWLt2Lb766isEBwcjKSmJzrNGJCcn46abbhIsa9q0Kc6ePQvAeq6Ufjtq1qyJy5cvC9ZXVFQgOzubznUlL730El599VXcf//9aNGiBR566CE899xzSE9PB0Dn2R1odU7d+VtC4sZFQkND0a5dO6xcudKyzGQyYeXKlejSpYsXZ+ZfcByHMWPGYN68eVi1apWNqbJdu3YICQkRnOejR4/i7NmzlvPcpUsX7N+/X/CFWr58OWJiYmwuMlWVvn37Yv/+/dizZ4/lX/v27TFixAjLYzrP2tCtWzebcgbHjh1DvXr1AAD169dHzZo1Bec6Ly8PW7duFZzrnJwc7Ny50zJm1apVMJlM6NSpkwdehe9TVFQEvV54KQsKCoLJZAJA59kdaHVOu3TpgnXr1qG8vNwyZvny5WjcuLFLLikAlAquBbNnz+YMBgM3ffp07tChQ9yTTz7JxcXFCbJJCGWefvppLjY2lluzZg138eJFy7+ioiLLmNGjR3N169blVq1axe3YsYPr0qUL16VLF8t6c4py//79uT179nBLlizhatSoQSnKdmCzpTiOzrNWbNu2jQsODuYmTZrEHT9+nJs5cyYXERHB/f7775YxkydP5uLi4rgFCxZw+/bt4+68807JdNo2bdpwW7du5TZs2MA1atSoSqcoixk1ahRXu3ZtSyr4P//8w1WvXp17+eWXLWPoPDtOfn4+t3v3bm737t0cAO6zzz7jdu/ezZ05c4bjOG3OaU5ODpeUlMQ99NBD3IEDB7jZs2dzERERlAruS3z99ddc3bp1udDQUK5jx47cli1bvD0lvwKA5L9ffvnFMqa4uJh75plnuPj4eC4iIoK76667uIsXLwr2k5GRwQ0aNIgLDw/nqlevzr3wwgtceXm5h1+NfyEWN3SeteN///sf17x5c85gMHBNmjThpk6dKlhvMpm4N954g0tKSuIMBgPXt29f7ujRo4Ix165d44YPH85FRUVxMTEx3COPPMLl5+d78mX4NHl5edy4ceO4unXrcmFhYVyDBg24iRMnCtKL6Tw7zurVqyV/k0eNGsVxnHbndO/evVz37t05g8HA1a5dm5s8ebIm89dxHFPGkSAIgiAIws+hmBuCIAiCIAIKEjcEQRAEQQQUJG4IgiAIgggoSNwQBEEQBBFQkLghCIIgCCKgIHFDEARBEERAQeKGIAiCIIiAgsQNQRAEQRABBYkbgiCqJDqdDvPnz/f2NAiCcAMkbgiC8DgPP/wwdDqdzb+BAwd6e2oEQQQAwd6eAEEQVZOBAwfil19+ESwzGAxemg1BEIEEWW4IgvAKBoMBNWvWFPyLj48HwLuMpkyZgkGDBiE8PBwNGjTA3LlzBdvv378fN998M8LDw1GtWjU8+eSTKCgoEIz5+eef0axZMxgMBiQnJ2PMmDGC9VevXsVdd92FiIgINGrUCAsXLrSsu379OkaMGIEaNWogPDwcjRo1shFjBEH4JiRuCILwSd544w0MHToUe/fuxYgRI3D//ffj8OHDAIDCwkIMGDAA8fHx2L59O+bMmYMVK1YIxMuUKVOQlpaGJ598Evv378fChQvRsGFDwTHeeecd3Hvvvdi3bx9uvfVWjBgxAtnZ2ZbjHzp0CP/99x8OHz6MKVOmoHr16p47AQRBOI8mvcUJgiAcYNSoUVxQUBAXGRkp+Ddp0iSO4zgOADd69GjBNp06deKefvppjuM4burUqVx8fDxXUFBgWf/vv/9yer2ey8rK4jiO42rVqsVNnDhRdg4AuNdff93yvKCggAPA/ffffxzHcdzgwYO5Rx55RJsXTBCER6GYG4IgvEKfPn0wZcoUwbKEhATL4y5dugjWdenSBXv27AEAHD58GK1atUJkZKRlfbdu3WAymXD06FHodDpcuHABffv2VZxDy5YtLY8jIyMRExODy5cvAwCefvppDB06FLt27UL//v0xZMgQdO3a1anXShCEZyFxQxCEV4iMjLRxE2lFeHi4qnEhISGC5zqdDiaTCQAwaNAgnDlzBosXL8by5cvRt29fpKWl4ZNPPtF8vgRBaAvF3BAE4ZNs2bLF5nnTpk0BAE2bNsXevXtRWFhoWb9x40bo9Xo0btwY0dHRSE1NxcqVK12aQ40aNTBq1Cj8/vvv+OKLLzB16lSX9kcQhGcgyw1BEF6htLQUWVlZgmXBwcGWoN05c+agffv26N69O2bOnIlt27Zh2rRpAIARI0bgrbfewqhRo/D222/jypUrGDt2LB566CEkJSUBAN5++22MHj0aiYmJGDRoEPLz87Fx40aMHTtW1fzefPNNtGvXDs2aNUNpaSkWLVpkEVcEQfg2JG4IgvAKS5YsQXJysmBZ48aNceTIEQB8JtPs2bPxzDPPIDk5GX/88QduuukmAEBERASWLl2KcePGoUOHDoiIiMDQoUPx2WefWfY1atQolJSU4PPPP8eLL76I6tWrY9iwYarnFxoaigkTJiAjIwPh4eHo0aMHZs+ercErJwjC3eg4juO8PQmCIAgWnU6HefPmYciQId6eCkEQfgjF3BAEQRAEEVCQuCEIgiAIIqCgmBuCIHwO8pYTBOEKZLkhCIIgCCKgIHFDEARBEERAQeKGIAiCIIiAgsQNQRAEQRABBYkbgiAIgiACChI3BEEQBEEEFCRuCIIgCIIIKEjcEARBEAQRUPwfQKF2UZYhKhkAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"audio_dm_3_featu_.jpg\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
